{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cloud-V Documentation This documentation lists the tooling and necessary information to use Cloud-V for RISC-V application development on RISC-V compute instances provided by Cloud-V Overview Slides: link Youtube Channel: youtube.com/@Cloud-V","title":"Home"},{"location":"#cloud-v-documentation","text":"This documentation lists the tooling and necessary information to use Cloud-V for RISC-V application development on RISC-V compute instances provided by Cloud-V Overview Slides: link Youtube Channel: youtube.com/@Cloud-V","title":"Cloud-V Documentation"},{"location":"Creating_CI_CD_pipeline/","text":"Creating a CI/CD pipeline in Jenkins Pre Requisistes For sake of this documentation, jenkins built-in sample script is used to create and execute a cd/cd pipeline in jenkins. In the built-in script maven is used as M3, so one must install Maven plugin inside jenkins and name it M3 . Usually maven is already present inside jenkins and can be configured from Global Configuration Tools . Following steps demonstrate configuring Maven plugin. Go to Jenkins Dashboard and click on Manage Jenkins In Manage Jenkins , under System Configuration section, click on Global Tool Configuration . In Global Tool Configuration , scroll down to Maven section and click on the respective option under the Maven Section (should be Maven installations\u2026 or Add Maven ). Under Maven installations , enter M3 in \u201cname\u201d text box, check Install Automatically and select Version greater than 3, then click Apply and Save . This should install Maven version 3 and configure as M3 . Steps for Jenkins pipeline creation After installing Jenkins and having all the suggested plugins installed, go to Jenkins dashboard and click on Create Job . On the next page, give your pipeline a name, select Pipeline and click OK . A Configuration page for the pipeline will appear. Select Build Triggers options and General options according to need and scroll down to the Pipeline section. Definition section contains configuration for stages and steps of the pipeline. Under Definition section, you can either choose Pipeline script and try writing your own script or try some sample pipeline (like Hello World , Github+Maven etc) or you could select Pipeline script from SCM and give a github repository containing configurations of Pipeline. Press Save and Apply . This should take you to the Pipeline and you can build the pipeline and if no unresolved dependencies are present, the pipeline should build without any error.","title":"Creating a CI/CD pipeline in Jenkins"},{"location":"Creating_CI_CD_pipeline/#creating-a-cicd-pipeline-in-jenkins","text":"","title":"Creating a CI/CD pipeline in Jenkins"},{"location":"Creating_CI_CD_pipeline/#pre-requisistes","text":"For sake of this documentation, jenkins built-in sample script is used to create and execute a cd/cd pipeline in jenkins. In the built-in script maven is used as M3, so one must install Maven plugin inside jenkins and name it M3 . Usually maven is already present inside jenkins and can be configured from Global Configuration Tools . Following steps demonstrate configuring Maven plugin. Go to Jenkins Dashboard and click on Manage Jenkins In Manage Jenkins , under System Configuration section, click on Global Tool Configuration . In Global Tool Configuration , scroll down to Maven section and click on the respective option under the Maven Section (should be Maven installations\u2026 or Add Maven ). Under Maven installations , enter M3 in \u201cname\u201d text box, check Install Automatically and select Version greater than 3, then click Apply and Save . This should install Maven version 3 and configure as M3 .","title":"Pre Requisistes"},{"location":"Creating_CI_CD_pipeline/#steps-for-jenkins-pipeline-creation","text":"After installing Jenkins and having all the suggested plugins installed, go to Jenkins dashboard and click on Create Job . On the next page, give your pipeline a name, select Pipeline and click OK . A Configuration page for the pipeline will appear. Select Build Triggers options and General options according to need and scroll down to the Pipeline section. Definition section contains configuration for stages and steps of the pipeline. Under Definition section, you can either choose Pipeline script and try writing your own script or try some sample pipeline (like Hello World , Github+Maven etc) or you could select Pipeline script from SCM and give a github repository containing configurations of Pipeline. Press Save and Apply . This should take you to the Pipeline and you can build the pipeline and if no unresolved dependencies are present, the pipeline should build without any error.","title":"Steps for Jenkins pipeline creation"},{"location":"Creating_jenkinsfile/","text":"Creating a Cloud-V CI Pipeline file What is a Cloud-V CI Pipeline file A cloud-v-pipeline file is a Continuous Integration (CI) jenkins pipeline script which is written in Groovy. It describes various stages (and possibly steps) which are executed in the defined pattern. These stages can be written in bash or they can be written in Groovy itself. Mainly there can be two types of Cloud-V pipeline files are of two types: Scripted: Only contains stages Declarative: Contains stages as well as steps (more feature-rich and recommended) This documentation will cover how to create a cloud-v-pipeline file with bash script inside it and run it on various compute instances (which are known as Nodes in jenkins). Jenkins Node In jenkins, node represents a compute instance. In simple words, it is the platform on which our job build is going to run. Jenkins Master There is a jenkins Master node which is actually the compute instance on which Jenkins is installed. It is the node which schedules builds on runners. For security reason, no job is allowed to run on this node. Jenkins Slave Jenkins slave nodes are the compute instances on which our job builds run safely. They may be attached with jenkins master via hardware or they may be connected through remote SSH connection. cloud-v-pipeline file written with bash Simple Hello World cloud-v-pipeline file In Cloud-V, all the platforms are running Linux operating system, so the cloud-v-pipeline should be written in bash. Following script is an example of how can we run a bash script in scripted cloud-v-pipeline . node{ stage('*** Phase 1 ***') { //Using bash commands sh '''#!/bin/bash echo \"Hello World !\\n\" ''' } } The keyword sh is used to specify a shell script As there is nothing mentioned with node , so the script will run job build on any available compute instance. cloud-v-pipeline for a Specific Node In previous script, the cloud-v-pipeline would run on any compute instances which are available. But in case if someone wants to run a job build on specific node, then a compute instance name must be specified with keyword node . The following script is an example of running above Hello World program on node named hifive_unleashed . node('hifive_unleashed'){ stage('*** Phase 1 ***') { //Using bash commands sh '''#!/bin/bash echo \"Hello World !\\n\" ''' } } cloud-v-pipeline for Cross-Platform Compilation and Execution Cloud-V supports cross-compilation and execution on emulated RISC-V compute instances. Following tools help in cross compilation and cross-platfrom execution: RISC-V GNU Toolchain QEMU user mode (for running standalone binaries) QEMU System (for running application in Linux) An example pipeline script is given below in scripted pipeline. node('x86_runner2'){ checkout scm //Getting content of this repo stage('*** Compilation Phase ***') { // for display purposes //Compiling helloworld.c using bash commands sh '''#!/bin/bash gcc -g ./helloworld.c -o helloworld.out riscv64-unknown-linux-gnu-gcc ./helloworld.c -o helloworld_riscv_compiled.out //Cross compiling for RISC-V ''' } stage (' *** Running Binaries ***'){ sh '''#!/bin/bash ./helloworld.out qemu-riscv64 -L $RISCV_SYSROOT helloworld_riscv_compiled.out //Running executable on RISC-V emulated platform ''' } } The equivalent declarative pipeline is as follows: pipeline { agent {label \"x86_runner2\"} stages { stage('Clone Repository') { steps('delegate'){ checkout scm //Clones the repository on the local machine } } stage ('Compilation Phase'){ steps{ sh '''#!/bin/bash gcc -g ./helloworld.c -o helloworld.out ''' sh '''#!/bin/bash riscv64-unknown-linux-gnu-gcc ./helloworld.c -o helloworld_riscv_compiled.out ''' } } stage ('Running Binaries'){ steps { sh ''' #!/bin/bash ./helloworld.out ''' sh'''#!/bin/bash qemu-riscv64 -L $RISCV_SYSROOT helloworld_riscv_compiled.out ''' } } } } Reference Links https://www.jenkins.io/doc/book/pipeline/syntax/","title":"Creating a CI pipeline file"},{"location":"Creating_jenkinsfile/#creating-a-cloud-v-ci-pipeline-file","text":"","title":"Creating a Cloud-V CI Pipeline file"},{"location":"Creating_jenkinsfile/#what-is-a-cloud-v-ci-pipeline-file","text":"A cloud-v-pipeline file is a Continuous Integration (CI) jenkins pipeline script which is written in Groovy. It describes various stages (and possibly steps) which are executed in the defined pattern. These stages can be written in bash or they can be written in Groovy itself. Mainly there can be two types of Cloud-V pipeline files are of two types: Scripted: Only contains stages Declarative: Contains stages as well as steps (more feature-rich and recommended) This documentation will cover how to create a cloud-v-pipeline file with bash script inside it and run it on various compute instances (which are known as Nodes in jenkins).","title":"What is a Cloud-V CI Pipeline file"},{"location":"Creating_jenkinsfile/#jenkins-node","text":"In jenkins, node represents a compute instance. In simple words, it is the platform on which our job build is going to run.","title":"Jenkins Node"},{"location":"Creating_jenkinsfile/#jenkins-master","text":"There is a jenkins Master node which is actually the compute instance on which Jenkins is installed. It is the node which schedules builds on runners. For security reason, no job is allowed to run on this node.","title":"Jenkins Master"},{"location":"Creating_jenkinsfile/#jenkins-slave","text":"Jenkins slave nodes are the compute instances on which our job builds run safely. They may be attached with jenkins master via hardware or they may be connected through remote SSH connection.","title":"Jenkins Slave"},{"location":"Creating_jenkinsfile/#cloud-v-pipeline-file-written-with-bash","text":"","title":"cloud-v-pipeline file written with bash"},{"location":"Creating_jenkinsfile/#simple-hello-world-cloud-v-pipeline-file","text":"In Cloud-V, all the platforms are running Linux operating system, so the cloud-v-pipeline should be written in bash. Following script is an example of how can we run a bash script in scripted cloud-v-pipeline . node{ stage('*** Phase 1 ***') { //Using bash commands sh '''#!/bin/bash echo \"Hello World !\\n\" ''' } } The keyword sh is used to specify a shell script As there is nothing mentioned with node , so the script will run job build on any available compute instance.","title":"Simple Hello World cloud-v-pipeline file"},{"location":"Creating_jenkinsfile/#cloud-v-pipeline-for-a-specific-node","text":"In previous script, the cloud-v-pipeline would run on any compute instances which are available. But in case if someone wants to run a job build on specific node, then a compute instance name must be specified with keyword node . The following script is an example of running above Hello World program on node named hifive_unleashed . node('hifive_unleashed'){ stage('*** Phase 1 ***') { //Using bash commands sh '''#!/bin/bash echo \"Hello World !\\n\" ''' } }","title":"cloud-v-pipeline for a Specific Node"},{"location":"Creating_jenkinsfile/#cloud-v-pipeline-for-cross-platform-compilation-and-execution","text":"Cloud-V supports cross-compilation and execution on emulated RISC-V compute instances. Following tools help in cross compilation and cross-platfrom execution: RISC-V GNU Toolchain QEMU user mode (for running standalone binaries) QEMU System (for running application in Linux) An example pipeline script is given below in scripted pipeline. node('x86_runner2'){ checkout scm //Getting content of this repo stage('*** Compilation Phase ***') { // for display purposes //Compiling helloworld.c using bash commands sh '''#!/bin/bash gcc -g ./helloworld.c -o helloworld.out riscv64-unknown-linux-gnu-gcc ./helloworld.c -o helloworld_riscv_compiled.out //Cross compiling for RISC-V ''' } stage (' *** Running Binaries ***'){ sh '''#!/bin/bash ./helloworld.out qemu-riscv64 -L $RISCV_SYSROOT helloworld_riscv_compiled.out //Running executable on RISC-V emulated platform ''' } } The equivalent declarative pipeline is as follows: pipeline { agent {label \"x86_runner2\"} stages { stage('Clone Repository') { steps('delegate'){ checkout scm //Clones the repository on the local machine } } stage ('Compilation Phase'){ steps{ sh '''#!/bin/bash gcc -g ./helloworld.c -o helloworld.out ''' sh '''#!/bin/bash riscv64-unknown-linux-gnu-gcc ./helloworld.c -o helloworld_riscv_compiled.out ''' } } stage ('Running Binaries'){ steps { sh ''' #!/bin/bash ./helloworld.out ''' sh'''#!/bin/bash qemu-riscv64 -L $RISCV_SYSROOT helloworld_riscv_compiled.out ''' } } } }","title":"cloud-v-pipeline for Cross-Platform Compilation and Execution"},{"location":"Creating_jenkinsfile/#reference-links","text":"https://www.jenkins.io/doc/book/pipeline/syntax/","title":"Reference Links"},{"location":"Multinode_Pipelines/","text":"Multinode Pipelines One can create multinode pipelines in jenkins freestyle pipeline job as well as in jenkins native pipeline job. In such a pipeline, different stages of the pipeline can run on separate nodes. The jenkins native pipeline job can have two types. Scripted Pipeline Declarative pipeline For the sake of this documentation, two nodes are used: container-node and container-node2 Label In jenkins, label specifies where the job or a stage of the job can run. In jenkins, a label can be explicitly added in the node's configuration so that each time that label is mentioned in a jenkins job, that specific node is used for building the job, or the name of the node can also be used as label in case if no label is added in the node. Creating multinode pipelines in Jenkins native pipeline job Scripted Pipeline For creating a multinode pipeline with jenkins native scripted pipeline job, jenkinsfile can be tweaked. The keyword node can be used to specify the node in which the stages are going to run and this keyword can also be used in between stages. Following screenshot indicates a code in which first stage runs on node named container-node . While the second node runs on node named container-node2 . node ('container-node') { stage ('*** Creating a directory in container-node ***'){ sh '''#!/bin/bash mkdir newdir_container-node-$RANDOM ''' } stage ('*** Creating a directory in container-node2'){ node ('container-node2'){ sh'''#!/bin/bash mkdir newdir_container-node2-$RANDOM ''' } } } Declarative pipeline For a declarative pipeline the keyword label can be used in the agent block. This can either be done at the start of the pipeline after the keyword pipeline or it can be used inside the stages. The following code builds two stages each of which is built in separate node. The keyword none with agent means that the agent is not specified globally for each stage and it should be specified inside each stage. pipeline { agent none // Means no agent specified. This means each node will specify its own agent stages { stage('container-node') { agent{ label \"container-node\" //Selecting container-node for this stage } steps { sh '''#!/bin/bash echo 'Hello container-node' mkdir \"newdir-container-node-$RANDOM\" ''' } } stage('container-node2'){ agent{ label \"container-node2\" //Selecting container-node2 for this stage } steps{ sh'''#!/bin/bash echo ''Hello container-node2 mkdir \"newdir-container-node2-$RANDOM\" ''' } } } } Creating multinode pipelines using Jenkins freestyle jobs Jenkins freestyle jobs can be combined together by mentioning post-build jobs in the freestyle job settings. Freestyle jobs are customizable and offer more options than jenkins native pipeline jobs. Each job represents a stage and each stage has separate settings. One can use jenkins freestyle jobs to run on a separate node by selecting the option Restrict where this project can be run in the General section of the job's configuration. In the Label Expression , the node's name can be mentioned in which to run the job. The following image shows the option from job's configuration. This can be done in all the job's configuration for the specified node.","title":"Multinode Pipelines"},{"location":"Multinode_Pipelines/#multinode-pipelines","text":"One can create multinode pipelines in jenkins freestyle pipeline job as well as in jenkins native pipeline job. In such a pipeline, different stages of the pipeline can run on separate nodes. The jenkins native pipeline job can have two types. Scripted Pipeline Declarative pipeline For the sake of this documentation, two nodes are used: container-node and container-node2","title":"Multinode Pipelines"},{"location":"Multinode_Pipelines/#label","text":"In jenkins, label specifies where the job or a stage of the job can run. In jenkins, a label can be explicitly added in the node's configuration so that each time that label is mentioned in a jenkins job, that specific node is used for building the job, or the name of the node can also be used as label in case if no label is added in the node.","title":"Label"},{"location":"Multinode_Pipelines/#creating-multinode-pipelines-in-jenkins-native-pipeline-job","text":"","title":"Creating multinode pipelines in Jenkins native pipeline job"},{"location":"Multinode_Pipelines/#scripted-pipeline","text":"For creating a multinode pipeline with jenkins native scripted pipeline job, jenkinsfile can be tweaked. The keyword node can be used to specify the node in which the stages are going to run and this keyword can also be used in between stages. Following screenshot indicates a code in which first stage runs on node named container-node . While the second node runs on node named container-node2 . node ('container-node') { stage ('*** Creating a directory in container-node ***'){ sh '''#!/bin/bash mkdir newdir_container-node-$RANDOM ''' } stage ('*** Creating a directory in container-node2'){ node ('container-node2'){ sh'''#!/bin/bash mkdir newdir_container-node2-$RANDOM ''' } } }","title":"Scripted Pipeline"},{"location":"Multinode_Pipelines/#declarative-pipeline","text":"For a declarative pipeline the keyword label can be used in the agent block. This can either be done at the start of the pipeline after the keyword pipeline or it can be used inside the stages. The following code builds two stages each of which is built in separate node. The keyword none with agent means that the agent is not specified globally for each stage and it should be specified inside each stage. pipeline { agent none // Means no agent specified. This means each node will specify its own agent stages { stage('container-node') { agent{ label \"container-node\" //Selecting container-node for this stage } steps { sh '''#!/bin/bash echo 'Hello container-node' mkdir \"newdir-container-node-$RANDOM\" ''' } } stage('container-node2'){ agent{ label \"container-node2\" //Selecting container-node2 for this stage } steps{ sh'''#!/bin/bash echo ''Hello container-node2 mkdir \"newdir-container-node2-$RANDOM\" ''' } } } }","title":"Declarative pipeline"},{"location":"Multinode_Pipelines/#creating-multinode-pipelines-using-jenkins-freestyle-jobs","text":"Jenkins freestyle jobs can be combined together by mentioning post-build jobs in the freestyle job settings. Freestyle jobs are customizable and offer more options than jenkins native pipeline jobs. Each job represents a stage and each stage has separate settings. One can use jenkins freestyle jobs to run on a separate node by selecting the option Restrict where this project can be run in the General section of the job's configuration. In the Label Expression , the node's name can be mentioned in which to run the job. The following image shows the option from job's configuration. This can be done in all the job's configuration for the specified node.","title":"Creating multinode pipelines using Jenkins freestyle jobs"},{"location":"Software_Developer_Guide/","text":"Software Developer Guide for RISC-V CI A software developer is the end-user who will develop or build his/her projects on RISC-V CI infrastructure. This guide will cover all the things a software developer needs to create a project based on Cloud-V Continuous Integration (CI). Pre-requisites GitHub account. GitHub project repository with owner rights. Getting an account for Cloud-V Fill out this google form with all the required information for getting an account on Cloud-V. After this we will get back to you with login credentials. Setting up cloud-v-pipeline inside github project repository Cloud-V will need a cloud-v-pipeline written with jenkinsfile pipeline syntax to start execution of tests/checks (see link ). This pipeline will contain all the stages (and may be steps) of a CI/CD pipeline. This pipeline can be scripted pipeline which will only have stages or it can also be declarative pipeline which may also have steps inside stages. A simple scripted Helloworld pipeline in linux is as follows: node{ stage('*** Phase 1 ***') { //Using bash commands sh '''#!/bin/bash echo \"Hello World !\\n\" ''' } } Upon execution of such a pipeline, the console output can be viewed as follows. Note: This cloud-v-pipeline should remain same in all the branches and pull requests. Setting credentials for webhook Cloud-V supports webhooks which can trigger the job from external sources such as GitHub. They work in a way such that, if a specified branch is committed or if a pull request is created, the specified job build starts running depending upon the trigger event which is set in build's configuration in Cloud-V. This process requires access token of the repository CREATED BY OWNER OF REPOSITORY on which the webhook is to be set. These credentials can be safely added to Cloud-V without anyone (even administrator) seeing the passwords as follows. Obtaining github access token for repository Navigate to the dashboard of your github account and click on the your github profile picture on the top-right corner on dashboard. Then click on the \"Settings\" from the list. From the left option bar in Settings scroll down and click on \"Developer settings\". Once there, click on \"Personal access tokens\", then click on \"Fine-grained tokens\" from the dropdown list and after that click on \"Generate new token\". This will open the page for setting up new access token. Follow following steps for creating a token: Give your token a meaningful name under \"Token name\" Set expiration date in \"Expiration\" depending upon how long you would like your repository to be integrated with Cloud-V (think of a meaningful upper bound) The \"Resource owner\" should be the owner of the repository who can access all kinds of settings of the repository Under \"Repository access\", check \"Only select repositories\" and then select the repository for which you would like to create the token Under \"Permissions\" section, expand \"Repository Permissions\" and give the following two permissions: \"Read and write\" access to \"Commit statuses\" (Because after the CI has run, Cloud-V will be able to set the status of the commit accordingly) \"Read-only\" access to \"Webhooks\" Configuring repository webhook In GitHub, Go to repository settings which you want to integrate for Cloud-V. Go to Webhooks Click on Add webhook Add Payload URL as https://dash.cloud-v.co/ghprbhook/ Select content type as application/json Check Enable SSL verification In the section Which events would you like to trigger this webhook? check Let me select individual events and check Pul requests as individual events and dont check any other permission. Webhook settings will look something like this: Configurations inside Cloud-V Note: Currently users are not able to see or modify pipeline build configuration inside Jenkins, that is currently managed by administrator. Users are requested to inform administrator about how they want their pipeline configured. We will provide you with Cloud-V credentials on the provided email. Login with provided credentials. Click on the Credentials in the left menu. This will take you to the credentials page. Scroll down to the Stores scoped to Jenkins and click on the System as shown in the image. Click on Global credentials (unrestricted) . Click on Add Credentials . This will take you to the New Credentials page. Select Kind as Username with password . Select Scope as Global (Jenkins, nodes, items, all child items etc) . Enter your GitHub username in Username Enter Password as GitHub personal authentication token (PAT) which can be acquired from Github account settings. ID is optional but you can enter a unique ID . Description can be left empty. But it is recommended to give a suitable but careful description by which administrator will be able to identify and use these credentials to set up github webhook Select Create This process will look something like this Now credentials will be available in the credentials list and will be shown to you as well as administrator as shown in the image below. This will create an option in configurations for using these credentials in github webhook without changing or viewing them. Note the credentials ID (as shown in the image below) and email it to the same administrator email on which you received the login credentials for Cloud-V. It is important that administrator knows the credentials ID because they will use it in the job build configurations. Note: Please make sure to inform the administrator via email that you have added the credentials in Cloud-V Dashboard. Also, send administrator the ID of credentials via email. Requirements for administrator After the above setup is complete from software developer's side, developer will need to provide the administrator with following information. Dependencies for running the project which can be packages which are needed to install in the RISC-V CI environment by administrator. Events for triggering the job build. URL of GitHub repository. Path and name of cloud-v-pipeline file on the provided GitHub repository. Any additional information which should be given for successful execution of job builds.","title":"Getting Started"},{"location":"Software_Developer_Guide/#software-developer-guide-for-risc-v-ci","text":"A software developer is the end-user who will develop or build his/her projects on RISC-V CI infrastructure. This guide will cover all the things a software developer needs to create a project based on Cloud-V Continuous Integration (CI).","title":"Software Developer Guide for RISC-V CI"},{"location":"Software_Developer_Guide/#pre-requisites","text":"GitHub account. GitHub project repository with owner rights.","title":"Pre-requisites"},{"location":"Software_Developer_Guide/#getting-an-account-for-cloud-v","text":"Fill out this google form with all the required information for getting an account on Cloud-V. After this we will get back to you with login credentials.","title":"Getting an account for Cloud-V"},{"location":"Software_Developer_Guide/#setting-up-cloud-v-pipeline-inside-github-project-repository","text":"Cloud-V will need a cloud-v-pipeline written with jenkinsfile pipeline syntax to start execution of tests/checks (see link ). This pipeline will contain all the stages (and may be steps) of a CI/CD pipeline. This pipeline can be scripted pipeline which will only have stages or it can also be declarative pipeline which may also have steps inside stages. A simple scripted Helloworld pipeline in linux is as follows: node{ stage('*** Phase 1 ***') { //Using bash commands sh '''#!/bin/bash echo \"Hello World !\\n\" ''' } } Upon execution of such a pipeline, the console output can be viewed as follows. Note: This cloud-v-pipeline should remain same in all the branches and pull requests.","title":"Setting up cloud-v-pipeline inside github project repository"},{"location":"Software_Developer_Guide/#setting-credentials-for-webhook","text":"Cloud-V supports webhooks which can trigger the job from external sources such as GitHub. They work in a way such that, if a specified branch is committed or if a pull request is created, the specified job build starts running depending upon the trigger event which is set in build's configuration in Cloud-V. This process requires access token of the repository CREATED BY OWNER OF REPOSITORY on which the webhook is to be set. These credentials can be safely added to Cloud-V without anyone (even administrator) seeing the passwords as follows.","title":"Setting credentials for webhook"},{"location":"Software_Developer_Guide/#obtaining-github-access-token-for-repository","text":"Navigate to the dashboard of your github account and click on the your github profile picture on the top-right corner on dashboard. Then click on the \"Settings\" from the list. From the left option bar in Settings scroll down and click on \"Developer settings\". Once there, click on \"Personal access tokens\", then click on \"Fine-grained tokens\" from the dropdown list and after that click on \"Generate new token\". This will open the page for setting up new access token. Follow following steps for creating a token: Give your token a meaningful name under \"Token name\" Set expiration date in \"Expiration\" depending upon how long you would like your repository to be integrated with Cloud-V (think of a meaningful upper bound) The \"Resource owner\" should be the owner of the repository who can access all kinds of settings of the repository Under \"Repository access\", check \"Only select repositories\" and then select the repository for which you would like to create the token Under \"Permissions\" section, expand \"Repository Permissions\" and give the following two permissions: \"Read and write\" access to \"Commit statuses\" (Because after the CI has run, Cloud-V will be able to set the status of the commit accordingly) \"Read-only\" access to \"Webhooks\"","title":"Obtaining github access token for repository"},{"location":"Software_Developer_Guide/#configuring-repository-webhook","text":"In GitHub, Go to repository settings which you want to integrate for Cloud-V. Go to Webhooks Click on Add webhook Add Payload URL as https://dash.cloud-v.co/ghprbhook/ Select content type as application/json Check Enable SSL verification In the section Which events would you like to trigger this webhook? check Let me select individual events and check Pul requests as individual events and dont check any other permission. Webhook settings will look something like this:","title":"Configuring repository webhook"},{"location":"Software_Developer_Guide/#configurations-inside-cloud-v","text":"Note: Currently users are not able to see or modify pipeline build configuration inside Jenkins, that is currently managed by administrator. Users are requested to inform administrator about how they want their pipeline configured. We will provide you with Cloud-V credentials on the provided email. Login with provided credentials. Click on the Credentials in the left menu. This will take you to the credentials page. Scroll down to the Stores scoped to Jenkins and click on the System as shown in the image. Click on Global credentials (unrestricted) . Click on Add Credentials . This will take you to the New Credentials page. Select Kind as Username with password . Select Scope as Global (Jenkins, nodes, items, all child items etc) . Enter your GitHub username in Username Enter Password as GitHub personal authentication token (PAT) which can be acquired from Github account settings. ID is optional but you can enter a unique ID . Description can be left empty. But it is recommended to give a suitable but careful description by which administrator will be able to identify and use these credentials to set up github webhook Select Create This process will look something like this Now credentials will be available in the credentials list and will be shown to you as well as administrator as shown in the image below. This will create an option in configurations for using these credentials in github webhook without changing or viewing them. Note the credentials ID (as shown in the image below) and email it to the same administrator email on which you received the login credentials for Cloud-V. It is important that administrator knows the credentials ID because they will use it in the job build configurations. Note: Please make sure to inform the administrator via email that you have added the credentials in Cloud-V Dashboard. Also, send administrator the ID of credentials via email.","title":"Configurations inside Cloud-V"},{"location":"Software_Developer_Guide/#requirements-for-administrator","text":"After the above setup is complete from software developer's side, developer will need to provide the administrator with following information. Dependencies for running the project which can be packages which are needed to install in the RISC-V CI environment by administrator. Events for triggering the job build. URL of GitHub repository. Path and name of cloud-v-pipeline file on the provided GitHub repository. Any additional information which should be given for successful execution of job builds.","title":"Requirements for administrator"},{"location":"Tooling/","text":"Tools on Cloud-V Using Environment Modules Users can use environment modules to load different versions of same program. For using environment modules the pattern is as follows: module load <PACKAGENAME/VERSION> Important Note: Be sure to use #!/bin/bash -l instead of #!/bin/bash in CI pipeline file since that is required for environment modules to load For example if you want to load python version 3.9.2 compiled for x86, you will need to use following command: module load python/3.9.2 For packages compiled for RISC-V architecture host, you will need to append _riscv to package name. For example, for python 3.8.15 compiled for RISC-V, following command will be used. module load python_riscv/3.8.15","title":"Using Environment Modules"},{"location":"Tooling/#tools-on-cloud-v","text":"","title":"Tools on Cloud-V"},{"location":"Tooling/#using-environment-modules","text":"Users can use environment modules to load different versions of same program. For using environment modules the pattern is as follows: module load <PACKAGENAME/VERSION> Important Note: Be sure to use #!/bin/bash -l instead of #!/bin/bash in CI pipeline file since that is required for environment modules to load For example if you want to load python version 3.9.2 compiled for x86, you will need to use following command: module load python/3.9.2 For packages compiled for RISC-V architecture host, you will need to append _riscv to package name. For example, for python 3.8.15 compiled for RISC-V, following command will be used. module load python_riscv/3.8.15","title":"Using Environment Modules"},{"location":"jenkins_gitlab_integration/","text":"Integrating GitLab with Jenkins This documentation will cover how to create an integration between jenkins and GitLab. This will allow users to trigger jenkins job when a merge request or a push is detected in GitLab. Pre-requisites GitLab plugin Git plugin GitLab repository with owner's credentials Configuring Jenkins System First jenkins needs to be configured. Go to Dashboard > Manage Jenkins > Configure System Scroll down to Gitlab section Check Enable authentication for '/project' end-point Enter a Connection name . Enter Gitlab host URL as https://gitlab.com/ . In case there is a different domain name, then enter there instead of above url. In Credentials , click on Add then click on Jenkins . In Kind , select GitLab API token . In API token , enter the gitlab personal access token (this will be obtained below while configuring GitLab). Click on Advanced . Click on Test Connection . If everything goes right, it should print success . Configuring GitLab Click on profile avatar in the top right. Click on Edit profile . Click on Access Tokens . Create a new personal access token and copy it (this is the GitLab API token used in above section Configuring Jenkins System ). Go to repository settings. On left-side pane, select Webhooks . Enter GitLab webhook URL (this is explained below in next section). Enter Secret Token (this is explained in the below section). Check desirable trigger options. Click Add webhook . Configuring Jenkins Job Create a jenkins job. On job configuration page, scroll down to Source Code Management . Select Git . In Credentials , add the owner credentials of GitLab repository. This will be Username and Password . Select the appropriate branch (generally it is main ). Scroll down to Build Triggers . Check Build when a change is pushed to GitLab . There will also be a GitLab webhook URL . This is needed in GitLab. This URL will be called as Webhook URL . Click on Advanced in the same option, then scroll down and generate a secret token. This token is also needed. This token will be called as Secret Token . Scroll down to Post-build Actions and select Publish build status to GitLab . Click on Apply and Save . Now whenever there will be a push (or whatever build trigger option is set in jenkins and gitlab webhook), specified jenkins job will be triggered.","title":"Integrating GitLab with Jenkins"},{"location":"jenkins_gitlab_integration/#integrating-gitlab-with-jenkins","text":"This documentation will cover how to create an integration between jenkins and GitLab. This will allow users to trigger jenkins job when a merge request or a push is detected in GitLab.","title":"Integrating GitLab with Jenkins"},{"location":"jenkins_gitlab_integration/#pre-requisites","text":"GitLab plugin Git plugin GitLab repository with owner's credentials","title":"Pre-requisites"},{"location":"jenkins_gitlab_integration/#configuring-jenkins-system","text":"First jenkins needs to be configured. Go to Dashboard > Manage Jenkins > Configure System Scroll down to Gitlab section Check Enable authentication for '/project' end-point Enter a Connection name . Enter Gitlab host URL as https://gitlab.com/ . In case there is a different domain name, then enter there instead of above url. In Credentials , click on Add then click on Jenkins . In Kind , select GitLab API token . In API token , enter the gitlab personal access token (this will be obtained below while configuring GitLab). Click on Advanced . Click on Test Connection . If everything goes right, it should print success .","title":"Configuring Jenkins System"},{"location":"jenkins_gitlab_integration/#configuring-gitlab","text":"Click on profile avatar in the top right. Click on Edit profile . Click on Access Tokens . Create a new personal access token and copy it (this is the GitLab API token used in above section Configuring Jenkins System ). Go to repository settings. On left-side pane, select Webhooks . Enter GitLab webhook URL (this is explained below in next section). Enter Secret Token (this is explained in the below section). Check desirable trigger options. Click Add webhook .","title":"Configuring GitLab"},{"location":"jenkins_gitlab_integration/#configuring-jenkins-job","text":"Create a jenkins job. On job configuration page, scroll down to Source Code Management . Select Git . In Credentials , add the owner credentials of GitLab repository. This will be Username and Password . Select the appropriate branch (generally it is main ). Scroll down to Build Triggers . Check Build when a change is pushed to GitLab . There will also be a GitLab webhook URL . This is needed in GitLab. This URL will be called as Webhook URL . Click on Advanced in the same option, then scroll down and generate a secret token. This token is also needed. This token will be called as Secret Token . Scroll down to Post-build Actions and select Publish build status to GitLab . Click on Apply and Save . Now whenever there will be a push (or whatever build trigger option is set in jenkins and gitlab webhook), specified jenkins job will be triggered.","title":"Configuring Jenkins Job"},{"location":"runner_specs/","text":"Specifications of compute instances in Cloud-V This document contains the specifications of the compute instances available for users to run builds in Cloud-V. The term \"Compute Instance\", can also be safely interchanged with the terms \"Build Executor\" and \"Runner\". Name String Architecture ISA String Cores Memory Compute Instance Type J-x86-1 (or) J-QMU-1 x86_64 N/A 4 8GiB Hardware with application-level emulator J-TESTVM-1 x86_64 N/A 4 8GiB Hardware J-RASP4-1 aarch64 ARMv8-A 4 4GiB Hardware J-QMS-1 riscv64 See Ext 1 at bottom 2 2GiB QEMU System emulator J-VF1-1 riscv64 rv64imafdc 2 8GiB Hardware J-VF1-2 riscv64 rv64imafdc 2 8GiB Hardware J-VF1-3 riscv64 rv64imafdc 2 8GiB Hardware J-VF2-1 riscv64 rv64imafdc 4 8GiB Hardware J-VF2-2 riscv64 rv64imafdc 4 8GiB Hardware J-VF2-3 riscv64 rv64imafdc 4 8GiB Hardware J-VF2-4 riscv64 rv64imafdc 4 8GiB Hardware J-VF2-5 riscv64 rv64imafdc 4 8GiB Hardware J-VF2-6 riscv64 rv64imafdc 4 8GiB Hardware J-HF-1 riscv64 rv64imafdc 4 8GiB Hardware J-K230-1 riscv64 rv64imafdcvxthead 1 512MiB Hardware Ext1: rv64imafdch_zicbom_zicboz_zicntr_zicsr_zifencei_zihintpause_zihpm_zba_zbb_zbs_sstc Note: The J-QMU-1 and J-x86-1 are one and the same runner. The purpose of creating two separate executors for same hardware is that J-x86-1 is supposed to be specifically for x86 architecture whereas J-QMU-1 is specifically for the users who want to cross compile source code for riscv64 architecture and then use qemu-usermode to execute them. Nevertheless, the tooling available for J-x86-1 can also be used for J-QMU-1","title":"Compute Instance Specifications"},{"location":"runner_specs/#specifications-of-compute-instances-in-cloud-v","text":"This document contains the specifications of the compute instances available for users to run builds in Cloud-V. The term \"Compute Instance\", can also be safely interchanged with the terms \"Build Executor\" and \"Runner\". Name String Architecture ISA String Cores Memory Compute Instance Type J-x86-1 (or) J-QMU-1 x86_64 N/A 4 8GiB Hardware with application-level emulator J-TESTVM-1 x86_64 N/A 4 8GiB Hardware J-RASP4-1 aarch64 ARMv8-A 4 4GiB Hardware J-QMS-1 riscv64 See Ext 1 at bottom 2 2GiB QEMU System emulator J-VF1-1 riscv64 rv64imafdc 2 8GiB Hardware J-VF1-2 riscv64 rv64imafdc 2 8GiB Hardware J-VF1-3 riscv64 rv64imafdc 2 8GiB Hardware J-VF2-1 riscv64 rv64imafdc 4 8GiB Hardware J-VF2-2 riscv64 rv64imafdc 4 8GiB Hardware J-VF2-3 riscv64 rv64imafdc 4 8GiB Hardware J-VF2-4 riscv64 rv64imafdc 4 8GiB Hardware J-VF2-5 riscv64 rv64imafdc 4 8GiB Hardware J-VF2-6 riscv64 rv64imafdc 4 8GiB Hardware J-HF-1 riscv64 rv64imafdc 4 8GiB Hardware J-K230-1 riscv64 rv64imafdcvxthead 1 512MiB Hardware Ext1: rv64imafdch_zicbom_zicboz_zicntr_zicsr_zifencei_zihintpause_zihpm_zba_zbb_zbs_sstc Note: The J-QMU-1 and J-x86-1 are one and the same runner. The purpose of creating two separate executors for same hardware is that J-x86-1 is supposed to be specifically for x86 architecture whereas J-QMU-1 is specifically for the users who want to cross compile source code for riscv64 architecture and then use qemu-usermode to execute them. Nevertheless, the tooling available for J-x86-1 can also be used for J-QMU-1","title":"Specifications of compute instances in Cloud-V"},{"location":"tooling_J-HF-1/","text":"Tools on J-HF-1 node This compute instance is Sifive's HiFive Unleased board and it has available packages ONLY for RISC-V architecture. Operating System: Ubuntu 20.04.6 (Focal Fossa) Tool Version Installed from Git 2.25.1 apt OpenJDK 11.0.20.1 apt GCC 10.5.0 apt Python3 3.8.10 apt OpenSSL 1.1.1f apt Ruby 2.7.0p0 apt Go 1.14.3 apt rustc 1.41.0 apt Flex 2.6.4 apt Ninja 1.10.0 apt Bison 3.5.1 apt autoconf 2.69 apt gperf 3.1 apt cmake 3.16.3 apt make 4.2.1 apt automake 1.16.1 apt gfortran 9.4.0 apt openssh-server 8.2p1 apt","title":"J-HF-1"},{"location":"tooling_J-HF-1/#tools-on-j-hf-1-node","text":"This compute instance is Sifive's HiFive Unleased board and it has available packages ONLY for RISC-V architecture. Operating System: Ubuntu 20.04.6 (Focal Fossa) Tool Version Installed from Git 2.25.1 apt OpenJDK 11.0.20.1 apt GCC 10.5.0 apt Python3 3.8.10 apt OpenSSL 1.1.1f apt Ruby 2.7.0p0 apt Go 1.14.3 apt rustc 1.41.0 apt Flex 2.6.4 apt Ninja 1.10.0 apt Bison 3.5.1 apt autoconf 2.69 apt gperf 3.1 apt cmake 3.16.3 apt make 4.2.1 apt automake 1.16.1 apt gfortran 9.4.0 apt openssh-server 8.2p1 apt","title":"Tools on J-HF-1 node"},{"location":"tooling_J-K230-1/","text":"Tools on J-K230-1 node This compute instance is Starfive's VisionFive 2 board and it has available packages ONLY for RISC-V architecture. Operating System: Debian 12 (Bookworm) Tool Version Installed from Git 2.40.1 apt OpenJDK 21.0.2 apt GCC 13.2.0 apt Python3 3.11.6 apt OpenSSL 3.0.10 apt Ruby 3.1.2p20 apt Go 1.21.1 apt rustc 1.75.0 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.27.4 apt make 4.3 apt automake 1.16.5 apt gfortran 13.2.0 apt openssh-server 9.3p1 apt vim 9.0 apt vim 3.3a apt","title":"J-K230-1"},{"location":"tooling_J-K230-1/#tools-on-j-k230-1-node","text":"This compute instance is Starfive's VisionFive 2 board and it has available packages ONLY for RISC-V architecture. Operating System: Debian 12 (Bookworm) Tool Version Installed from Git 2.40.1 apt OpenJDK 21.0.2 apt GCC 13.2.0 apt Python3 3.11.6 apt OpenSSL 3.0.10 apt Ruby 3.1.2p20 apt Go 1.21.1 apt rustc 1.75.0 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.27.4 apt make 4.3 apt automake 1.16.5 apt gfortran 13.2.0 apt openssh-server 9.3p1 apt vim 9.0 apt vim 3.3a apt","title":"Tools on J-K230-1 node"},{"location":"tooling_J-QMS-1/","text":"Tools on J-QMS-1 node This is QEMU system compute instance with 64-bit RISC-V Linux and it has all the packages ONLY for RISC-V architecture Operating System: Ubuntu 22.04.4 LTS (Jammy Jellyfish) QEMU Linux Version: 7.2.90 (v9.0.0-rc0-68-g853546f812) Tool Version Installed from pip 22.0.2 apt Git 2.34.1 apt OpenJDK 19.0.1 apt GCC 11.4.0 apt Python3 3.10.12 apt OpenSSL 3.0.2 apt Ruby 3.0.2p107 apt Go 1.18.8 apt rustc 1.75.0 apt Flex 2.6.4 apt Ninja 1.10.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt make 4.3 apt cmake 3.22.1 apt","title":"J-QMS-1"},{"location":"tooling_J-QMS-1/#tools-on-j-qms-1-node","text":"This is QEMU system compute instance with 64-bit RISC-V Linux and it has all the packages ONLY for RISC-V architecture Operating System: Ubuntu 22.04.4 LTS (Jammy Jellyfish) QEMU Linux Version: 7.2.90 (v9.0.0-rc0-68-g853546f812) Tool Version Installed from pip 22.0.2 apt Git 2.34.1 apt OpenJDK 19.0.1 apt GCC 11.4.0 apt Python3 3.10.12 apt OpenSSL 3.0.2 apt Ruby 3.0.2p107 apt Go 1.18.8 apt rustc 1.75.0 apt Flex 2.6.4 apt Ninja 1.10.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt make 4.3 apt cmake 3.22.1 apt","title":"Tools on J-QMS-1 node"},{"location":"tooling_J-QMU-1J-VM-1/","text":"Tools on J-x86-1 or J-QMU-1 node Tools which are mentioned for x86 architecture are able to run on J-x86-1 . Tools which are mentioned for RISC-V architecture are able to run on J-QMU-1 . The packages which are supported for QEMU User mode can be used by normal commands once they are loaded. Here PACKAGE_NAME is the package which you want to run on QEMU user mode. Operating System: Debian 11 (bullseye) QEMU User Mode Version: Different Versions (see the table below) Tooling available for J-x86-1 The tools available for J-x86-1 is for use on x86 architecture and these tools do not support execution on RISC-V architecture Tool Versions Installed from Host Architecture Environment Modules Support Git 2.3.0.2 source x86 N/A OpenJDK 19.0.1 apt x86 N/A GCC 10.4.0, 12.2.0 apt x86 Yes Python3 3.8.15, 3.9.2 source x86 Yes Go 1.18.8 apt x86 N/A rustc 1.65.0 source x86 N/A Flex 2.6.4 apt x86 N/A Ninja 1.10.1-1 apt x86 N/A Bison 3.7.5 apt x86 N/A autoconf 2.69 apt x86 N/A gperf 2.2.4 apt x86 N/A spike 1.1.1-dev source x86 Yes Verilator 4.038 apt x86 N/A Sail (riscv_sim_RV64, riscv_sim_RV32) 0.5 source x86 Yes cmake 3.18.4 apt x86 N/A make 4.3 apt x86 N/A ARM EABI GNU toolchain 13.2.Rel1 source x86 Yes qemu-system-arm 8.2.2 source x86 Yes SCons 4.7.0 source x86 Yes Tooling available for J-QMU-1 The tools available for J-QMU-1 is for use on RISC-V architecture and these tools do not support execution on x86 architecture Tool Versions Installed from Host Architecture Environment Modules Support Python3 3.8.15 source RISC-V Yes zlib 1.2.13 source RISC-V N/A OpenSSL 1.1.1r source RISC-V Yes Ruby (without IRB) 3.2.0dev source RISC-V Yes rustc 1.65.0 source RISC-V N/A Flex 2.6.4 source RISC-V yes Ninja 1.12.0.git source RISC-V Yes Bison 3.8.2, 2.3 source RISC-V Yes clang 16.0.0 source RISC-V (cross compiler) Yes riscv-pk 1.0.0-91-g573c858 source RISC-V Yes QEMU User mode and RISC-V GNU Cross compilers From now on RISC-V cross-compilers can only be loaded with their respective QEMU User mode on Cloud-V. This is configured so that there is no confusion between toolchain version and qemu user mode being used because both of these will be \"generally\" taken from the latest releases of nightly builds. Loading a certain RISC-V toolchain using environment modules will automatically load the respective qemu usermode version unless otherwise specified. The loading pattern for RISC-V 64-bit GNU Glibc toolchain will be as follows: module load riscv64-gnu-glibc/<release-date> And the loading pattern for RISC-V 64-bit GNU Glibc toolchain will be as follows: module load riscv64-gnu-elf/<release-date> Following table provides relevant information about version of the toolchain and the respective QEMU User mode version (where the release date is mentioned in pattern MMDDYYYY ). Release date GNU Toolchain version (elf and glibc) QEMU Version 03012024 13.2.0 8.2.1 02022024 13.2.0 8.2.1 02022024 13.2.0 8.1.1 Note: The J-QMU-1 and J-x86-1 are one and the same runner. The purpose of creating two separate executors for same hardware is that J-x86-1 is supposed to be specifically for x86 architecture whereas J-QMU-1 is specifically for the users who want to cross compile source code for riscv64 architecture and then use qemu-usermode to execute them. Nevertheless, the tooling available for J-x86-1 can also be used for J-QMU-1","title":"J-x86-1 or J-QMU-1"},{"location":"tooling_J-QMU-1J-VM-1/#tools-on-j-x86-1-or-j-qmu-1-node","text":"Tools which are mentioned for x86 architecture are able to run on J-x86-1 . Tools which are mentioned for RISC-V architecture are able to run on J-QMU-1 . The packages which are supported for QEMU User mode can be used by normal commands once they are loaded. Here PACKAGE_NAME is the package which you want to run on QEMU user mode. Operating System: Debian 11 (bullseye) QEMU User Mode Version: Different Versions (see the table below)","title":"Tools on J-x86-1 or J-QMU-1 node"},{"location":"tooling_J-QMU-1J-VM-1/#tooling-available-for-j-x86-1","text":"The tools available for J-x86-1 is for use on x86 architecture and these tools do not support execution on RISC-V architecture Tool Versions Installed from Host Architecture Environment Modules Support Git 2.3.0.2 source x86 N/A OpenJDK 19.0.1 apt x86 N/A GCC 10.4.0, 12.2.0 apt x86 Yes Python3 3.8.15, 3.9.2 source x86 Yes Go 1.18.8 apt x86 N/A rustc 1.65.0 source x86 N/A Flex 2.6.4 apt x86 N/A Ninja 1.10.1-1 apt x86 N/A Bison 3.7.5 apt x86 N/A autoconf 2.69 apt x86 N/A gperf 2.2.4 apt x86 N/A spike 1.1.1-dev source x86 Yes Verilator 4.038 apt x86 N/A Sail (riscv_sim_RV64, riscv_sim_RV32) 0.5 source x86 Yes cmake 3.18.4 apt x86 N/A make 4.3 apt x86 N/A ARM EABI GNU toolchain 13.2.Rel1 source x86 Yes qemu-system-arm 8.2.2 source x86 Yes SCons 4.7.0 source x86 Yes","title":"Tooling available for J-x86-1"},{"location":"tooling_J-QMU-1J-VM-1/#tooling-available-for-j-qmu-1","text":"The tools available for J-QMU-1 is for use on RISC-V architecture and these tools do not support execution on x86 architecture Tool Versions Installed from Host Architecture Environment Modules Support Python3 3.8.15 source RISC-V Yes zlib 1.2.13 source RISC-V N/A OpenSSL 1.1.1r source RISC-V Yes Ruby (without IRB) 3.2.0dev source RISC-V Yes rustc 1.65.0 source RISC-V N/A Flex 2.6.4 source RISC-V yes Ninja 1.12.0.git source RISC-V Yes Bison 3.8.2, 2.3 source RISC-V Yes clang 16.0.0 source RISC-V (cross compiler) Yes riscv-pk 1.0.0-91-g573c858 source RISC-V Yes","title":"Tooling available for J-QMU-1"},{"location":"tooling_J-QMU-1J-VM-1/#qemu-user-mode-and-risc-v-gnu-cross-compilers","text":"From now on RISC-V cross-compilers can only be loaded with their respective QEMU User mode on Cloud-V. This is configured so that there is no confusion between toolchain version and qemu user mode being used because both of these will be \"generally\" taken from the latest releases of nightly builds. Loading a certain RISC-V toolchain using environment modules will automatically load the respective qemu usermode version unless otherwise specified. The loading pattern for RISC-V 64-bit GNU Glibc toolchain will be as follows: module load riscv64-gnu-glibc/<release-date> And the loading pattern for RISC-V 64-bit GNU Glibc toolchain will be as follows: module load riscv64-gnu-elf/<release-date> Following table provides relevant information about version of the toolchain and the respective QEMU User mode version (where the release date is mentioned in pattern MMDDYYYY ). Release date GNU Toolchain version (elf and glibc) QEMU Version 03012024 13.2.0 8.2.1 02022024 13.2.0 8.2.1 02022024 13.2.0 8.1.1 Note: The J-QMU-1 and J-x86-1 are one and the same runner. The purpose of creating two separate executors for same hardware is that J-x86-1 is supposed to be specifically for x86 architecture whereas J-QMU-1 is specifically for the users who want to cross compile source code for riscv64 architecture and then use qemu-usermode to execute them. Nevertheless, the tooling available for J-x86-1 can also be used for J-QMU-1","title":"QEMU User mode and RISC-V GNU Cross compilers"},{"location":"tooling_J-VF1-x/","text":"Tools on J-VF1-x node This compute instance is Starfive's VisionFive 1 board and it has available packages ONLY for RISC-V architecture. Operating System: Ubuntu 23.04 (Lunar Lobster) Tool Version Installed from Git 2.39.2 apt OpenJDK 21-ea apt GCC 13.1.0 apt Python3 3.11.4 apt OpenSSL 3.0.8 apt Ruby 3.1.2p20 apt Go 1.20.3 apt rustc 1.67.1 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.25.1 apt make 4.3 apt automake 1.16.5 apt gfortran 13.1.0 apt openssh-server 9.0p1 apt","title":"J-VF1-x"},{"location":"tooling_J-VF1-x/#tools-on-j-vf1-x-node","text":"This compute instance is Starfive's VisionFive 1 board and it has available packages ONLY for RISC-V architecture. Operating System: Ubuntu 23.04 (Lunar Lobster) Tool Version Installed from Git 2.39.2 apt OpenJDK 21-ea apt GCC 13.1.0 apt Python3 3.11.4 apt OpenSSL 3.0.8 apt Ruby 3.1.2p20 apt Go 1.20.3 apt rustc 1.67.1 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.25.1 apt make 4.3 apt automake 1.16.5 apt gfortran 13.1.0 apt openssh-server 9.0p1 apt","title":"Tools on J-VF1-x node"},{"location":"tooling_J-VF2-x/","text":"Tools on J-VF2-x node This compute instance is Starfive's VisionFive 2 board and it has available packages ONLY for RISC-V architecture. Operating System: Debian 12 (Bookworm) Tool Version Installed from Git 2.43.0 apt OpenJDK 21.0.2 apt GCC 13.2.0 apt Python3 3.11.7 apt OpenSSL 3.1.5 apt Ruby 3.1.2p20 apt Go 1.22.0 apt rustc 1.70.0 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.28.3 apt make 4.3 apt automake 1.16.5 apt gfortran 13.2.0 apt openssh-server 9.6p1 apt vim 9.1 apt","title":"J-VF2-x"},{"location":"tooling_J-VF2-x/#tools-on-j-vf2-x-node","text":"This compute instance is Starfive's VisionFive 2 board and it has available packages ONLY for RISC-V architecture. Operating System: Debian 12 (Bookworm) Tool Version Installed from Git 2.43.0 apt OpenJDK 21.0.2 apt GCC 13.2.0 apt Python3 3.11.7 apt OpenSSL 3.1.5 apt Ruby 3.1.2p20 apt Go 1.22.0 apt rustc 1.70.0 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.28.3 apt make 4.3 apt automake 1.16.5 apt gfortran 13.2.0 apt openssh-server 9.6p1 apt vim 9.1 apt","title":"Tools on J-VF2-x node"}]}