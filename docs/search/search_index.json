{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RISC-V CI Documentation On this site, you will find all the documentation regarding Jenkins, LXC containers and QEMU configured to use with RISC-V cloud infrastructure","title":"Home"},{"location":"#risc-v-ci-documentation","text":"On this site, you will find all the documentation regarding Jenkins, LXC containers and QEMU configured to use with RISC-V cloud infrastructure","title":"RISC-V CI Documentation"},{"location":"Adding_Custom-build_packages_in_Jenkins/","text":"Adding Custom-build packages in Jenkins __NOTE: For this documentation, Ubuntu 22.04 and Jenkins version 2.371 is used. While working open-source, many times one may need to build the package from source code. While working on the local machine, the built package may be accessible by adding it to the $PATH environment variable. But while working with Jenkins, This may not work. Where to install package for Jenkins job In Jenkins job, the package is installed on the node (or agent) on which the specific job is being used and NOT on the node where the jenkins controller node is installed. For example if the Jenkins controller node is on computer1 and slave node on which the job is destined to run is on computer2 then computer2 should have all the packages used by the job and not the computer1 . Adding the package to Jenkins Considering there is a node with name Runner1 in jenkins on which the job is destined to run and that job uses a toolchain riscv64-unknown-elf-gcc to run properly and that toolchain is present in directory /home/runner1/path_to_install/bin , then the following procedure is used to add the toolchain in Jenkins. Go to Dashboard > Manage Jenkins > Nodes > Runner1 . Scroll down and check Environment variables . Under section List of variables add: Name as PATH Value as $PATH:/home/runner1/path_to_install/bin . Click on Save . After this, the package should be available to be used.","title":"Adding custom-build packages in Jenkins"},{"location":"Adding_Custom-build_packages_in_Jenkins/#adding-custom-build-packages-in-jenkins","text":"__NOTE: For this documentation, Ubuntu 22.04 and Jenkins version 2.371 is used. While working open-source, many times one may need to build the package from source code. While working on the local machine, the built package may be accessible by adding it to the $PATH environment variable. But while working with Jenkins, This may not work.","title":"Adding Custom-build packages in Jenkins"},{"location":"Adding_Custom-build_packages_in_Jenkins/#where-to-install-package-for-jenkins-job","text":"In Jenkins job, the package is installed on the node (or agent) on which the specific job is being used and NOT on the node where the jenkins controller node is installed. For example if the Jenkins controller node is on computer1 and slave node on which the job is destined to run is on computer2 then computer2 should have all the packages used by the job and not the computer1 .","title":"Where to install package for Jenkins job"},{"location":"Adding_Custom-build_packages_in_Jenkins/#adding-the-package-to-jenkins","text":"Considering there is a node with name Runner1 in jenkins on which the job is destined to run and that job uses a toolchain riscv64-unknown-elf-gcc to run properly and that toolchain is present in directory /home/runner1/path_to_install/bin , then the following procedure is used to add the toolchain in Jenkins. Go to Dashboard > Manage Jenkins > Nodes > Runner1 . Scroll down and check Environment variables . Under section List of variables add: Name as PATH Value as $PATH:/home/runner1/path_to_install/bin . Click on Save . After this, the package should be available to be used.","title":"Adding the package to Jenkins"},{"location":"Booting_ubuntu22.04_riscv64/","text":"Booting RISC-V Ubuntu 22.04 on qemu-system-riscv64 Pre-requisites Following are the pre-requisites which are needed to be installed before booting ubuntu 22.04 for RISC-V on qemu . Note: Make sure RISC-V GNU Toolchain is installed before proceeding U-boot Qemu version 7.0 or greater with networking Ubuntu 22.04 pre-built image for RISC-V 1. Installing U-boot Note: If you plan on installing U-boot from apt or some system repository, install the one which comes with ubuntu 22.04. Older will not work with this process. Also, be sure to checkout a latest Stable version instead of development version. Get source code of u-boot and checkout stable version using commands below. git clone https://github.com/qemu/u-boot.git cd u-boot git checkout v2022.10 Generate configurations for supervisor mode with following command. make qemu-riscv64_smode_defconfig CROSS_COMPILE=riscv64-unknown-linux-gnu- Execute following command to start build process. make CROSS_COMPILE=riscv64-unknown-linux-gnu- This should install u-boot.bin in the source directory. This file will be used later so its path must be kept remember. Here it will be refered to as $UBOOTPATH . 2. Installing Qemu Qemu version 7.0 or greater should be installed with networking for ubuntu 22.04 to work. See Installing Qemu for RISC-V for instructions on installing qemu-system-riscv64 . 3. Getting Ubuntu 22.04 pre-build image for RISC-V Ubuntu 22.04 image can be downloaded from https://ubuntu.com/download/risc-v . Booting Ubuntu 22.04 Image on qemu In the directory where ubuntu 22.04 image is present, execute following command to boot into ubuntu 22.04 with qemu-system-riscv64 . If you need more space, you can use following command. qemu-img resize -f raw ubuntu-22.04.1-preinstalled-server-riscv64+unmatched.img +5G This will increase storage size of the image by 5GB. qemu-system-riscv64 \\ -machine virt -nographic -m 2048 -smp 4 \\ -kernel $UBOOTPATH/u-boot.bin \\ -device virtio-net-device,netdev=eth0 -netdev user,id=eth0,hostfwd=::2222-:22 \\ -drive file=ubuntu-22.04.1-preinstalled-server-riscv64+unmatched.img,format=raw,if=virtio Here -m is the memory in Megabytes and -smp is number of cores. -nographic means qemu will use same terminal instance instead of opening a new window of its own (which is beneficial while running servers without gui). Whereas hostfwd=::2222-:22 will forward traffic going to port 2222 to port 22 . Due to this port 2222 will be used to access ssh on qemu machine. This should boot ubuntu 22.04. But it will take a while on first start. On start, credentials will be as follows. Username: ubuntu Password: ubuntu After entering credentials, terminal will prompt for change of password after which ubuntu will be ready to use.","title":"Booting RISC-V Ubuntu 22.04 on `qemu-system-riscv64`"},{"location":"Booting_ubuntu22.04_riscv64/#booting-risc-v-ubuntu-2204-on-qemu-system-riscv64","text":"","title":"Booting RISC-V Ubuntu 22.04 on qemu-system-riscv64"},{"location":"Booting_ubuntu22.04_riscv64/#pre-requisites","text":"Following are the pre-requisites which are needed to be installed before booting ubuntu 22.04 for RISC-V on qemu . Note: Make sure RISC-V GNU Toolchain is installed before proceeding U-boot Qemu version 7.0 or greater with networking Ubuntu 22.04 pre-built image for RISC-V","title":"Pre-requisites"},{"location":"Booting_ubuntu22.04_riscv64/#1-installing-u-boot","text":"Note: If you plan on installing U-boot from apt or some system repository, install the one which comes with ubuntu 22.04. Older will not work with this process. Also, be sure to checkout a latest Stable version instead of development version. Get source code of u-boot and checkout stable version using commands below. git clone https://github.com/qemu/u-boot.git cd u-boot git checkout v2022.10 Generate configurations for supervisor mode with following command. make qemu-riscv64_smode_defconfig CROSS_COMPILE=riscv64-unknown-linux-gnu- Execute following command to start build process. make CROSS_COMPILE=riscv64-unknown-linux-gnu- This should install u-boot.bin in the source directory. This file will be used later so its path must be kept remember. Here it will be refered to as $UBOOTPATH .","title":"1. Installing U-boot"},{"location":"Booting_ubuntu22.04_riscv64/#2-installing-qemu","text":"Qemu version 7.0 or greater should be installed with networking for ubuntu 22.04 to work. See Installing Qemu for RISC-V for instructions on installing qemu-system-riscv64 .","title":"2. Installing Qemu"},{"location":"Booting_ubuntu22.04_riscv64/#3-getting-ubuntu-2204-pre-build-image-for-risc-v","text":"Ubuntu 22.04 image can be downloaded from https://ubuntu.com/download/risc-v .","title":"3. Getting Ubuntu 22.04 pre-build image for RISC-V"},{"location":"Booting_ubuntu22.04_riscv64/#booting-ubuntu-2204-image-on-qemu","text":"In the directory where ubuntu 22.04 image is present, execute following command to boot into ubuntu 22.04 with qemu-system-riscv64 . If you need more space, you can use following command. qemu-img resize -f raw ubuntu-22.04.1-preinstalled-server-riscv64+unmatched.img +5G This will increase storage size of the image by 5GB. qemu-system-riscv64 \\ -machine virt -nographic -m 2048 -smp 4 \\ -kernel $UBOOTPATH/u-boot.bin \\ -device virtio-net-device,netdev=eth0 -netdev user,id=eth0,hostfwd=::2222-:22 \\ -drive file=ubuntu-22.04.1-preinstalled-server-riscv64+unmatched.img,format=raw,if=virtio Here -m is the memory in Megabytes and -smp is number of cores. -nographic means qemu will use same terminal instance instead of opening a new window of its own (which is beneficial while running servers without gui). Whereas hostfwd=::2222-:22 will forward traffic going to port 2222 to port 22 . Due to this port 2222 will be used to access ssh on qemu machine. This should boot ubuntu 22.04. But it will take a while on first start. On start, credentials will be as follows. Username: ubuntu Password: ubuntu After entering credentials, terminal will prompt for change of password after which ubuntu will be ready to use.","title":"Booting Ubuntu 22.04 Image on qemu"},{"location":"Building_Jenkins_github_repo/","text":"Building Github Repository of Jenkins Linux kernel version, distribution and release at the time of build process Linux Kernel : 5.15.0-46-generic (can be checked using uname -r in ubuntu) Distribution : Ubuntu Release : focal (20.04); also works without issue on ubuntu 22.04 (release can be checked using lsb_release -a in ubuntu) Cloning the github repository First clone the repository using the command below (here it is assumed to be cloned at user's home directory: ~/) git clone https://github.com/jenkinsci/jenkins.git Resolving the dependencies After cloning the repository at ~/, the file CONTRIBUTING.md should be available in ~/jenkins/. This file contains all the information to resolve the dependencies and building the repository. Some notable dependencies are Java Development Kit (JDK), Apache Maven (latest version will be preferable) and git. Running Following command will resolve the mentioned dependencies. sudo apt update && sudo apt install default-jdk default-jre maven git -y Building Jenkins using maven on linux For having the jenkins build up as fast as possible, following command can be used in ~/jenkins/. mvn -am -pl war,bom -Pquick-build clean install Executing Jenkins After the above commands successfully completes execution, jenkins.war should be present in ~/jenkins/war/target and can be executed to run at port 8080 on localhost using following command. java -jar ~/jenkins/war/target/jenkins.war --httpPort=8080 #Considering jenkins repo is cloned at ~/ After this process jenkins UI can be accessed using http://localhost:8080 in a browser and a password will be shown on terminal to login to jenkins first time After this, Jenkins UI will go through a very simple post installation process which one can configure according to his needs.","title":"Building Jenkins"},{"location":"Building_Jenkins_github_repo/#building-github-repository-of-jenkins","text":"","title":"Building Github Repository of Jenkins"},{"location":"Building_Jenkins_github_repo/#linux-kernel-version-distribution-and-release-at-the-time-of-build-process","text":"Linux Kernel : 5.15.0-46-generic (can be checked using uname -r in ubuntu) Distribution : Ubuntu Release : focal (20.04); also works without issue on ubuntu 22.04 (release can be checked using lsb_release -a in ubuntu)","title":"Linux kernel version, distribution and release at the time of build process"},{"location":"Building_Jenkins_github_repo/#cloning-the-github-repository","text":"First clone the repository using the command below (here it is assumed to be cloned at user's home directory: ~/) git clone https://github.com/jenkinsci/jenkins.git","title":"Cloning the github repository"},{"location":"Building_Jenkins_github_repo/#resolving-the-dependencies","text":"After cloning the repository at ~/, the file CONTRIBUTING.md should be available in ~/jenkins/. This file contains all the information to resolve the dependencies and building the repository. Some notable dependencies are Java Development Kit (JDK), Apache Maven (latest version will be preferable) and git. Running Following command will resolve the mentioned dependencies. sudo apt update && sudo apt install default-jdk default-jre maven git -y","title":"Resolving the dependencies"},{"location":"Building_Jenkins_github_repo/#building-jenkins-using-maven-on-linux","text":"For having the jenkins build up as fast as possible, following command can be used in ~/jenkins/. mvn -am -pl war,bom -Pquick-build clean install","title":"Building Jenkins using maven on linux"},{"location":"Building_Jenkins_github_repo/#executing-jenkins","text":"After the above commands successfully completes execution, jenkins.war should be present in ~/jenkins/war/target and can be executed to run at port 8080 on localhost using following command. java -jar ~/jenkins/war/target/jenkins.war --httpPort=8080 #Considering jenkins repo is cloned at ~/ After this process jenkins UI can be accessed using http://localhost:8080 in a browser and a password will be shown on terminal to login to jenkins first time After this, Jenkins UI will go through a very simple post installation process which one can configure according to his needs.","title":"Executing Jenkins"},{"location":"Building_Linux_Kernel/","text":"Building a RISCV Linux kernel and booting it in QEMU inside LXC container Reference links : https://risc-v-getting-started-guide.readthedocs.io/en/latest/linux-qemu.html https://hackernoon.com/how-to-set-up-the-environment-for-riscv-64-linux-kernel-development-in-ubuntu-2004-si5p35kv This documentation covers how to build a linux kernel with RISCV linux toolchain inside an un-privileged LXC container and then boot it on qemu. Doing this process on privileged lxc container makes life easier, but privileged containers always have security loop holes. For instance, their root id is mapped to root id of host machine. On the other hand un-privileged containers are the safest (see link ). Machine's and LXC Container's Operating System Specifications At the time of creating this documentation, following is the specification of host machine and operating system. Host Machine: Ubuntu focal (20.04) 64-bit. LXC Container: Ubuntu jammy (22.04) 64-bit. LXC Container is unprivileged with non-sudo user. NOTE: Throughout this documentation, name of the lxc container will be qemu_container or qemucontainer with non-sudo user as qemu-user which is running on ubuntu 22.04 and host machine x86. Do not confuse the name with assumption that it is booting on qemu emulator. It is just a naming convention. Pre-requisites Following programs may also have their own pre-requisites. Git: For cloning repositories of following programs. Install it with [sudo] apt install git . TMUX: For convenience of multiple terminals. Install it using [sudo] apt install tmux RISCV GNU toolchain built as linux: For compiling the linux kernel. Busybox: For Generating the binaries for linux kernel boot. QEMU Emulator: For booting the linux kernel Linux Kernel (latest version which is used at point of writing this documentation is 6.0.0 ) The working directory inside lxc container for all of this documentation will be ~/riscv-linux or /home/qemu-user/riscv-linux . NOTE: Busybox will not be built inside the lxc container rather it will be built in (any) host linux machine with sudo privileges. 3. RISCV GNU Toolchain Log in the lxc container to the non-sudo user (here SSH is used to log in). Install the prerequisites for building RISCV GNU TOOLCHAIN inside lxc container with root user using command below. apt-get install autoconf automake autotools-dev curl python3 libmpc-dev libmpfr-dev libgmp-dev gawk build-essential bison flex texinfo gperf libtool patchutils bc zlib1g-dev libexpat-dev libncurses-dev Clone the GNU toolchain using the command below. git clone https://github.com/riscv-collab/riscv-gnu-toolchain.git Create a directory in which the RISCV GNU toolchain is desired to be installed (here it will be /home/qemu-user/riscv-linux/riscv-gnu-installed ) Execute following command inside cloned repository with --prefix as the absolute directory path to where the RISCV toolchain is to be installed ./configure --prefix=/home/qemu-user/riscv-linux/riscv-gnu-installed Execute following inside cloned repository (execution of this command will take a while to complete) make linux -j$(nproc) # 'nproc' is the command used to determine the number of processors in machine so that 'make' can use parallelism. After the execution of command is complete, add bin directory created inside the riscv installation directory to the $PATH and add the expression to .bashrc . According to this documentation, following expression will be added to .bashrc . PATH=\"/home/qemu-user/riscv-linux/riscv-gnu-installed/bin:$PATH\" Check if the toolchain is installed by executing following commands. exec $SHELL riscv64-unknown-linux-gnu-gcc Expected Output: fatal error: no input files compilation terminated. Now RISCV linux toolchain is ready ! 4. Busybox Busybox is the package for creating linux binutils and set of directories for linux to boot into. Busybox will be installed in host machine instead of lxc container. The reason for this is linux kernel requires block oriented device and character oriented device for it to boot. Those devices can be created using mknod command which can only be created inside a host machine with sudo privileges. Busybox will be used in creating initial ram disk file (in gz format) which is used to boot kernel. It does not matter at which operating system or on which machine this file is created. But the machine on which it is compiled, must also have RISCV GNU toolchain installed above. Clone Busybox using the command below. git clone https://git.busybox.net/busybox Navigate to cloned directory. cd busybox Before building busybox, we need to produce a configuration (.config file) for busybox. It is better to apply default configurations and then change only those which are desired. make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- defconfig After the command is executed, a .config file will be present in the busybox cloned directory. Now, an additional option is to be enabled which enables busybox to build the libraries in the executable instead of separate shared libraries. For this purpose, execute the following command to access the configuration menu. Then go to Settings menu by pressing enter and from there, enable [ ] Build static binary (no shared libs) by pressing space. After the option is enabled, exit by pressing esc twice two times and press yes to the prompt about file saving. make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- menuconfig Now that configuration is complete, build busybox by executing following command. make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- -j$(nproc) Execute following command which will produce all the basic linux utilities in _install directory in busybox repo directory. make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- -j$(nproc) install Navigate to _install directory and create dev directory. cd _install mkdir dev Now, a linux console and a ram devices are to be created inside dev directory. A fact to understand here is that, every device is a file in linux but they are special kind of files. A detail of these devices can be found here . In this documentation mknod command is used to create these devices. sudo mknod dev/console c 5 1 sudo mknod dev/ram b 1 0 After executing above commands, following files with names ram and console will be created as shown in the image below. Now an init file is needed because linux kernel does not boot itself, it rather searches for init file in the directories (read linux kernel messages during build procedure). init file contains commands to mount some directories during boot (more information can be found here ). In busybox/_install , create a file with following contents (be sure to make it executable with chmod +x init ). vim init #!/bin/sh echo \"### INIT SCRIPT ###\" mkdir /proc /sys /tmp mount -t proc none /proc #For processes mount -t sysfs none /sys #For all the devices on the machine mount -t tmpfs none /tmp #For virtual memory echo -e \"\\nThis boot took $(cut -d' ' -f1 /proc/uptime) seconds\\n\" exec /bin/sh Now that all the files are ready for linux to boot into, pack them in cpio format and then to gz format. It is because cpio format is in \"Early userspace support\" in linux kernel (see link ) whereas gz format is needed because it is one of the formats needed by qemu emulator. Following command produces a qemu-compatible initramfs file for linux kernel to boot in. find -print0 | cpio -0oH newc | gzip -9 > ../initramfs.cpio.gz Command Details: find -print0 separates the file names with null character cpio -0oH newc produces an archive file in newc format gzip -9 creates a gz format zip file. -9 represents the best compression level at the slowest speed ../initramfs.cpio.gz represents the output file which is created in parent directory to present working directory. At this point, our work with busybox is done. Copy the produced file into the lxc container (use scp command if it is on ssh). 5. QEMU Emulator Install the pre-requisites of qemu emulator on lxc container with root user with following command (see link ). apt-get install git libglib2.0-dev libfdt-dev libpixman-1-dev zlib1g-dev ninja-build Clone the QEMU Emulator repository using the command below with root user: git clone https://github.com/qemu/qemu.git Build QEMU for RISCV with root user using commands below (see link ). ./configure --target-list=riscv64-softmmu make -j$(nproc) For system-wide installation of QEMU Emulator, run following command with root user. make -j$(nproc) install After execution of this command, work with QEMU Emulator is done and its commands can be accessed anywhere. 6. Linux Kernel Clone the linux kernel from Linus Torvalds' repository using the command below. git clone https://github.com/torvalds/linux.git Navigate to cloned repository. cd linux Building Linux kernel with riscv64-unknown-linux-gnu-gcc Before building linux kernel with RISCV toolchain a configuration file (.config) must be produced in its directory. First, produce a file with default configurations, then change configurations according to needs. make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- defconfig make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- menuconfig Above command will open configuration menu. Enter General setup , scroll down and enable Initial RAM filesystem and RAM disk (initramfs/initrd) support using Space key. Then enter () Initramfs source file(s) and here put the absolute path to initramfs.cpio.gz file which was just created using busybox. For this documentation, it is /home/qemu-user/riscv-linux/initramfs.cpio.gz . Double-press esc and save the file. Now linux kernel is ready to be compiled with riscv64-linux-gnu-gcc toolchain. So execute the command below. make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- -j$(nproc) If the above command successfully executed without any errors, there must be Kernel: arch/riscv/boot/Image.gz is ready printed on terminal. On newer linux kernels, it might be scrolled up a little, use Ctrl-shift-f to find it. Booting into the linux kernel using QEMU emulator Now that everything is ready, execute the following command in linux/arch/riscv/boot/ directory to boot linux on sifive's unleashed . qemu-system-riscv64 -kernel Image -machine sifive_u -nographic Command Details qemu-system-riscv64 is the qemu built for riscv64 -kernel takes the image produced by linux kernel compilation and is present in linux/arch/riscv/boot/ directory. -machine takes one of the machine names as arguments available in qemu-system-riscv64 . Available machines can be listed on terminal using command qemu-system-riscv64 -machine help . -nographic restricts the use of GUI (which is a better option considering lxc container does not support gtk initialization). If everything goes on right, the kernel will boot successfully as shown in the picture below.","title":"Building Linux kernel and booting on QEMU"},{"location":"Building_Linux_Kernel/#building-a-riscv-linux-kernel-and-booting-it-in-qemu-inside-lxc-container","text":"Reference links : https://risc-v-getting-started-guide.readthedocs.io/en/latest/linux-qemu.html https://hackernoon.com/how-to-set-up-the-environment-for-riscv-64-linux-kernel-development-in-ubuntu-2004-si5p35kv This documentation covers how to build a linux kernel with RISCV linux toolchain inside an un-privileged LXC container and then boot it on qemu. Doing this process on privileged lxc container makes life easier, but privileged containers always have security loop holes. For instance, their root id is mapped to root id of host machine. On the other hand un-privileged containers are the safest (see link ).","title":"Building a RISCV Linux kernel and booting it in QEMU inside LXC container"},{"location":"Building_Linux_Kernel/#machines-and-lxc-containers-operating-system-specifications","text":"At the time of creating this documentation, following is the specification of host machine and operating system. Host Machine: Ubuntu focal (20.04) 64-bit. LXC Container: Ubuntu jammy (22.04) 64-bit. LXC Container is unprivileged with non-sudo user. NOTE: Throughout this documentation, name of the lxc container will be qemu_container or qemucontainer with non-sudo user as qemu-user which is running on ubuntu 22.04 and host machine x86. Do not confuse the name with assumption that it is booting on qemu emulator. It is just a naming convention.","title":"Machine's and LXC Container's Operating System Specifications"},{"location":"Building_Linux_Kernel/#pre-requisites","text":"Following programs may also have their own pre-requisites. Git: For cloning repositories of following programs. Install it with [sudo] apt install git . TMUX: For convenience of multiple terminals. Install it using [sudo] apt install tmux RISCV GNU toolchain built as linux: For compiling the linux kernel. Busybox: For Generating the binaries for linux kernel boot. QEMU Emulator: For booting the linux kernel Linux Kernel (latest version which is used at point of writing this documentation is 6.0.0 ) The working directory inside lxc container for all of this documentation will be ~/riscv-linux or /home/qemu-user/riscv-linux . NOTE: Busybox will not be built inside the lxc container rather it will be built in (any) host linux machine with sudo privileges.","title":"Pre-requisites"},{"location":"Building_Linux_Kernel/#3-riscv-gnu-toolchain","text":"Log in the lxc container to the non-sudo user (here SSH is used to log in). Install the prerequisites for building RISCV GNU TOOLCHAIN inside lxc container with root user using command below. apt-get install autoconf automake autotools-dev curl python3 libmpc-dev libmpfr-dev libgmp-dev gawk build-essential bison flex texinfo gperf libtool patchutils bc zlib1g-dev libexpat-dev libncurses-dev Clone the GNU toolchain using the command below. git clone https://github.com/riscv-collab/riscv-gnu-toolchain.git Create a directory in which the RISCV GNU toolchain is desired to be installed (here it will be /home/qemu-user/riscv-linux/riscv-gnu-installed ) Execute following command inside cloned repository with --prefix as the absolute directory path to where the RISCV toolchain is to be installed ./configure --prefix=/home/qemu-user/riscv-linux/riscv-gnu-installed Execute following inside cloned repository (execution of this command will take a while to complete) make linux -j$(nproc) # 'nproc' is the command used to determine the number of processors in machine so that 'make' can use parallelism. After the execution of command is complete, add bin directory created inside the riscv installation directory to the $PATH and add the expression to .bashrc . According to this documentation, following expression will be added to .bashrc . PATH=\"/home/qemu-user/riscv-linux/riscv-gnu-installed/bin:$PATH\" Check if the toolchain is installed by executing following commands. exec $SHELL riscv64-unknown-linux-gnu-gcc Expected Output: fatal error: no input files compilation terminated. Now RISCV linux toolchain is ready !","title":"3. RISCV GNU Toolchain"},{"location":"Building_Linux_Kernel/#4-busybox","text":"Busybox is the package for creating linux binutils and set of directories for linux to boot into. Busybox will be installed in host machine instead of lxc container. The reason for this is linux kernel requires block oriented device and character oriented device for it to boot. Those devices can be created using mknod command which can only be created inside a host machine with sudo privileges. Busybox will be used in creating initial ram disk file (in gz format) which is used to boot kernel. It does not matter at which operating system or on which machine this file is created. But the machine on which it is compiled, must also have RISCV GNU toolchain installed above. Clone Busybox using the command below. git clone https://git.busybox.net/busybox Navigate to cloned directory. cd busybox Before building busybox, we need to produce a configuration (.config file) for busybox. It is better to apply default configurations and then change only those which are desired. make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- defconfig After the command is executed, a .config file will be present in the busybox cloned directory. Now, an additional option is to be enabled which enables busybox to build the libraries in the executable instead of separate shared libraries. For this purpose, execute the following command to access the configuration menu. Then go to Settings menu by pressing enter and from there, enable [ ] Build static binary (no shared libs) by pressing space. After the option is enabled, exit by pressing esc twice two times and press yes to the prompt about file saving. make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- menuconfig Now that configuration is complete, build busybox by executing following command. make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- -j$(nproc) Execute following command which will produce all the basic linux utilities in _install directory in busybox repo directory. make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- -j$(nproc) install Navigate to _install directory and create dev directory. cd _install mkdir dev Now, a linux console and a ram devices are to be created inside dev directory. A fact to understand here is that, every device is a file in linux but they are special kind of files. A detail of these devices can be found here . In this documentation mknod command is used to create these devices. sudo mknod dev/console c 5 1 sudo mknod dev/ram b 1 0 After executing above commands, following files with names ram and console will be created as shown in the image below. Now an init file is needed because linux kernel does not boot itself, it rather searches for init file in the directories (read linux kernel messages during build procedure). init file contains commands to mount some directories during boot (more information can be found here ). In busybox/_install , create a file with following contents (be sure to make it executable with chmod +x init ). vim init #!/bin/sh echo \"### INIT SCRIPT ###\" mkdir /proc /sys /tmp mount -t proc none /proc #For processes mount -t sysfs none /sys #For all the devices on the machine mount -t tmpfs none /tmp #For virtual memory echo -e \"\\nThis boot took $(cut -d' ' -f1 /proc/uptime) seconds\\n\" exec /bin/sh Now that all the files are ready for linux to boot into, pack them in cpio format and then to gz format. It is because cpio format is in \"Early userspace support\" in linux kernel (see link ) whereas gz format is needed because it is one of the formats needed by qemu emulator. Following command produces a qemu-compatible initramfs file for linux kernel to boot in. find -print0 | cpio -0oH newc | gzip -9 > ../initramfs.cpio.gz Command Details: find -print0 separates the file names with null character cpio -0oH newc produces an archive file in newc format gzip -9 creates a gz format zip file. -9 represents the best compression level at the slowest speed ../initramfs.cpio.gz represents the output file which is created in parent directory to present working directory. At this point, our work with busybox is done. Copy the produced file into the lxc container (use scp command if it is on ssh).","title":"4. Busybox"},{"location":"Building_Linux_Kernel/#5-qemu-emulator","text":"Install the pre-requisites of qemu emulator on lxc container with root user with following command (see link ). apt-get install git libglib2.0-dev libfdt-dev libpixman-1-dev zlib1g-dev ninja-build Clone the QEMU Emulator repository using the command below with root user: git clone https://github.com/qemu/qemu.git Build QEMU for RISCV with root user using commands below (see link ). ./configure --target-list=riscv64-softmmu make -j$(nproc) For system-wide installation of QEMU Emulator, run following command with root user. make -j$(nproc) install After execution of this command, work with QEMU Emulator is done and its commands can be accessed anywhere.","title":"5. QEMU Emulator"},{"location":"Building_Linux_Kernel/#6-linux-kernel","text":"Clone the linux kernel from Linus Torvalds' repository using the command below. git clone https://github.com/torvalds/linux.git Navigate to cloned repository. cd linux","title":"6. Linux Kernel"},{"location":"Building_Linux_Kernel/#building-linux-kernel-with-riscv64-unknown-linux-gnu-gcc","text":"Before building linux kernel with RISCV toolchain a configuration file (.config) must be produced in its directory. First, produce a file with default configurations, then change configurations according to needs. make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- defconfig make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- menuconfig Above command will open configuration menu. Enter General setup , scroll down and enable Initial RAM filesystem and RAM disk (initramfs/initrd) support using Space key. Then enter () Initramfs source file(s) and here put the absolute path to initramfs.cpio.gz file which was just created using busybox. For this documentation, it is /home/qemu-user/riscv-linux/initramfs.cpio.gz . Double-press esc and save the file. Now linux kernel is ready to be compiled with riscv64-linux-gnu-gcc toolchain. So execute the command below. make ARCH=riscv CROSS_COMPILE=riscv64-unknown-linux-gnu- -j$(nproc) If the above command successfully executed without any errors, there must be Kernel: arch/riscv/boot/Image.gz is ready printed on terminal. On newer linux kernels, it might be scrolled up a little, use Ctrl-shift-f to find it.","title":"Building Linux kernel with riscv64-unknown-linux-gnu-gcc"},{"location":"Building_Linux_Kernel/#booting-into-the-linux-kernel-using-qemu-emulator","text":"Now that everything is ready, execute the following command in linux/arch/riscv/boot/ directory to boot linux on sifive's unleashed . qemu-system-riscv64 -kernel Image -machine sifive_u -nographic Command Details qemu-system-riscv64 is the qemu built for riscv64 -kernel takes the image produced by linux kernel compilation and is present in linux/arch/riscv/boot/ directory. -machine takes one of the machine names as arguments available in qemu-system-riscv64 . Available machines can be listed on terminal using command qemu-system-riscv64 -machine help . -nographic restricts the use of GUI (which is a better option considering lxc container does not support gtk initialization). If everything goes on right, the kernel will boot successfully as shown in the picture below.","title":"Booting into the linux kernel using QEMU emulator"},{"location":"Building_qemu/","text":"Installing QEMU for emulating riscv64 QEMU is an open-source emulator. It can be used to emulate different architectures on a single machine. In RISC-V CI there are various programs which run on RISC-V architecture. But instead of porting them to a dedicated board of riscv architecture, they can be run readily on qemu emulator. Here two types of QEMU emulators will be used for RISC-V applications: qemu-system-riscv64: It can be used to load a complete linux operating system image. qemu-riscv64: It can be used to execute program's binary directly without need of a complete operating system. Installing Pre-requisites Execute the following command to install the pre-requisites for installing qemu on ubuntu 22.04 (jammy) sudo apt-get install meson git libglib2.0-dev libfdt-dev libpixman-1-dev zlib1g-dev ninja-build qemu-slirp is important for enabling user-level networking with qemu-system-riscv64 qemu-system-riscv64 while loading image of server installation of ubuntu. So it needs to be installed first. Get the source code of qemu-slirp using following command: git clone https://github.com/openSUSE/qemu-slirp.git Then Execute following commands to install slirp in meson, which can later be used by qemu during build. meson build ninja -C build install Note: Make sure you have riscv64-unknown-linux-gnu toolchain installed for compiling program and executing them on qemu later. There are some optional dependencies which one can download, but they are actually not needed for installing qemu and make it work. Installing qemu-system-riscv64 What is qemu-system-riscv64 qemu-system-riscv64 is qemu executable program. It can load a complete linux distribution. It cannot take program's executable binary as argument and run it without a dedicated linux distribution. Installing qemu-system-riscv64 on ubuntu Get source code of qemu from github using the command below git clone https://github.com/qemu/qemu.git Configure qemu for riscv64-softmmu with following command (replace $PREFIX with valid location of installation). ./configure --prefix=$PREFIX --target-list=riscv64-linux-user,riscv64-softmmu --enable-slirp Execute following command to start the build. make Execute following command to install the binaries at $PREFIX location. make install Note: After the installation with slirp following error can be encountered on some systems . qemu-system-riscv64: symbol lookup error: qemu-system-riscv64: undefined symbol: slirp_new, version SLIRP_4.0 Solution: This can be solved by executing following command in source directory of qemu (which is cloned from github). [sudo] ldconfig Testing qemu-system-riscv64 qemu-system-riscv64 can only be tested by booting a linux operating system. See Booting RISC-V Ubuntu 22.04 on qemu-system-riscv64 . Installing qemu-riscv64 What is qemu-riscv64 qemu-riscv64 is qemu executable program. But instead of porting a complete operating system (like qemu-system-riscv64 ), it can readily execute binaries. Throughout cross-compiling section, qemu-riscv64 will be used with linux-user and executable of every program can be tested on qemu (e.g. python, ruby etc.). Installing qemu-riscv64 on ubuntu 22.04 Get source code of qemu using the command below git clone https://github.com/qemu/qemu.git Use following command in the root directory of repository to configure qemu for riscv64-linux-user ./configure --target-list=riscv64-linux-user --prefix=$PREFIX # Replace $PREFIX with a valid location to install at Note: If this is not your architecture/platform, you can see a list of available platform/architecture by executing following command. ./configure --help Use the following command to start the build process make -j$(nproc) After the builld is complete without any error, use the following command to install binaries at $PREFIX location make install Add the $PREFIX/bin to $PATH variable so that it may be recognized as a command. Using this method causes the qemu-riscv64 to have an issue with sysroot. It starts searching for libraries in the root folder of the machine which is based on x86_64-linux-gnu . A simple workaround is to give path of the sysroot/ folder where riscv64-unknown-linux-gnu toolchain is installed. Here that directory will be denoted as $RISCV_SYSROOT Testing qemu-riscv64 Create a C file in your favorite editor or by using the commands below: echo \"#include<stdio.h>\" > helloworld.c echo \"int main(){\" >> helloworld.c echo \"printf(\"Hello World !\");\" >> helloworld.c echo '}' >> helloworld.c Execute following command to compile C program with riscv gnu toolchain riscv64-unknown-linux-gnu-gcc helloworld.c -o helloworld Execute following command to execute the compiled binary on qemu-riscv64 qemu-riscv64 -L $RISCV_SYSROOT ./helloworld If everything went right, following output will be shown. Hello World !","title":"Installing Qemu for RISC-V"},{"location":"Building_qemu/#installing-qemu-for-emulating-riscv64","text":"QEMU is an open-source emulator. It can be used to emulate different architectures on a single machine. In RISC-V CI there are various programs which run on RISC-V architecture. But instead of porting them to a dedicated board of riscv architecture, they can be run readily on qemu emulator. Here two types of QEMU emulators will be used for RISC-V applications: qemu-system-riscv64: It can be used to load a complete linux operating system image. qemu-riscv64: It can be used to execute program's binary directly without need of a complete operating system.","title":"Installing QEMU for emulating riscv64"},{"location":"Building_qemu/#installing-pre-requisites","text":"Execute the following command to install the pre-requisites for installing qemu on ubuntu 22.04 (jammy) sudo apt-get install meson git libglib2.0-dev libfdt-dev libpixman-1-dev zlib1g-dev ninja-build qemu-slirp is important for enabling user-level networking with qemu-system-riscv64 qemu-system-riscv64 while loading image of server installation of ubuntu. So it needs to be installed first. Get the source code of qemu-slirp using following command: git clone https://github.com/openSUSE/qemu-slirp.git Then Execute following commands to install slirp in meson, which can later be used by qemu during build. meson build ninja -C build install Note: Make sure you have riscv64-unknown-linux-gnu toolchain installed for compiling program and executing them on qemu later. There are some optional dependencies which one can download, but they are actually not needed for installing qemu and make it work.","title":"Installing Pre-requisites"},{"location":"Building_qemu/#installing-qemu-system-riscv64","text":"","title":"Installing qemu-system-riscv64"},{"location":"Building_qemu/#what-is-qemu-system-riscv64","text":"qemu-system-riscv64 is qemu executable program. It can load a complete linux distribution. It cannot take program's executable binary as argument and run it without a dedicated linux distribution.","title":"What is qemu-system-riscv64"},{"location":"Building_qemu/#installing-qemu-system-riscv64-on-ubuntu","text":"Get source code of qemu from github using the command below git clone https://github.com/qemu/qemu.git Configure qemu for riscv64-softmmu with following command (replace $PREFIX with valid location of installation). ./configure --prefix=$PREFIX --target-list=riscv64-linux-user,riscv64-softmmu --enable-slirp Execute following command to start the build. make Execute following command to install the binaries at $PREFIX location. make install Note: After the installation with slirp following error can be encountered on some systems . qemu-system-riscv64: symbol lookup error: qemu-system-riscv64: undefined symbol: slirp_new, version SLIRP_4.0 Solution: This can be solved by executing following command in source directory of qemu (which is cloned from github). [sudo] ldconfig","title":"Installing qemu-system-riscv64 on ubuntu"},{"location":"Building_qemu/#testing-qemu-system-riscv64","text":"qemu-system-riscv64 can only be tested by booting a linux operating system. See Booting RISC-V Ubuntu 22.04 on qemu-system-riscv64 .","title":"Testing qemu-system-riscv64"},{"location":"Building_qemu/#installing-qemu-riscv64","text":"","title":"Installing qemu-riscv64"},{"location":"Building_qemu/#what-is-qemu-riscv64","text":"qemu-riscv64 is qemu executable program. But instead of porting a complete operating system (like qemu-system-riscv64 ), it can readily execute binaries. Throughout cross-compiling section, qemu-riscv64 will be used with linux-user and executable of every program can be tested on qemu (e.g. python, ruby etc.).","title":"What is qemu-riscv64"},{"location":"Building_qemu/#installing-qemu-riscv64-on-ubuntu-2204","text":"Get source code of qemu using the command below git clone https://github.com/qemu/qemu.git Use following command in the root directory of repository to configure qemu for riscv64-linux-user ./configure --target-list=riscv64-linux-user --prefix=$PREFIX # Replace $PREFIX with a valid location to install at Note: If this is not your architecture/platform, you can see a list of available platform/architecture by executing following command. ./configure --help Use the following command to start the build process make -j$(nproc) After the builld is complete without any error, use the following command to install binaries at $PREFIX location make install Add the $PREFIX/bin to $PATH variable so that it may be recognized as a command. Using this method causes the qemu-riscv64 to have an issue with sysroot. It starts searching for libraries in the root folder of the machine which is based on x86_64-linux-gnu . A simple workaround is to give path of the sysroot/ folder where riscv64-unknown-linux-gnu toolchain is installed. Here that directory will be denoted as $RISCV_SYSROOT","title":"Installing qemu-riscv64 on ubuntu 22.04"},{"location":"Building_qemu/#testing-qemu-riscv64","text":"Create a C file in your favorite editor or by using the commands below: echo \"#include<stdio.h>\" > helloworld.c echo \"int main(){\" >> helloworld.c echo \"printf(\"Hello World !\");\" >> helloworld.c echo '}' >> helloworld.c Execute following command to compile C program with riscv gnu toolchain riscv64-unknown-linux-gnu-gcc helloworld.c -o helloworld Execute following command to execute the compiled binary on qemu-riscv64 qemu-riscv64 -L $RISCV_SYSROOT ./helloworld If everything went right, following output will be shown. Hello World !","title":"Testing qemu-riscv64"},{"location":"Creating_CI_CD_pipeline/","text":"Creating a CI/CD pipeline in Jenkins Pre Requisistes For sake of this documentation, jenkins built-in sample script is used to create and execute a cd/cd pipeline in jenkins. In the built-in script maven is used as M3, so one must install Maven plugin inside jenkins and name it M3 . Usually maven is already present inside jenkins and can be configured from Global Configuration Tools . Following steps demonstrate configuring Maven plugin. Go to Jenkins Dashboard and click on Manage Jenkins In Manage Jenkins , under System Configuration section, click on Global Tool Configuration . In Global Tool Configuration , scroll down to Maven section and click on the respective option under the Maven Section (should be Maven installations\u2026 or Add Maven ). Under Maven installations , enter M3 in \u201cname\u201d text box, check Install Automatically and select Version greater than 3, then click Apply and Save . This should install Maven version 3 and configure as M3 . Steps for Jenkins pipeline creation After installing Jenkins and having all the suggested plugins installed, go to Jenkins dashboard and click on Create Job . On the next page, give your pipeline a name, select Pipeline and click OK . A Configuration page for the pipeline will appear. Select Build Triggers options and General options according to need and scroll down to the Pipeline section. Definition section contains configuration for stages and steps of the pipeline. Under Definition section, you can either choose Pipeline script and try writing your own script or try some sample pipeline (like Hello World , Github+Maven etc) or you could select Pipeline script from SCM and give a github repository containing configurations of Pipeline. Press Save and Apply . This should take you to the Pipeline and you can build the pipeline and if no unresolved dependencies are present, the pipeline should build without any error.","title":"Creating a pipeline project"},{"location":"Creating_CI_CD_pipeline/#creating-a-cicd-pipeline-in-jenkins","text":"","title":"Creating a CI/CD pipeline in Jenkins"},{"location":"Creating_CI_CD_pipeline/#pre-requisistes","text":"For sake of this documentation, jenkins built-in sample script is used to create and execute a cd/cd pipeline in jenkins. In the built-in script maven is used as M3, so one must install Maven plugin inside jenkins and name it M3 . Usually maven is already present inside jenkins and can be configured from Global Configuration Tools . Following steps demonstrate configuring Maven plugin. Go to Jenkins Dashboard and click on Manage Jenkins In Manage Jenkins , under System Configuration section, click on Global Tool Configuration . In Global Tool Configuration , scroll down to Maven section and click on the respective option under the Maven Section (should be Maven installations\u2026 or Add Maven ). Under Maven installations , enter M3 in \u201cname\u201d text box, check Install Automatically and select Version greater than 3, then click Apply and Save . This should install Maven version 3 and configure as M3 .","title":"Pre Requisistes"},{"location":"Creating_CI_CD_pipeline/#steps-for-jenkins-pipeline-creation","text":"After installing Jenkins and having all the suggested plugins installed, go to Jenkins dashboard and click on Create Job . On the next page, give your pipeline a name, select Pipeline and click OK . A Configuration page for the pipeline will appear. Select Build Triggers options and General options according to need and scroll down to the Pipeline section. Definition section contains configuration for stages and steps of the pipeline. Under Definition section, you can either choose Pipeline script and try writing your own script or try some sample pipeline (like Hello World , Github+Maven etc) or you could select Pipeline script from SCM and give a github repository containing configurations of Pipeline. Press Save and Apply . This should take you to the Pipeline and you can build the pipeline and if no unresolved dependencies are present, the pipeline should build without any error.","title":"Steps for Jenkins pipeline creation"},{"location":"Creating_Jenkins_Node_on_LXC/","text":"Creating a Jenkins Node on LXC Container Reference Links Documentation for LXC containers can be found at: https://linuxcontainers.org Details regarding Jenkins ssh agents can be found at: https://acloudguru.com/blog/engineering/adding-a-jenkins-agent-node What is a container A container is a virtualization method for isolating the applications (or even operating systems) from each other. Why do we need a container for Jenkins node In Jenkins, node is a location where our jobs run. One user can use one node for all of his processes and multiple users may also use one node for all of their processes. In Jenkins freestyle project, we can use bash shell or windows command shell due to which there is a possibility to navigate anywhere in the server machine. This possibility can lead to various security and integrity issues for server administrators and also for other users using that webserver. So one must isolate each node and allocate each node to each user separately. What is LXC Container LXC stands for Linux Containers. LXC is a package for linux operating systems and provides linux users with containers which may contain a whole linux operating system while also being lightweight than a virtual machine. More information regarding LXC can be found at https://linuxcontainers.org/lxc/introduction/ On ubuntu 20.04 one can install LXC using command [sudo] apt-get install lxc Creating a container with LXC NOTE: Throughout this document, the name of the container will be my-container and the name of the user will be user1 . So wherever my-container is written, one may change the name to whatever one wants to give it. Pre-requisites Before proceeding, it is important to mention that at the point of writing this document following are the specifications for linux kernel and distribution: Linux Kernel : 5.15.0-46-generic (can be checked on ubuntu by command uname -r ) Distribution : Ubuntu focal (20.04) (can be checked on ubuntu by command lsb_release -a ) By default linux users are not allowed to create any network device on the machine. For doing that, one must add uid and gid in the /etc/lxc/lxc-usernet . The uid and gid of the user you want to use can be found in the files /etc/uid and /etc/gid respectively. After getting the gid and uid of the user you want to allow for creating the network devices, one must go to /etc/lxc/lxc-usernet and add the uid and gid in the following format: <username> veth lxcbr0 10 e.g. jenkins veth lxcbr0 10 In above example jenkins is the username, veth is the command used for creating the bridges between virtual network devices and physical network devices (you will be able to see that ethernet device in our container will be lxcbr0 ) and lxbr0 is the network device we want to create. \u201810\u2019 represents the number of devices we want to create using our specified user. According lxc documentation, in ubuntu 20.04 an additional command is required before creating lxc container: export DOWNLOAD_KEYSERVER=\"hkp://keyserver.ubuntu.com\" Creating image After this, one can create container using following command systemd-run --unit=my-unit --user --scope -p \"Delegate=yes\" -- lxc-create -t download -n my-container This runs lxc container with unit name my-unit , container name my-container and delegates a control group (also known as the cgroup ) which is needed for resource allocation for processes in container. This will output the list of available linux distributions in which one may want to run the container and will prompt for Distribution as shown in the following image. After selecting the suitable distribution, release and architecture (which is also mentioned in the table), the container may be created as shown in the image below: Next thing is to start a container which will change its state from STOPPED to RUNNING using the following command. lxc-start -n my-container Above command will have no output if it succeeds. The state of the container can be checked using the following command. lxc-info -n my-container After starting the container, its state will be set as running and is just like turning a linux machine ON. From this point onwards, if one wants to use the machine in the terminal then use the following command and this will switch the terminal to the root of the container. lxc-attach my-container Now the container is ready to be used and is completely isolated from the host machine. Using SSH to access container with username and password The above mentioned method can be used to attach the host machine terminal to the container and this can be used to access the container. But if one wants to access the machine remotely then one possible and well-known method will be to configure and use SSH on the container. As it is an out of the box linux distro and only the root user is present, so first create another user using the following command and then manage its permissions for /home folder . #Considering you remain the root user for execution of all the following #commands useradd user1 cd /home mkdir user1 #Creating home directory for user1 chown user1:user1 user1 #Giving ownership of home directory to new user #For adding the same shell and bashrc configurations for new user, use #following #command, otherwise the shell will be very basic for new user and will be #very inconvenient to use. usermod -s /bin/bash user1 For the sake of simplicity of this document the name used for the new user is user1 here. (You may want to set the password for user1 by executing the passwd command in root.) At this point user1 is not in sudoers. For adding it to the sudoers, it must be added in the sudo group which can be done by using the following command. usermod -a -G sudo user1 Now switch to the user1 using the following command. su - user Now install openssh-server for configuring the ssh on user1. sudo apt install openssh-server After that one must find the ip of the container we are using, for this either run following command while in container with user1, sudo apt-install net-tools #Because ifconfig is part of net-tools which are by default not installed on new #container Ifconfig -a OR open a new terminal in the host machine and execute the following command. lxc-info my-container -iH So, the ip of the container is 10.0.3.127 . The command for establishing an ssh connection to a remote machine is mentioned below and it will ask for the password of the remote machine which is actually the container in our case. ssh user1@10.0.3.127 After entering the password, terminal will switch to the container\u2019s user1 as can be seen in the following image. Using SSH to access Jenkins agents on the container First install some initial dependencies (git, jdk, jre) on the containers for running agents on the container. sudo apt update sudo apt install default-jdk default-jre git maven Now login to jenkins with administrator privileges and create a node in it from Dashboard > Manage Jenkins > Nodes and press + New Node . Enter a name for the node and select the desirable node type. For this documentation, the node will be a permanent type and the name will be temp_node . After this click on Create which will display the configuration page of the node. Write the description of the node as desired. Number of executors means number of threads running at a time (it will be better to set it to the number of processors present on the machine which is running this node). Remote root directory will be the directory where the jobs will run by default on the node. In our case this will be a specified directory inside the container. Labels indicate that this node will run only when a job with specified labels is run, otherwise this node will not be used (also depends on the usage method in the next option). If the purpose is to use the node by default for every job, then leave it empty. Select a desired usage option. In launch methods, select \u201cLaunch agents via SSH\u201d In \u201cHost\u201d enter the ip address of the container which is 10.0.3.127 in our case. In \u201cCredentials\u201d, click Add and this will open another sub dialog for entering credentials information. Select the kind as \u201cUsername wih password\u201d. Leave other options as is and write the username and password of the container user, in our case the username will be user1 and password will be the password which was set for user1. \u201cID\u201d and \u201cDescription\u201d are optional. Click on \u201cAdd\u201d. Now as the credentials are added, click on the dropdown menu and select the username you just added. In our case it is user1 as the username added was user1. After this rest of the options need not to be changed if this node is going to be a default node. Click on \u201cSave\u201d. After complete setup, the configuration for this node will look something like this. If no issue is encountered during this whole setup, jenkins will take us to the log and after sometime (when the ssh connection is established) we can see \u201cAgent successfully connected and online\u201d at the bottom of the log as can be seen in the screenshot below. After this point, node will be able to run jobs from the container directory.","title":"Creating Jenkins Node on LXC"},{"location":"Creating_Jenkins_Node_on_LXC/#creating-a-jenkins-node-on-lxc-container","text":"","title":"Creating a Jenkins Node on LXC Container"},{"location":"Creating_Jenkins_Node_on_LXC/#reference-links","text":"Documentation for LXC containers can be found at: https://linuxcontainers.org Details regarding Jenkins ssh agents can be found at: https://acloudguru.com/blog/engineering/adding-a-jenkins-agent-node","title":"Reference Links"},{"location":"Creating_Jenkins_Node_on_LXC/#what-is-a-container","text":"A container is a virtualization method for isolating the applications (or even operating systems) from each other.","title":"What is a container"},{"location":"Creating_Jenkins_Node_on_LXC/#why-do-we-need-a-container-for-jenkins-node","text":"In Jenkins, node is a location where our jobs run. One user can use one node for all of his processes and multiple users may also use one node for all of their processes. In Jenkins freestyle project, we can use bash shell or windows command shell due to which there is a possibility to navigate anywhere in the server machine. This possibility can lead to various security and integrity issues for server administrators and also for other users using that webserver. So one must isolate each node and allocate each node to each user separately.","title":"Why do we need a container for Jenkins node"},{"location":"Creating_Jenkins_Node_on_LXC/#what-is-lxc-container","text":"LXC stands for Linux Containers. LXC is a package for linux operating systems and provides linux users with containers which may contain a whole linux operating system while also being lightweight than a virtual machine. More information regarding LXC can be found at https://linuxcontainers.org/lxc/introduction/ On ubuntu 20.04 one can install LXC using command [sudo] apt-get install lxc","title":"What is LXC Container"},{"location":"Creating_Jenkins_Node_on_LXC/#creating-a-container-with-lxc","text":"NOTE: Throughout this document, the name of the container will be my-container and the name of the user will be user1 . So wherever my-container is written, one may change the name to whatever one wants to give it.","title":"Creating a container with LXC"},{"location":"Creating_Jenkins_Node_on_LXC/#pre-requisites","text":"Before proceeding, it is important to mention that at the point of writing this document following are the specifications for linux kernel and distribution: Linux Kernel : 5.15.0-46-generic (can be checked on ubuntu by command uname -r ) Distribution : Ubuntu focal (20.04) (can be checked on ubuntu by command lsb_release -a ) By default linux users are not allowed to create any network device on the machine. For doing that, one must add uid and gid in the /etc/lxc/lxc-usernet . The uid and gid of the user you want to use can be found in the files /etc/uid and /etc/gid respectively. After getting the gid and uid of the user you want to allow for creating the network devices, one must go to /etc/lxc/lxc-usernet and add the uid and gid in the following format: <username> veth lxcbr0 10 e.g. jenkins veth lxcbr0 10 In above example jenkins is the username, veth is the command used for creating the bridges between virtual network devices and physical network devices (you will be able to see that ethernet device in our container will be lxcbr0 ) and lxbr0 is the network device we want to create. \u201810\u2019 represents the number of devices we want to create using our specified user. According lxc documentation, in ubuntu 20.04 an additional command is required before creating lxc container: export DOWNLOAD_KEYSERVER=\"hkp://keyserver.ubuntu.com\"","title":"Pre-requisites"},{"location":"Creating_Jenkins_Node_on_LXC/#creating-image","text":"After this, one can create container using following command systemd-run --unit=my-unit --user --scope -p \"Delegate=yes\" -- lxc-create -t download -n my-container This runs lxc container with unit name my-unit , container name my-container and delegates a control group (also known as the cgroup ) which is needed for resource allocation for processes in container. This will output the list of available linux distributions in which one may want to run the container and will prompt for Distribution as shown in the following image. After selecting the suitable distribution, release and architecture (which is also mentioned in the table), the container may be created as shown in the image below: Next thing is to start a container which will change its state from STOPPED to RUNNING using the following command. lxc-start -n my-container Above command will have no output if it succeeds. The state of the container can be checked using the following command. lxc-info -n my-container After starting the container, its state will be set as running and is just like turning a linux machine ON. From this point onwards, if one wants to use the machine in the terminal then use the following command and this will switch the terminal to the root of the container. lxc-attach my-container Now the container is ready to be used and is completely isolated from the host machine.","title":"Creating image"},{"location":"Creating_Jenkins_Node_on_LXC/#using-ssh-to-access-container-with-username-and-password","text":"The above mentioned method can be used to attach the host machine terminal to the container and this can be used to access the container. But if one wants to access the machine remotely then one possible and well-known method will be to configure and use SSH on the container. As it is an out of the box linux distro and only the root user is present, so first create another user using the following command and then manage its permissions for /home folder . #Considering you remain the root user for execution of all the following #commands useradd user1 cd /home mkdir user1 #Creating home directory for user1 chown user1:user1 user1 #Giving ownership of home directory to new user #For adding the same shell and bashrc configurations for new user, use #following #command, otherwise the shell will be very basic for new user and will be #very inconvenient to use. usermod -s /bin/bash user1 For the sake of simplicity of this document the name used for the new user is user1 here. (You may want to set the password for user1 by executing the passwd command in root.) At this point user1 is not in sudoers. For adding it to the sudoers, it must be added in the sudo group which can be done by using the following command. usermod -a -G sudo user1 Now switch to the user1 using the following command. su - user Now install openssh-server for configuring the ssh on user1. sudo apt install openssh-server After that one must find the ip of the container we are using, for this either run following command while in container with user1, sudo apt-install net-tools #Because ifconfig is part of net-tools which are by default not installed on new #container Ifconfig -a OR open a new terminal in the host machine and execute the following command. lxc-info my-container -iH So, the ip of the container is 10.0.3.127 . The command for establishing an ssh connection to a remote machine is mentioned below and it will ask for the password of the remote machine which is actually the container in our case. ssh user1@10.0.3.127 After entering the password, terminal will switch to the container\u2019s user1 as can be seen in the following image.","title":"Using SSH to access container with username and password"},{"location":"Creating_Jenkins_Node_on_LXC/#using-ssh-to-access-jenkins-agents-on-the-container","text":"First install some initial dependencies (git, jdk, jre) on the containers for running agents on the container. sudo apt update sudo apt install default-jdk default-jre git maven Now login to jenkins with administrator privileges and create a node in it from Dashboard > Manage Jenkins > Nodes and press + New Node . Enter a name for the node and select the desirable node type. For this documentation, the node will be a permanent type and the name will be temp_node . After this click on Create which will display the configuration page of the node. Write the description of the node as desired. Number of executors means number of threads running at a time (it will be better to set it to the number of processors present on the machine which is running this node). Remote root directory will be the directory where the jobs will run by default on the node. In our case this will be a specified directory inside the container. Labels indicate that this node will run only when a job with specified labels is run, otherwise this node will not be used (also depends on the usage method in the next option). If the purpose is to use the node by default for every job, then leave it empty. Select a desired usage option. In launch methods, select \u201cLaunch agents via SSH\u201d In \u201cHost\u201d enter the ip address of the container which is 10.0.3.127 in our case. In \u201cCredentials\u201d, click Add and this will open another sub dialog for entering credentials information. Select the kind as \u201cUsername wih password\u201d. Leave other options as is and write the username and password of the container user, in our case the username will be user1 and password will be the password which was set for user1. \u201cID\u201d and \u201cDescription\u201d are optional. Click on \u201cAdd\u201d. Now as the credentials are added, click on the dropdown menu and select the username you just added. In our case it is user1 as the username added was user1. After this rest of the options need not to be changed if this node is going to be a default node. Click on \u201cSave\u201d. After complete setup, the configuration for this node will look something like this. If no issue is encountered during this whole setup, jenkins will take us to the log and after sometime (when the ssh connection is established) we can see \u201cAgent successfully connected and online\u201d at the bottom of the log as can be seen in the screenshot below. After this point, node will be able to run jobs from the container directory.","title":"Using SSH to access Jenkins agents on the container"},{"location":"Multinode_Pipelines/","text":"Multinode Pipelines One can create multinode pipelines in jenkins freestyle pipeline job as well as in jenkins native pipeline job. In such a pipeline, different stages of the pipeline can run on separate nodes. The jenkins native pipeline job can have two types. Scripted Pipeline Declarative pipeline For the sake of this documentation, two nodes are used: container-node and container-node2 Label In jenkins, label specifies where the job or a stage of the job can run. In jenkins, a label can be explicitly added in the node's configuration so that each time that label is mentioned in a jenkins job, that specific node is used for building the job, or the name of the node can also be used as label in case if no label is added in the node. Creating multinode pipelines in Jenkins native pipeline job Scripted Pipeline For creating a multinode pipeline with jenkins native scripted pipeline job, jenkinsfile can be tweaked. The keyword node can be used to specify the node in which the stages are going to run and this keyword can also be used in between stages. Following screenshot indicates a code in which first stage runs on node named container-node . While the second node runs on node named container-node2 . node ('container-node') { stage ('*** Creating a directory in container-node ***'){ sh '''#!/bin/bash mkdir newdir_container-node-$RANDOM ''' } stage ('*** Creating a directory in container-node2'){ node ('container-node2'){ sh'''#!/bin/bash mkdir newdir_container-node2-$RANDOM ''' } } } Declarative pipeline For a declarative pipeline the keyword label can be used in the agent block. This can either be done at the start of the pipeline after the keyword pipeline or it can be used inside the stages. The following code builds two stages each of which is built in separate node. The keyword none with agent means that the agent is not specified globally for each stage and it should be specified inside each stage. pipeline { agent none // Means no agent specified. This means each node will specify its own agent stages { stage('container-node') { agent{ label \"container-node\" //Selecting container-node for this stage } steps { sh '''#!/bin/bash echo 'Hello container-node' mkdir \"newdir-container-node-$RANDOM\" ''' } } stage('container-node2'){ agent{ label \"container-node2\" //Selecting container-node2 for this stage } steps{ sh'''#!/bin/bash echo ''Hello container-node2 mkdir \"newdir-container-node2-$RANDOM\" ''' } } } } Creating multinode pipelines using Jenkins freestyle jobs Jenkins freestyle jobs can be combined together by mentioning post-build jobs in the freestyle job settings. Freestyle jobs are customizable and offer more options than jenkins native pipeline jobs. Each job represents a stage and each stage has separate settings. One can use jenkins freestyle jobs to run on a separate node by selecting the option Restrict where this project can be run in the General section of the job's configuration. In the Label Expression , the node's name can be mentioned in which to run the job. The following image shows the option from job's configuration. This can be done in all the job's configuration for the specified node.","title":"Creating Multinode pipelines"},{"location":"Multinode_Pipelines/#multinode-pipelines","text":"One can create multinode pipelines in jenkins freestyle pipeline job as well as in jenkins native pipeline job. In such a pipeline, different stages of the pipeline can run on separate nodes. The jenkins native pipeline job can have two types. Scripted Pipeline Declarative pipeline For the sake of this documentation, two nodes are used: container-node and container-node2","title":"Multinode Pipelines"},{"location":"Multinode_Pipelines/#label","text":"In jenkins, label specifies where the job or a stage of the job can run. In jenkins, a label can be explicitly added in the node's configuration so that each time that label is mentioned in a jenkins job, that specific node is used for building the job, or the name of the node can also be used as label in case if no label is added in the node.","title":"Label"},{"location":"Multinode_Pipelines/#creating-multinode-pipelines-in-jenkins-native-pipeline-job","text":"","title":"Creating multinode pipelines in Jenkins native pipeline job"},{"location":"Multinode_Pipelines/#scripted-pipeline","text":"For creating a multinode pipeline with jenkins native scripted pipeline job, jenkinsfile can be tweaked. The keyword node can be used to specify the node in which the stages are going to run and this keyword can also be used in between stages. Following screenshot indicates a code in which first stage runs on node named container-node . While the second node runs on node named container-node2 . node ('container-node') { stage ('*** Creating a directory in container-node ***'){ sh '''#!/bin/bash mkdir newdir_container-node-$RANDOM ''' } stage ('*** Creating a directory in container-node2'){ node ('container-node2'){ sh'''#!/bin/bash mkdir newdir_container-node2-$RANDOM ''' } } }","title":"Scripted Pipeline"},{"location":"Multinode_Pipelines/#declarative-pipeline","text":"For a declarative pipeline the keyword label can be used in the agent block. This can either be done at the start of the pipeline after the keyword pipeline or it can be used inside the stages. The following code builds two stages each of which is built in separate node. The keyword none with agent means that the agent is not specified globally for each stage and it should be specified inside each stage. pipeline { agent none // Means no agent specified. This means each node will specify its own agent stages { stage('container-node') { agent{ label \"container-node\" //Selecting container-node for this stage } steps { sh '''#!/bin/bash echo 'Hello container-node' mkdir \"newdir-container-node-$RANDOM\" ''' } } stage('container-node2'){ agent{ label \"container-node2\" //Selecting container-node2 for this stage } steps{ sh'''#!/bin/bash echo ''Hello container-node2 mkdir \"newdir-container-node2-$RANDOM\" ''' } } } }","title":"Declarative pipeline"},{"location":"Multinode_Pipelines/#creating-multinode-pipelines-using-jenkins-freestyle-jobs","text":"Jenkins freestyle jobs can be combined together by mentioning post-build jobs in the freestyle job settings. Freestyle jobs are customizable and offer more options than jenkins native pipeline jobs. Each job represents a stage and each stage has separate settings. One can use jenkins freestyle jobs to run on a separate node by selecting the option Restrict where this project can be run in the General section of the job's configuration. In the Label Expression , the node's name can be mentioned in which to run the job. The following image shows the option from job's configuration. This can be done in all the job's configuration for the specified node.","title":"Creating multinode pipelines using Jenkins freestyle jobs"},{"location":"Software_Developer_Guide/","text":"Software Developer Guide for RISC-V CI A software developer is the end-user who will develop or build his/her projects on RISC-V CI infrastructure. This guide will cover all the things a software developer needs to create a project based on RISC-V Continuous Integration (CI). Currently, the specifications for RISC-V CI is as follows. Jenkins latest version Ubuntu 22.04 LTS Pre-requisites GitHub account. GitHub project repository with owner rights. Setting up Jenkinsfile inside github project repository Jenkins pipeline will need a Jenkinsfile written with jenkins pipeline syntax to start execution of tests/checks (see link ). This pipeline will contain all the stages (and may be steps) of a CI/CD pipeline. This pipeline can be scripted pipeline which will only have stages or it can also be declarative pipeline which may also have steps inside stages. A simple scripted Helloworld pipeline in linux is as follows: node{ stage('*** Compilation Phase ***') { //Using bash commands sh '''#!/bin/bash echo \"Hello World !\\n\" ''' } } Upon execution of such a pipeline, the console output can be viewed as follows. Note: This jenkinsfile should remain same in all the branches and pull requests. Setting credentials for webhook Jenkins supports webhooks which can trigger the job from external sources such as GitHub. They work in a way such that, if a specified branch is committed or if a pull request is created, the specified job build starts running depending upon the trigger event which is set in build's configuration in jenkins. This process requires GitHub credentials of owner of repository on which the webhook is to be set. These credentials can be safely added to jenkins without anyone (even administrator) seeing the passwords as follows. Contact us for a new account by creating an issue on github or through email with following required details: Name Contact Email URL of the GitHub project repository Name of your organization We will provide you the credentials on the provided email. Login with provided credentials. Click on the drop down near user profile as shown in image below. This will take you to the credentials page. Scroll down to the Stores scoped to Jenkins and click on the System as shown in the image. Click on Global credentials (unrestricted) . Click on Add Credentials . This will take you to the New Credentials page. Select Kind as Username with password . Select Scope as Global (Jenkins, nodes, items, all child items etc) . Enter the GitHub username in Username . You may check Treat username as secret which will mask username in console output of builds, but this is not recommended by jenkins due to performance issues. Enter Password as GitHub personal authentication token (PAT) which can be acquired from Github account settings. ID and Description can be left empty. But it is recommended to give a unique ID and a suitable but careful description by which administrator will be able to identify and use these credentials to set up github webhook. Select Create . This process will look something like this. Now credentials will be available in the credentials list and will be shown to you as well as administrator as shown in the image below. This will create an option in configurations for using these credentials in github webhook without changing or viewing them. Requirements for administrator After the above setup is complete from software developer's side, developer will need to provide the administrator with following information. Dependencies for running the project which can be packages which are needed to install in the RISC-V CI environment by administrator. Events for triggering the job build. URL of GitHub repository. Path and name of Jenkinsfile on the provided GitHub repository. Any additional information which should be given for successful execution of job builds.","title":"Guide for Software Developer"},{"location":"Software_Developer_Guide/#software-developer-guide-for-risc-v-ci","text":"A software developer is the end-user who will develop or build his/her projects on RISC-V CI infrastructure. This guide will cover all the things a software developer needs to create a project based on RISC-V Continuous Integration (CI). Currently, the specifications for RISC-V CI is as follows. Jenkins latest version Ubuntu 22.04 LTS","title":"Software Developer Guide for RISC-V CI"},{"location":"Software_Developer_Guide/#pre-requisites","text":"GitHub account. GitHub project repository with owner rights.","title":"Pre-requisites"},{"location":"Software_Developer_Guide/#setting-up-jenkinsfile-inside-github-project-repository","text":"Jenkins pipeline will need a Jenkinsfile written with jenkins pipeline syntax to start execution of tests/checks (see link ). This pipeline will contain all the stages (and may be steps) of a CI/CD pipeline. This pipeline can be scripted pipeline which will only have stages or it can also be declarative pipeline which may also have steps inside stages. A simple scripted Helloworld pipeline in linux is as follows: node{ stage('*** Compilation Phase ***') { //Using bash commands sh '''#!/bin/bash echo \"Hello World !\\n\" ''' } } Upon execution of such a pipeline, the console output can be viewed as follows. Note: This jenkinsfile should remain same in all the branches and pull requests.","title":"Setting up Jenkinsfile inside github project repository"},{"location":"Software_Developer_Guide/#setting-credentials-for-webhook","text":"Jenkins supports webhooks which can trigger the job from external sources such as GitHub. They work in a way such that, if a specified branch is committed or if a pull request is created, the specified job build starts running depending upon the trigger event which is set in build's configuration in jenkins. This process requires GitHub credentials of owner of repository on which the webhook is to be set. These credentials can be safely added to jenkins without anyone (even administrator) seeing the passwords as follows. Contact us for a new account by creating an issue on github or through email with following required details: Name Contact Email URL of the GitHub project repository Name of your organization We will provide you the credentials on the provided email. Login with provided credentials. Click on the drop down near user profile as shown in image below. This will take you to the credentials page. Scroll down to the Stores scoped to Jenkins and click on the System as shown in the image. Click on Global credentials (unrestricted) . Click on Add Credentials . This will take you to the New Credentials page. Select Kind as Username with password . Select Scope as Global (Jenkins, nodes, items, all child items etc) . Enter the GitHub username in Username . You may check Treat username as secret which will mask username in console output of builds, but this is not recommended by jenkins due to performance issues. Enter Password as GitHub personal authentication token (PAT) which can be acquired from Github account settings. ID and Description can be left empty. But it is recommended to give a unique ID and a suitable but careful description by which administrator will be able to identify and use these credentials to set up github webhook. Select Create . This process will look something like this. Now credentials will be available in the credentials list and will be shown to you as well as administrator as shown in the image below. This will create an option in configurations for using these credentials in github webhook without changing or viewing them.","title":"Setting credentials for webhook"},{"location":"Software_Developer_Guide/#requirements-for-administrator","text":"After the above setup is complete from software developer's side, developer will need to provide the administrator with following information. Dependencies for running the project which can be packages which are needed to install in the RISC-V CI environment by administrator. Events for triggering the job build. URL of GitHub repository. Path and name of Jenkinsfile on the provided GitHub repository. Any additional information which should be given for successful execution of job builds.","title":"Requirements for administrator"},{"location":"jenkins_gitlab_integration/","text":"Integrating GitLab with Jenkins This documentation will cover how to create an integration between jenkins and GitLab. This will allow users to trigger jenkins job when a merge request or a push is detected in GitLab. Pre-requisites GitLab plugin Git plugin GitLab repository with owner's credentials Configuring Jenkins System First jenkins needs to be configured. Go to Dashboard > Manage Jenkins > Configure System Scroll down to Gitlab section Check Enable authentication for '/project' end-point Enter a Connection name . Enter Gitlab host URL as https://gitlab.com/ . In case there is a different domain name, then enter there instead of above url. In Credentials , click on Add then click on Jenkins . In Kind , select GitLab API token . In API token , enter the gitlab personal access token (this will be obtained below while configuring GitLab). Click on Advanced . Click on Test Connection . If everything goes right, it should print success . Configuring GitLab Click on profile avatar in the top right. Click on Edit profile . Click on Access Tokens . Create a new personal access token and copy it (this is the GitLab API token used in above section Configuring Jenkins System ). Go to repository settings. On left-side pane, select Webhooks . Enter GitLab webhook URL (this is explained below in next section). Enter Secret Token (this is explained in the below section). Check desirable trigger options. Click Add webhook . Configuring Jenkins Job Create a jenkins job. On job configuration page, scroll down to Source Code Management . Select Git . In Credentials , add the owner credentials of GitLab repository. This will be Username and Password . Select the appropriate branch (generally it is main ). Scroll down to Build Triggers . Check Build when a change is pushed to GitLab . There will also be a GitLab webhook URL . This is needed in GitLab. This URL will be called as Webhook URL . Click on Advanced in the same option, then scroll down and generate a secret token. This token is also needed. This token will be called as Secret Token . Scroll down to Post-build Actions and select Publish build status to GitLab . Click on Apply and Save . Now whenever there will be a push (or whatever build trigger option is set in jenkins and gitlab webhook), specified jenkins job will be triggered.","title":"GitLab Integration"},{"location":"jenkins_gitlab_integration/#integrating-gitlab-with-jenkins","text":"This documentation will cover how to create an integration between jenkins and GitLab. This will allow users to trigger jenkins job when a merge request or a push is detected in GitLab.","title":"Integrating GitLab with Jenkins"},{"location":"jenkins_gitlab_integration/#pre-requisites","text":"GitLab plugin Git plugin GitLab repository with owner's credentials","title":"Pre-requisites"},{"location":"jenkins_gitlab_integration/#configuring-jenkins-system","text":"First jenkins needs to be configured. Go to Dashboard > Manage Jenkins > Configure System Scroll down to Gitlab section Check Enable authentication for '/project' end-point Enter a Connection name . Enter Gitlab host URL as https://gitlab.com/ . In case there is a different domain name, then enter there instead of above url. In Credentials , click on Add then click on Jenkins . In Kind , select GitLab API token . In API token , enter the gitlab personal access token (this will be obtained below while configuring GitLab). Click on Advanced . Click on Test Connection . If everything goes right, it should print success .","title":"Configuring Jenkins System"},{"location":"jenkins_gitlab_integration/#configuring-gitlab","text":"Click on profile avatar in the top right. Click on Edit profile . Click on Access Tokens . Create a new personal access token and copy it (this is the GitLab API token used in above section Configuring Jenkins System ). Go to repository settings. On left-side pane, select Webhooks . Enter GitLab webhook URL (this is explained below in next section). Enter Secret Token (this is explained in the below section). Check desirable trigger options. Click Add webhook .","title":"Configuring GitLab"},{"location":"jenkins_gitlab_integration/#configuring-jenkins-job","text":"Create a jenkins job. On job configuration page, scroll down to Source Code Management . Select Git . In Credentials , add the owner credentials of GitLab repository. This will be Username and Password . Select the appropriate branch (generally it is main ). Scroll down to Build Triggers . Check Build when a change is pushed to GitLab . There will also be a GitLab webhook URL . This is needed in GitLab. This URL will be called as Webhook URL . Click on Advanced in the same option, then scroll down and generate a secret token. This token is also needed. This token will be called as Secret Token . Scroll down to Post-build Actions and select Publish build status to GitLab . Click on Apply and Save . Now whenever there will be a push (or whatever build trigger option is set in jenkins and gitlab webhook), specified jenkins job will be triggered.","title":"Configuring Jenkins Job"},{"location":"Cross_Compiling/Cross_compiling_coremark/","text":"Cross-compiling Coremark Coremark is another benchmarking tool. Here coremark will be cross-compiled for riscv64-unknown-linux-gnu and will be run on qemu-riscv64 . The github source code commit at the time of build is eefc986ebd3452d6adde22eafaff3e5c859f29e4 and branch is main . Getting the source code Execute the following command to get the source code of coremark. git clone https://github.com/eembc/coremark.git Tweaking source files for riscv64-unknown-linux-gnu At the time of this documentation, linux is being used for this test. First of all core_portme.mak will be changed. Navigate to linux/ directory in source repository. Open core_portme.mak . Here a single line will be used to include core_portme.mak from posix directory. So, navigate to posix/ directory in source folder Open core_portme.mak in posix/ directory and do the following changes to variables here. Change CC?=cc to CC=riscv64-unknown-linux-gnu-gcc . Scroll down and change EXE=.exe to EXE= (it should be blank). Scroll down and change LD=gcc to LD=riscv64-unknown-linux-gnu-ld . As we are using qemu-riscv64 , so change RUN= to RUN=qemu-riscv64 -L \"$$RISCV_SYSROOT\" Save changes and exit this file. Now open core_portme.h and change #define USE_CLOCK 0 to #define USE_CLOCK 1 and save. Navigate to source directory of repository and execute following command make PORT_DIR=linux/ If everything went right, the output result will be stored in run1.log and run2.log and will be of the form as shown below. 2K validation run parameters for coremark. CoreMark Size : 666 Total ticks : 12368459 Total time (secs): 12.368459 Iterations/Sec : 8893.589735 Iterations : 110000 Compiler version : GCC12.2.0 Compiler flags : -O2 -DPERFORMANCE_RUN=1 -lrt Memory location : Please put data memory location here (e.g. code in flash, data on heap etc) seedcrc : 0x18f2 [0]crclist : 0xe3c1 [0]crcmatrix : 0x0747 [0]crcstate : 0x8d84 [0]crcfinal : 0x0956 Correct operation validated. See README.md for run and reporting rules.","title":"Cross-compiling Coremark"},{"location":"Cross_Compiling/Cross_compiling_coremark/#cross-compiling-coremark","text":"Coremark is another benchmarking tool. Here coremark will be cross-compiled for riscv64-unknown-linux-gnu and will be run on qemu-riscv64 . The github source code commit at the time of build is eefc986ebd3452d6adde22eafaff3e5c859f29e4 and branch is main .","title":"Cross-compiling Coremark"},{"location":"Cross_Compiling/Cross_compiling_coremark/#getting-the-source-code","text":"Execute the following command to get the source code of coremark. git clone https://github.com/eembc/coremark.git","title":"Getting the source code"},{"location":"Cross_Compiling/Cross_compiling_coremark/#tweaking-source-files-for-riscv64-unknown-linux-gnu","text":"At the time of this documentation, linux is being used for this test. First of all core_portme.mak will be changed. Navigate to linux/ directory in source repository. Open core_portme.mak . Here a single line will be used to include core_portme.mak from posix directory. So, navigate to posix/ directory in source folder Open core_portme.mak in posix/ directory and do the following changes to variables here. Change CC?=cc to CC=riscv64-unknown-linux-gnu-gcc . Scroll down and change EXE=.exe to EXE= (it should be blank). Scroll down and change LD=gcc to LD=riscv64-unknown-linux-gnu-ld . As we are using qemu-riscv64 , so change RUN= to RUN=qemu-riscv64 -L \"$$RISCV_SYSROOT\" Save changes and exit this file. Now open core_portme.h and change #define USE_CLOCK 0 to #define USE_CLOCK 1 and save. Navigate to source directory of repository and execute following command make PORT_DIR=linux/ If everything went right, the output result will be stored in run1.log and run2.log and will be of the form as shown below. 2K validation run parameters for coremark. CoreMark Size : 666 Total ticks : 12368459 Total time (secs): 12.368459 Iterations/Sec : 8893.589735 Iterations : 110000 Compiler version : GCC12.2.0 Compiler flags : -O2 -DPERFORMANCE_RUN=1 -lrt Memory location : Please put data memory location here (e.g. code in flash, data on heap etc) seedcrc : 0x18f2 [0]crclist : 0xe3c1 [0]crcmatrix : 0x0747 [0]crcstate : 0x8d84 [0]crcfinal : 0x0956 Correct operation validated. See README.md for run and reporting rules.","title":"Tweaking source files for riscv64-unknown-linux-gnu"},{"location":"Cross_Compiling/Cross_compiling_dhrystone/","text":"Cross-compiling dhrystone Dhrystone is a benchmarking tool. Here dhrystone will be compiled from source and run on qemu-riscv64 . Dhrystone is comparable to VAX 11/780 in a way that VAX 11/780 achieves 1757 dhrystones per second which is also referred to as 1 MIPS of VAX11/780. So number of dhrystones per seconds are obtained and then divided by 1757 to get MIPS. See this link for more details. Cross-compiling for riscv64-unknown-linux-gnu Get the source code of dhrystone using the command below git clone https://github.com/sifive/benchmark-dhrystone.git Navigate to root directory of repository and compile program with riscv64-unknown-linux-gnu-gcc instead of native gcc cd benchmark-dhrystone make CC=riscv64-unknown-linux-gnu-gcc Execute following command to execute dhrystone binary qemu-riscv64 -L $RISCV_SYSROOT ./dhrystone Note: You may want to tweak Makefile and dhry_1.c a little bit to get the correct results.","title":"Cross Compiling Drhystone"},{"location":"Cross_Compiling/Cross_compiling_dhrystone/#cross-compiling-dhrystone","text":"Dhrystone is a benchmarking tool. Here dhrystone will be compiled from source and run on qemu-riscv64 . Dhrystone is comparable to VAX 11/780 in a way that VAX 11/780 achieves 1757 dhrystones per second which is also referred to as 1 MIPS of VAX11/780. So number of dhrystones per seconds are obtained and then divided by 1757 to get MIPS. See this link for more details.","title":"Cross-compiling dhrystone"},{"location":"Cross_Compiling/Cross_compiling_dhrystone/#cross-compiling-for-riscv64-unknown-linux-gnu","text":"Get the source code of dhrystone using the command below git clone https://github.com/sifive/benchmark-dhrystone.git Navigate to root directory of repository and compile program with riscv64-unknown-linux-gnu-gcc instead of native gcc cd benchmark-dhrystone make CC=riscv64-unknown-linux-gnu-gcc Execute following command to execute dhrystone binary qemu-riscv64 -L $RISCV_SYSROOT ./dhrystone Note: You may want to tweak Makefile and dhry_1.c a little bit to get the correct results.","title":"Cross-compiling for riscv64-unknown-linux-gnu"},{"location":"Cross_Compiling/Cross_compiling_openssl/","text":"Cross-compiling openssl What is openssl Openssl is a software library which is used inside many high level languages (e.g. Python, Ruby etc.) and also in linux itself. It is used for security and other cryptography applications. Building openssl v1.0.1 for riscv64 architecture Following are the steps used to build openssl for riscv64 architecture. Get the source code of openssl and navigate inside the cloned repository using the commands below git clone https://github.com/openssl/openssl.git cd openssl Configure openssl for building. In openssl there are some os/compiler choices which one can use to build for his architecture. But in our case there is no support for building with riscv64. As it is written in C language, so it can be compiled whether or not the support is given. Use the following command to generate a Makefile for linux-generic64 ./Configure linux-generic64 --prefix=$PREFIX # Prefix is the directory where you want binaries to be installed at the end After the above command is successfully completed, run the following command to build openssl using riscv64-unknown-linux-gnu-gcc compiler instead of native gcc compiler. make -j$(nproc) CC=riscv64-unknown-linux-gnu-gcc Install the binaries in the specified --prefix using the command below make -j$(nproc) install CC=riscv64-unknown-linux-gnu-gcc The installed binary can be tested on qemu-riscv64 using the command below: qemu-riscv64 -L $RISCV_SYSROOT ./openssl Here $RISCV_SYSROOT is the sysroot/ folder located inside the riscv gnu toolchain installed directory. The above mentioned command will start the openssl console if everything went right. Note: Do not change the directory of openssl or rename it, as it some files inside it are inferred with absolute paths, changing the directory or renaming it will cause other packages to not configure openssl for them when cross-compiling. Building openssl v1.1.1r for riscv64 architecture In openssl v1.1.1r, there is a support for linux64-riscv64 . Following is the procedure for cross-compilation. Checkout the v1.1.1r of openssl by executing following command in the repository directory. git checkout OpenSSL_1_1_1r Execute following command to configure for riscv64 architecture and generate a Makefile . ./Configure linux64-riscv64 --prefix=$PREFIX # Replace $PREFIX with where you want to install binaries Execute following command to cross-compile for riscv64-unknown-linux-gnu . make CROSS_COMPILE=riscv64-unknown-linux-gnu- Then install binaries at $PREFIX with following command. make install Solving post-installation errors On some operating systems, the installed binaries may not run properly and will give following error. ./openssl: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory This means that shared libraries cannot be found in the path where the system is looking for them. This can be solved by setting LD_LIBRARY_PATH variable as follows. export LD_LIBRARY_PATH=$PREFIX/lib:$LD_LIBRARY_PATH It will be a good practice to include the above in the bashrc for debian users.","title":"Cross Compiling Openssl"},{"location":"Cross_Compiling/Cross_compiling_openssl/#cross-compiling-openssl","text":"","title":"Cross-compiling openssl"},{"location":"Cross_Compiling/Cross_compiling_openssl/#what-is-openssl","text":"Openssl is a software library which is used inside many high level languages (e.g. Python, Ruby etc.) and also in linux itself. It is used for security and other cryptography applications.","title":"What is openssl"},{"location":"Cross_Compiling/Cross_compiling_openssl/#building-openssl-v101-for-riscv64-architecture","text":"Following are the steps used to build openssl for riscv64 architecture. Get the source code of openssl and navigate inside the cloned repository using the commands below git clone https://github.com/openssl/openssl.git cd openssl Configure openssl for building. In openssl there are some os/compiler choices which one can use to build for his architecture. But in our case there is no support for building with riscv64. As it is written in C language, so it can be compiled whether or not the support is given. Use the following command to generate a Makefile for linux-generic64 ./Configure linux-generic64 --prefix=$PREFIX # Prefix is the directory where you want binaries to be installed at the end After the above command is successfully completed, run the following command to build openssl using riscv64-unknown-linux-gnu-gcc compiler instead of native gcc compiler. make -j$(nproc) CC=riscv64-unknown-linux-gnu-gcc Install the binaries in the specified --prefix using the command below make -j$(nproc) install CC=riscv64-unknown-linux-gnu-gcc The installed binary can be tested on qemu-riscv64 using the command below: qemu-riscv64 -L $RISCV_SYSROOT ./openssl Here $RISCV_SYSROOT is the sysroot/ folder located inside the riscv gnu toolchain installed directory. The above mentioned command will start the openssl console if everything went right. Note: Do not change the directory of openssl or rename it, as it some files inside it are inferred with absolute paths, changing the directory or renaming it will cause other packages to not configure openssl for them when cross-compiling.","title":"Building openssl v1.0.1 for riscv64 architecture"},{"location":"Cross_Compiling/Cross_compiling_openssl/#building-openssl-v111r-for-riscv64-architecture","text":"In openssl v1.1.1r, there is a support for linux64-riscv64 . Following is the procedure for cross-compilation. Checkout the v1.1.1r of openssl by executing following command in the repository directory. git checkout OpenSSL_1_1_1r Execute following command to configure for riscv64 architecture and generate a Makefile . ./Configure linux64-riscv64 --prefix=$PREFIX # Replace $PREFIX with where you want to install binaries Execute following command to cross-compile for riscv64-unknown-linux-gnu . make CROSS_COMPILE=riscv64-unknown-linux-gnu- Then install binaries at $PREFIX with following command. make install","title":"Building openssl v1.1.1r for riscv64 architecture"},{"location":"Cross_Compiling/Cross_compiling_openssl/#solving-post-installation-errors","text":"On some operating systems, the installed binaries may not run properly and will give following error. ./openssl: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory This means that shared libraries cannot be found in the path where the system is looking for them. This can be solved by setting LD_LIBRARY_PATH variable as follows. export LD_LIBRARY_PATH=$PREFIX/lib:$LD_LIBRARY_PATH It will be a good practice to include the above in the bashrc for debian users.","title":"Solving post-installation errors"},{"location":"Cross_Compiling/Cross_compiling_ruby/","text":"Cross Compilation of Ruby System Specifications Build Architecture: x86_64-linux-gnu Host Architecture: riscv64-unknown-linux-gnu Operating System for Installation Procedure: Ubuntu 20.04 Pre-requisites Pre-requisites for installing ruby from source can be installed using the following command sudo apt-get -y install libc6-dev libssl-dev libmysql++-dev libsqlite3-dev make build-essential libssl-dev libreadline6-dev zlib1g-dev libyaml-dev Other than this, ruby itself is needed for building ruby from source. sudo apt install ruby There is another thing which needs to be taken care of before building ruby from source. If ruby is installed on system itself using apt , then cross compiling ruby will end up in an error as shown in the image below. This error is seen in ruby 2.7.0p0 (2019-12-25 revision 647ee6f091) [x86_64-linux-gnu] . To tackle this issue, one workaround is to install build ruby for native system, then delete ruby which was installed through apt . This procedure will be added in the Build section. Getting source code Source code of ruby can be obtained from github repository using the command below: git clone https://github.com/ruby/ruby.git Build Installing ruby for native architecture Before cross-compiling, one must install ruby from source on the native machine which will solve the error described in Pre-requisites section above. ( THIS STEP IS STRONGLY RECOMMENDED ! )In the source directory, create a folder with any name in which Makefile will be generated otherwise there will be a lot of files made in the source directory (possibly create copy of repo directory). In the source directory of ruby run following command to generate configure file. ./autogen.sh After this, run the following configure command to generate Makefile . ../configure --prefix=$PREFIX #$PREFIX is where you want to install binary files at the end, so replace it. After the above command is completed, run following command to start the build make -j$(nproc) #-j$(nproc) uses parallelism for make After the above command is complete, run following command to install the binaries on the specified path mentioned in --prefix above make install Now ruby should be available in the $PREFIX path (also in the .bashrc ). Add $PREFIX path to $PATH variable and uninstall the the ruby installed using apt otherwise, the source will keep using that one for building and the error will persist. sudo apt purge ruby Cross-Compiling Ruby for riscv64-unknown-linux-gnu After the ruby installed using apt is uninstalled from the system, clean the working directory with following command. make clean After cleaning the working directory, generate the Makefile again for cross-compiling ruby for riscv64-unknown-linux-gnu target and host using the command below ../../configure --prefix=$PREFIX --build=x86_64-linux-gnu --host=riscv64-unknown-linux-gnu --target=riscv64-unknown-linux-gnu After the above command is successful, start build with following command make -j(nproc) Install the binaries in path mentioned with --prefix above with following command make install After this process, ruby will be installed inside $PREFIX/ directory. Note: Currently, this process (as checked on version 3.1.2) installs ruby without extensions shown in the following image","title":"Cross Compiling Ruby"},{"location":"Cross_Compiling/Cross_compiling_ruby/#cross-compilation-of-ruby","text":"","title":"Cross Compilation of Ruby"},{"location":"Cross_Compiling/Cross_compiling_ruby/#system-specifications","text":"Build Architecture: x86_64-linux-gnu Host Architecture: riscv64-unknown-linux-gnu Operating System for Installation Procedure: Ubuntu 20.04","title":"System Specifications"},{"location":"Cross_Compiling/Cross_compiling_ruby/#pre-requisites","text":"Pre-requisites for installing ruby from source can be installed using the following command sudo apt-get -y install libc6-dev libssl-dev libmysql++-dev libsqlite3-dev make build-essential libssl-dev libreadline6-dev zlib1g-dev libyaml-dev Other than this, ruby itself is needed for building ruby from source. sudo apt install ruby There is another thing which needs to be taken care of before building ruby from source. If ruby is installed on system itself using apt , then cross compiling ruby will end up in an error as shown in the image below. This error is seen in ruby 2.7.0p0 (2019-12-25 revision 647ee6f091) [x86_64-linux-gnu] . To tackle this issue, one workaround is to install build ruby for native system, then delete ruby which was installed through apt . This procedure will be added in the Build section.","title":"Pre-requisites"},{"location":"Cross_Compiling/Cross_compiling_ruby/#getting-source-code","text":"Source code of ruby can be obtained from github repository using the command below: git clone https://github.com/ruby/ruby.git","title":"Getting source code"},{"location":"Cross_Compiling/Cross_compiling_ruby/#build","text":"","title":"Build"},{"location":"Cross_Compiling/Cross_compiling_ruby/#installing-ruby-for-native-architecture","text":"Before cross-compiling, one must install ruby from source on the native machine which will solve the error described in Pre-requisites section above. ( THIS STEP IS STRONGLY RECOMMENDED ! )In the source directory, create a folder with any name in which Makefile will be generated otherwise there will be a lot of files made in the source directory (possibly create copy of repo directory). In the source directory of ruby run following command to generate configure file. ./autogen.sh After this, run the following configure command to generate Makefile . ../configure --prefix=$PREFIX #$PREFIX is where you want to install binary files at the end, so replace it. After the above command is completed, run following command to start the build make -j$(nproc) #-j$(nproc) uses parallelism for make After the above command is complete, run following command to install the binaries on the specified path mentioned in --prefix above make install Now ruby should be available in the $PREFIX path (also in the .bashrc ). Add $PREFIX path to $PATH variable and uninstall the the ruby installed using apt otherwise, the source will keep using that one for building and the error will persist. sudo apt purge ruby","title":"Installing ruby for native architecture"},{"location":"Cross_Compiling/Cross_compiling_ruby/#cross-compiling-ruby-for-riscv64-unknown-linux-gnu","text":"After the ruby installed using apt is uninstalled from the system, clean the working directory with following command. make clean After cleaning the working directory, generate the Makefile again for cross-compiling ruby for riscv64-unknown-linux-gnu target and host using the command below ../../configure --prefix=$PREFIX --build=x86_64-linux-gnu --host=riscv64-unknown-linux-gnu --target=riscv64-unknown-linux-gnu After the above command is successful, start build with following command make -j(nproc) Install the binaries in path mentioned with --prefix above with following command make install After this process, ruby will be installed inside $PREFIX/ directory. Note: Currently, this process (as checked on version 3.1.2) installs ruby without extensions shown in the following image","title":"Cross-Compiling Ruby for riscv64-unknown-linux-gnu"},{"location":"Cross_Compiling/Overview/","text":"Cross Compilation Need of Cross Compilation Lets say you have a computer A having processor with architecture a and another computer B having processor with architecture b . Assume computer A has all the necessary tools and softwares whereas computer B does not have any software, tools, compilers and also the dependencies needed to install these tools and softwares. In this scenario, you cannot download the software setups (say tarballs) directly on computer B and have them installed/compiled because they will not be able to get compiled by computer B and will be completely useless. Basic concept The basic workflow in such a condition will be as follows: Install a compiler on computer A such that it runs on computer A and compiles code for architecture a . Using the above installed compiler, install a compiler on computer A such that it runs on computer A but compiles code for architecture b . Such a compiler is called cross compiler. One example of such a cross-compiler is RISC-V GNU TOOLCHAIN . Using the above compiled cross compiler, compile all the programs and also the compiler itself for architecture b . Port all the compiled binaries to the computer B . Such a process is beneficial if you have created a custom architecture and there is no support available for it. Throughout this documentation, the cross compiler used will be RISCV GNU Toolchain mentioned with riscv64-unknown-linux-gcc running on x86_64 and compiling code for riscv64 architecture whereas the native compiler will be gcc running on x86_64 and compiling code for x86_64 unless otherwise specified.","title":"Overview"},{"location":"Cross_Compiling/Overview/#cross-compilation","text":"","title":"Cross Compilation"},{"location":"Cross_Compiling/Overview/#need-of-cross-compilation","text":"Lets say you have a computer A having processor with architecture a and another computer B having processor with architecture b . Assume computer A has all the necessary tools and softwares whereas computer B does not have any software, tools, compilers and also the dependencies needed to install these tools and softwares. In this scenario, you cannot download the software setups (say tarballs) directly on computer B and have them installed/compiled because they will not be able to get compiled by computer B and will be completely useless.","title":"Need of Cross Compilation"},{"location":"Cross_Compiling/Overview/#basic-concept","text":"The basic workflow in such a condition will be as follows: Install a compiler on computer A such that it runs on computer A and compiles code for architecture a . Using the above installed compiler, install a compiler on computer A such that it runs on computer A but compiles code for architecture b . Such a compiler is called cross compiler. One example of such a cross-compiler is RISC-V GNU TOOLCHAIN . Using the above compiled cross compiler, compile all the programs and also the compiler itself for architecture b . Port all the compiled binaries to the computer B . Such a process is beneficial if you have created a custom architecture and there is no support available for it. Throughout this documentation, the cross compiler used will be RISCV GNU Toolchain mentioned with riscv64-unknown-linux-gcc running on x86_64 and compiling code for riscv64 architecture whereas the native compiler will be gcc running on x86_64 and compiling code for x86_64 unless otherwise specified.","title":"Basic concept"},{"location":"jenkins_github_integration/Github_PR_webhook_integration/","text":"GitHub Pull Request hook integration with Jenkins Reference Link: https://plugins.jenkins.io/ghprb/ General Guidline If facing an issue, it is better to launch the Jenkins from running the .war file (which is obtained by building the Jenkins github repository) in terminal (which is mostly bash in case of linux) and observing the terminal's output. For example if a webhook is not created, the GUI may not show anything but the terminal will most probably print a message showing the reason for this behavior. GitHub Pull Request Builder Plugin In version control, it is better to check the changes and run tests before the changes are merged into main branch. Sometimes those changes can be enormous to check. So it becomes difficult for requested reviewer of the pull request to check all the changes and run tests on them manually. For that purpose, it is better to automate the process so that whenever a pull request is generated, all the tests are triggered and based on the results, the reviewer decides whether or not to merge the branch with main. This can be achieved using Jenkins' Github Pull Request Builder plugin. Specifications at the time of documentation Operating System: Linux Distribution: Ubuntu Release: Focal (also known as 20.04) Jenkins version: 2.371 (can be seen in config.xml) Github Webhook Builder version: 1.42.2 Pre-Requisites Jenkins. Git Plugin GitHub Plugin Github Pull Request Builder Plugin Github account and repository with permission to generate a pull request to merge the branch. Setting up Jenkins configuration Install the above mentioned plugins from Dashboard > Manage Jenkins > Manage Plugins > Available Plugins . Go to Dashboard > Manage Jenkins > Configure System . Scroll down to Github Pull Request Builder . Leave GitHub Server API URL and Jenkins URL override as it is. In Credentials , click on add and select Jenkins from drop down. Select Kind as Secret text . In Secret , add GitHub Personal authentication token which can be acquired from GitHub account settings. Add some safe description to remind what these credentials are about otherwise jenkins use a lot of credentials and it gets difficult to keep account of them. Leave ID empty. Click on Add . Now select the added credentials from the drop down menu of Credentials . Click on Test Credentials... . Check Test basic connection to GitHub . Click on Connect to API . This will show the message Connected to <API_URL> as <Name_of_GitHub_user> . Other settings can be left empty. Click on save. Setting up Jenkins job In this documentation, Pipeline job will be used, but any job is expected to work fine with these settings. For this documentation it is considered that the jenkinsfile for building the pipeline is present in the repository which is to be built. Go to Dashboard and click New Item . Enter a Job name (here it will be github_PR_webhook ). Click OK . It will navigate to the job's configuration page. Add a Description of choice. Scroll down and check GitHub project . Add the URL of GitHub repository. Here the URL of the repository should be added without .git extention. This is important that the person who is creating pull request should be either in admin list or whitelist because otherwise, the webhook will not be created. Scroll down to Build Triggers section. Check GitHub Pull Request Builder . This will open further configurations for this option. Leave GitHub API credentials . Add a GitHub admin's username in the Admin list . This is important becaue otherwise, the checks will not run on generating a pull request. Check Use github hooks for build triggering . Click on Advanced and in Whitelist Target Branches add the branch name for which, when a pull request is generated, the job is supposed to be trigger (here it is main ). Scroll down to Pipeline section. In Definition , select Pipeline script from SCM . Select SCM as Git from drop down. Enter the Repository URL from GitHub. Enter Credentials with access to this repository (This is optional if the repository is public). Under Advanced , enter Refspec as +refs/pull/*:refs/remotes/origin/pr/* [^note]. If the tests are to be run on actual commit in the pull request then, under Branches to build section, in Branch Specifier , enter ${ghprbActualCommit} [^note]. Leave other settings as it is. In Script Path , add the path and name of the jenkinsfile which is present in GitHub repository. Uncheck Lightweight checkout [^note1] Click Apply and then Save . After saving the job, a webhook should be created automatically in GitHub if the credentials provided in the settings are correct. Verifying if the procedure In the GitHub repository, add another branch aside from main . For this, expand main and click on View all branches . Click on New Branch , and insert a name. After a new branch is created, select new branch instead of main in repository page. Add some changings to either of the file (even adding a space is enough). Commit Changes. Create a Pull request. Now after checking the merge conflicts, the checks will run their results will be shown with pull request (as can be seen in the image below). NOTE: The Jenkinfile will run present in the pull request and not in the main branch. Clicking on the Details navigates the user to the Jenkins Job result page where the console output and each stage can be seen. [^note]: This point is taken from the jenkins GitHub Pull Request Builder plugin documentation at https://plugins.jenkins.io/ghprb/ [^note1]: This is an issue mentioned in the documentation of Github Pull Request Builder plugin at https://plugins.jenkins.io/ghprb/ .","title":"Github PR webhook integration"},{"location":"jenkins_github_integration/Github_PR_webhook_integration/#github-pull-request-hook-integration-with-jenkins","text":"Reference Link: https://plugins.jenkins.io/ghprb/","title":"GitHub Pull Request hook integration with Jenkins"},{"location":"jenkins_github_integration/Github_PR_webhook_integration/#general-guidline","text":"If facing an issue, it is better to launch the Jenkins from running the .war file (which is obtained by building the Jenkins github repository) in terminal (which is mostly bash in case of linux) and observing the terminal's output. For example if a webhook is not created, the GUI may not show anything but the terminal will most probably print a message showing the reason for this behavior.","title":"General Guidline"},{"location":"jenkins_github_integration/Github_PR_webhook_integration/#github-pull-request-builder-plugin","text":"In version control, it is better to check the changes and run tests before the changes are merged into main branch. Sometimes those changes can be enormous to check. So it becomes difficult for requested reviewer of the pull request to check all the changes and run tests on them manually. For that purpose, it is better to automate the process so that whenever a pull request is generated, all the tests are triggered and based on the results, the reviewer decides whether or not to merge the branch with main. This can be achieved using Jenkins' Github Pull Request Builder plugin.","title":"GitHub Pull Request Builder Plugin"},{"location":"jenkins_github_integration/Github_PR_webhook_integration/#specifications-at-the-time-of-documentation","text":"Operating System: Linux Distribution: Ubuntu Release: Focal (also known as 20.04) Jenkins version: 2.371 (can be seen in config.xml) Github Webhook Builder version: 1.42.2","title":"Specifications at the time of documentation"},{"location":"jenkins_github_integration/Github_PR_webhook_integration/#pre-requisites","text":"Jenkins. Git Plugin GitHub Plugin Github Pull Request Builder Plugin Github account and repository with permission to generate a pull request to merge the branch.","title":"Pre-Requisites"},{"location":"jenkins_github_integration/Github_PR_webhook_integration/#setting-up-jenkins-configuration","text":"Install the above mentioned plugins from Dashboard > Manage Jenkins > Manage Plugins > Available Plugins . Go to Dashboard > Manage Jenkins > Configure System . Scroll down to Github Pull Request Builder . Leave GitHub Server API URL and Jenkins URL override as it is. In Credentials , click on add and select Jenkins from drop down. Select Kind as Secret text . In Secret , add GitHub Personal authentication token which can be acquired from GitHub account settings. Add some safe description to remind what these credentials are about otherwise jenkins use a lot of credentials and it gets difficult to keep account of them. Leave ID empty. Click on Add . Now select the added credentials from the drop down menu of Credentials . Click on Test Credentials... . Check Test basic connection to GitHub . Click on Connect to API . This will show the message Connected to <API_URL> as <Name_of_GitHub_user> . Other settings can be left empty. Click on save.","title":"Setting up Jenkins configuration"},{"location":"jenkins_github_integration/Github_PR_webhook_integration/#setting-up-jenkins-job","text":"In this documentation, Pipeline job will be used, but any job is expected to work fine with these settings. For this documentation it is considered that the jenkinsfile for building the pipeline is present in the repository which is to be built. Go to Dashboard and click New Item . Enter a Job name (here it will be github_PR_webhook ). Click OK . It will navigate to the job's configuration page. Add a Description of choice. Scroll down and check GitHub project . Add the URL of GitHub repository. Here the URL of the repository should be added without .git extention. This is important that the person who is creating pull request should be either in admin list or whitelist because otherwise, the webhook will not be created. Scroll down to Build Triggers section. Check GitHub Pull Request Builder . This will open further configurations for this option. Leave GitHub API credentials . Add a GitHub admin's username in the Admin list . This is important becaue otherwise, the checks will not run on generating a pull request. Check Use github hooks for build triggering . Click on Advanced and in Whitelist Target Branches add the branch name for which, when a pull request is generated, the job is supposed to be trigger (here it is main ). Scroll down to Pipeline section. In Definition , select Pipeline script from SCM . Select SCM as Git from drop down. Enter the Repository URL from GitHub. Enter Credentials with access to this repository (This is optional if the repository is public). Under Advanced , enter Refspec as +refs/pull/*:refs/remotes/origin/pr/* [^note]. If the tests are to be run on actual commit in the pull request then, under Branches to build section, in Branch Specifier , enter ${ghprbActualCommit} [^note]. Leave other settings as it is. In Script Path , add the path and name of the jenkinsfile which is present in GitHub repository. Uncheck Lightweight checkout [^note1] Click Apply and then Save . After saving the job, a webhook should be created automatically in GitHub if the credentials provided in the settings are correct.","title":"Setting up Jenkins job"},{"location":"jenkins_github_integration/Github_PR_webhook_integration/#verifying-if-the-procedure","text":"In the GitHub repository, add another branch aside from main . For this, expand main and click on View all branches . Click on New Branch , and insert a name. After a new branch is created, select new branch instead of main in repository page. Add some changings to either of the file (even adding a space is enough). Commit Changes. Create a Pull request. Now after checking the merge conflicts, the checks will run their results will be shown with pull request (as can be seen in the image below). NOTE: The Jenkinfile will run present in the pull request and not in the main branch. Clicking on the Details navigates the user to the Jenkins Job result page where the console output and each stage can be seen. [^note]: This point is taken from the jenkins GitHub Pull Request Builder plugin documentation at https://plugins.jenkins.io/ghprb/ [^note1]: This is an issue mentioned in the documentation of Github Pull Request Builder plugin at https://plugins.jenkins.io/ghprb/ .","title":"Verifying if the procedure"},{"location":"jenkins_github_integration/Github_push_webhook/","text":"Github 'Push' webhook integration with Jenkins Purpose of using github webhook integration with jenkins Most of the time, after a push on the upstream repository, one may want to check the result of all the checks on the repository defined by CI/CD pipeline. This tells whether there is some issue with push and whether or not the defined checks/tests have passed. This can be achieved using github push webhook integration with jenkins. Jenkins version and operating system specifications The version of Jenkins and operating system specifications at the time of writing this documentation are mentioned below: Jenkins version: 2.370 Operating System: Linux Distribution: Ubuntu Release: Focal (also called 20.04) Pre-requisites Jenkins ngrok (only if a public IP is not available) Setting up the ngrok The localhost cannot be used for github webhook integration as it cannot be detected by online webservers. For this reason, a public ip must be used. For the sake of this documentation, ngrok is being used, which maps localhost to some public ip which can then be accessed publicly on the internet. Following steps can be used for setting up ngrok on ubuntu: Install ngrok. sudo apt install ngrok For using html content, a sign up is required on ngrok. So sign up on ngrok. Execute the following command to run ngrok which will provide a public ip mapped to localhost. ngrok http <port number> This will setup ngrok and provide a public ip for working online. Setting up Jenkins for github webhook Pre-requisites Following plugins should be installed in jenkins: Git Plugin GitHub API Plugin GitHub Plugin Jenkins Configuration in Settings Go to Manage Jenkins > Configure System and scroll down to GitHub section. Click on Add GitHub Server Add a name for the Github server. Leave API URL as is. In Credentials, click on Add . Jenkins will appear in drop down, click on it. Select Kind as Secret text Scroll down to the Secret and here, put down the github personal authentication token (PAT) which can be acquired from github account. Other options can be left unattended. Click on Add . Now the credentials should be added. In the Credentials drop down, select Secret text . Check Manage hooks . Now the connection can be established and can be checked by clicking the Test Connection . Click Save . Jenkins job setup for Github Webhook Create a new jenkins freestyle job and proceed with following settings on the configurations along with desired settings. Check GitHub project in General section and provide GitHub repository URL. In Source Code Management section, select Git . Give Repository URL In Credentials (if the credentials are not created already), click on Add , click on Jenkins from the drop down. Select Kind as Username with password . In Username , enter github username. In Password , enter github personal authentication token (PAT) which can be acquired from github account. Other fields can be left unattended. Click on Add From Credentials drop down, select your added credentials. In Branches to build section, in Branch Specifier field, enter the name of branch of github repository which needs to be built. In Build Triggers section, check GitHub hook trigger for GITScm polling . (Following step is for checking the commit status according to the Jenkins job status, means if Jenkins job fails, commit status is also Failure ) Scroll down to the bottom and Add post-build action . From drop down, select Set GitHub commit status . Leave other settings as is and click on Advanced . Check Handle errors Under drop down Result on failure , select FAILURE Click on Apply and Save By this point, Jenkins is setup for github webhooks. Setting up github repository webhook For the sake of this documentation, I have created a simple repository called jenkins_hello_world_integrated . Go to GitHub repository's settings In Webhooks section, click on Add webhook In Webhooks settings: Add Payload URL as the URL of jenkins and append /github-webhook/ at the end of it. Select Content type application/jason . It is recommended to add Secret which can be generated by jenkins API Token by going to account configuration. It is recommended to Enable SSL verification . Select the events which should trigger the build in jenkins. Check Active . Click on Add Webhook . After this point each time the github repository is commited with a change, jenkins job will start the build and will also denote on the repository if the build has passed or failed (as can be seen in the below screenshot).","title":"Github Push webhook integration"},{"location":"jenkins_github_integration/Github_push_webhook/#github-push-webhook-integration-with-jenkins","text":"","title":"Github 'Push' webhook integration with Jenkins"},{"location":"jenkins_github_integration/Github_push_webhook/#purpose-of-using-github-webhook-integration-with-jenkins","text":"Most of the time, after a push on the upstream repository, one may want to check the result of all the checks on the repository defined by CI/CD pipeline. This tells whether there is some issue with push and whether or not the defined checks/tests have passed. This can be achieved using github push webhook integration with jenkins.","title":"Purpose of using github webhook integration with jenkins"},{"location":"jenkins_github_integration/Github_push_webhook/#jenkins-version-and-operating-system-specifications","text":"The version of Jenkins and operating system specifications at the time of writing this documentation are mentioned below: Jenkins version: 2.370 Operating System: Linux Distribution: Ubuntu Release: Focal (also called 20.04)","title":"Jenkins version and operating system specifications"},{"location":"jenkins_github_integration/Github_push_webhook/#pre-requisites","text":"Jenkins ngrok (only if a public IP is not available)","title":"Pre-requisites"},{"location":"jenkins_github_integration/Github_push_webhook/#setting-up-the-ngrok","text":"The localhost cannot be used for github webhook integration as it cannot be detected by online webservers. For this reason, a public ip must be used. For the sake of this documentation, ngrok is being used, which maps localhost to some public ip which can then be accessed publicly on the internet. Following steps can be used for setting up ngrok on ubuntu: Install ngrok. sudo apt install ngrok For using html content, a sign up is required on ngrok. So sign up on ngrok. Execute the following command to run ngrok which will provide a public ip mapped to localhost. ngrok http <port number> This will setup ngrok and provide a public ip for working online.","title":"Setting up the ngrok"},{"location":"jenkins_github_integration/Github_push_webhook/#setting-up-jenkins-for-github-webhook","text":"","title":"Setting up Jenkins for github webhook"},{"location":"jenkins_github_integration/Github_push_webhook/#pre-requisites_1","text":"Following plugins should be installed in jenkins: Git Plugin GitHub API Plugin GitHub Plugin","title":"Pre-requisites"},{"location":"jenkins_github_integration/Github_push_webhook/#jenkins-configuration-in-settings","text":"Go to Manage Jenkins > Configure System and scroll down to GitHub section. Click on Add GitHub Server Add a name for the Github server. Leave API URL as is. In Credentials, click on Add . Jenkins will appear in drop down, click on it. Select Kind as Secret text Scroll down to the Secret and here, put down the github personal authentication token (PAT) which can be acquired from github account. Other options can be left unattended. Click on Add . Now the credentials should be added. In the Credentials drop down, select Secret text . Check Manage hooks . Now the connection can be established and can be checked by clicking the Test Connection . Click Save .","title":"Jenkins Configuration in Settings"},{"location":"jenkins_github_integration/Github_push_webhook/#jenkins-job-setup-for-github-webhook","text":"Create a new jenkins freestyle job and proceed with following settings on the configurations along with desired settings. Check GitHub project in General section and provide GitHub repository URL. In Source Code Management section, select Git . Give Repository URL In Credentials (if the credentials are not created already), click on Add , click on Jenkins from the drop down. Select Kind as Username with password . In Username , enter github username. In Password , enter github personal authentication token (PAT) which can be acquired from github account. Other fields can be left unattended. Click on Add From Credentials drop down, select your added credentials. In Branches to build section, in Branch Specifier field, enter the name of branch of github repository which needs to be built. In Build Triggers section, check GitHub hook trigger for GITScm polling . (Following step is for checking the commit status according to the Jenkins job status, means if Jenkins job fails, commit status is also Failure ) Scroll down to the bottom and Add post-build action . From drop down, select Set GitHub commit status . Leave other settings as is and click on Advanced . Check Handle errors Under drop down Result on failure , select FAILURE Click on Apply and Save By this point, Jenkins is setup for github webhooks.","title":"Jenkins job setup for Github Webhook"},{"location":"jenkins_github_integration/Github_push_webhook/#setting-up-github-repository-webhook","text":"For the sake of this documentation, I have created a simple repository called jenkins_hello_world_integrated . Go to GitHub repository's settings In Webhooks section, click on Add webhook In Webhooks settings: Add Payload URL as the URL of jenkins and append /github-webhook/ at the end of it. Select Content type application/jason . It is recommended to add Secret which can be generated by jenkins API Token by going to account configuration. It is recommended to Enable SSL verification . Select the events which should trigger the build in jenkins. Check Active . Click on Add Webhook . After this point each time the github repository is commited with a change, jenkins job will start the build and will also denote on the repository if the build has passed or failed (as can be seen in the below screenshot).","title":"Setting up github repository webhook"}]}