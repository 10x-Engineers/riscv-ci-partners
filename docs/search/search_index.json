{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cloud-V Documentation","text":"<p>This documentation lists the tooling and necessary information to use Cloud-V for RISC-V application development on RISC-V compute instances provided by Cloud-V</p> <p>Overview Slides: link</p> <p>Youtube Channel: youtube.com/@Cloud-V</p>"},{"location":"Creating_CI_CD_pipeline/","title":"Creating a CI/CD pipeline in Jenkins","text":""},{"location":"Creating_CI_CD_pipeline/#pre-requisistes","title":"Pre Requisistes","text":"<p>For sake of this documentation, jenkins built-in sample script is used to create and execute a cd/cd pipeline in jenkins. In the built-in script maven is used as M3, so one must install <code>Maven</code> plugin inside jenkins and name it <code>M3</code>. Usually maven is already present inside jenkins and can be configured from <code>Global Configuration Tools</code>. Following steps demonstrate configuring Maven plugin.  </p> <ul> <li>Go to Jenkins <code>Dashboard</code> and click on <code>Manage Jenkins</code></li> <li>In <code>Manage Jenkins</code>, under <code>System Configuration</code> section, click on <code>Global Tool Configuration</code>.  </li> </ul> <p></p> <ul> <li>In <code>Global Tool Configuration</code>, scroll down to Maven section and click on the respective option under the <code>Maven</code> Section (should be <code>Maven installations\u2026</code> or <code>Add Maven</code>).  </li> </ul> <p></p> <ul> <li> <p>Under <code>Maven installations</code>, enter <code>M3</code> in \u201cname\u201d text box, check <code>Install Automatically</code> and select <code>Version</code> greater than 3, then click <code>Apply</code> and <code>Save</code>.  </p> </li> <li> <p>This should install Maven version 3 and configure as <code>M3</code>.</p> </li> </ul>"},{"location":"Creating_CI_CD_pipeline/#steps-for-jenkins-pipeline-creation","title":"Steps for Jenkins pipeline creation","text":"<ul> <li> <p>After installing Jenkins and having all the suggested plugins installed, go to Jenkins dashboard and click on <code>Create Job</code>.  </p> </li> <li> <p>On the next page, give your pipeline a name, select <code>Pipeline</code> and click <code>OK</code>.</p> </li> </ul> <p></p> <ul> <li> <p>A <code>Configuration</code> page for the pipeline will appear.  </p> </li> <li> <p>Select <code>Build Triggers</code> options and <code>General</code> options according to need and scroll down to the Pipeline section.</p> </li> </ul> <p></p> <ul> <li> <p>Definition section contains configuration for stages and steps of the pipeline. Under <code>Definition</code> section, you can either choose <code>Pipeline script</code> and try writing your own script or try some sample pipeline (like <code>Hello World</code>, <code>Github+Maven</code> etc) or you could select <code>Pipeline script from SCM</code> and give a github repository containing configurations of Pipeline.  </p> </li> <li> <p>Press <code>Save</code> and <code>Apply</code>.  </p> </li> <li> <p>This should take you to the Pipeline and you can build the pipeline and if no unresolved dependencies are present, the pipeline should build without any error.</p> </li> </ul>"},{"location":"Creating_jenkinsfile/","title":"Creating a Jenkins CI Pipeline file","text":""},{"location":"Creating_jenkinsfile/#what-is-a-jenkins-ci-pipeline-file","title":"What is a Jenkins CI Pipeline file","text":"<p>A <code>jenkinsfile</code> (also known as <code>cloud-v-pipeline</code> file) is a Continuous Integration (CI) Jenkins pipeline script which is written in Groovy. It describes various stages (and possibly steps) which are executed in the defined pattern. These stages can be written in bash or they can be written in Groovy itself. Mainly there can be two types of Cloud-V pipeline files are of two types:  </p> <ol> <li>Scripted: Contains only stages</li> <li>Declarative: Contains stages as well as steps (more feature-rich and recommended)</li> </ol> <p>This documentation will cover how to create a <code>cloud-v-pipeline</code> file with bash script inside it and run it on various compute instances (which are known as <code>Nodes</code> in jenkins).</p>"},{"location":"Creating_jenkinsfile/#jenkins-node","title":"Jenkins Node","text":"<p>In jenkins, node represents a compute instance. In simple words, it is the platform on which our job build is going to run.  </p>"},{"location":"Creating_jenkinsfile/#jenkins-master","title":"Jenkins Master","text":"<p>There is a <code>jenkins Master</code> node which is actually the compute instance on which Jenkins is installed. It is the node which schedules builds on runners. For security reason, no job is allowed to run on this node.</p>"},{"location":"Creating_jenkinsfile/#jenkins-slave","title":"Jenkins Slave","text":"<p>Jenkins slave nodes are the compute instances on which our job builds run safely. They may be attached with jenkins master via hardware or they may be connected through remote SSH connection.</p>"},{"location":"Creating_jenkinsfile/#cloud-v-pipeline-file-written-with-bash","title":"<code>cloud-v-pipeline</code> file written with bash","text":""},{"location":"Creating_jenkinsfile/#simple-hello-world-cloud-v-pipeline-file","title":"Simple Hello World <code>cloud-v-pipeline</code> file","text":"<p>In Cloud-V, all the platforms are running Linux operating system, so the <code>cloud-v-pipeline</code> should be written in bash. Following script is an example of how can we run a bash script in scripted <code>cloud-v-pipeline</code>.  </p> <pre><code>\nnode{\n    stage('*** Phase 1 ***') {\n        //Using bash commands\n        sh '''#!/bin/bash\n            echo \"Hello World !\\n\"\n         '''\n    }\n}\n</code></pre> <p>The keyword <code>sh</code> is used to specify a shell script</p> <p>As there is nothing mentioned with <code>node</code>, so the script will run job build on any available compute instance.  </p>"},{"location":"Creating_jenkinsfile/#cloud-v-pipeline-for-a-specific-node","title":"<code>cloud-v-pipeline</code> for a Specific Node","text":"<p>In previous script, the <code>cloud-v-pipeline</code> would run on any compute instances which are available. But in case if someone wants to run a job build on specific node, then a compute instance name must be specified with keyword <code>node</code>. The following script is an example of running above <code>Hello World</code> program on node named <code>hifive_unleashed</code>.  </p> <pre><code>\nnode('hifive_unleashed'){\n    stage('*** Phase 1 ***') {\n        //Using bash commands\n        sh '''#!/bin/bash\n            echo \"Hello World !\\n\"\n         '''\n    }\n}\n</code></pre>"},{"location":"Creating_jenkinsfile/#cloud-v-pipeline-for-cross-platform-compilation-and-execution","title":"<code>cloud-v-pipeline</code> for Cross-Platform Compilation and Execution","text":"<p>Cloud-V supports cross-compilation and execution on emulated RISC-V compute instances. Following tools help in cross compilation and cross-platfrom execution:  </p> <ul> <li>RISC-V GNU Toolchain</li> <li>QEMU user mode (for running standalone binaries)</li> <li>QEMU System (for running application in Linux)  </li> </ul> <p>An example pipeline script is given below in scripted pipeline.  </p> <pre><code>node('x86_runner2'){\n    checkout scm //Getting content of this repo\n    stage('*** Compilation Phase ***') { // for display purposes\n        //Compiling helloworld.c using bash commands\n        sh '''#!/bin/bash\n            gcc -g ./helloworld.c -o helloworld.out\n            riscv64-unknown-linux-gnu-gcc ./helloworld.c -o helloworld_riscv_compiled.out //Cross compiling for RISC-V\n         '''\n    }\n    stage (' *** Running Binaries ***'){\n        sh '''#!/bin/bash\n            ./helloworld.out\n            qemu-riscv64 -L $RISCV_SYSROOT helloworld_riscv_compiled.out //Running executable on RISC-V emulated platform\n         '''\n    }\n}\n</code></pre> <p>The equivalent declarative pipeline is as follows:  </p> <pre><code>pipeline {\n    agent {label \"x86_runner2\"}\n\n    stages {\n        stage('Clone Repository') {\n            steps('delegate'){\n                    checkout scm //Clones the repository on the local machine\n            }\n        }\n        stage ('Compilation Phase'){\n            steps{\n                    sh '''#!/bin/bash\n                        gcc -g ./helloworld.c -o helloworld.out\n                    '''\n                    sh '''#!/bin/bash\n                        riscv64-unknown-linux-gnu-gcc ./helloworld.c -o helloworld_riscv_compiled.out\n                    '''\n            }\n        }\n        stage ('Running Binaries'){\n            steps {\n                    sh ''' #!/bin/bash\n                        ./helloworld.out\n                    '''\n                    sh'''#!/bin/bash\n                        qemu-riscv64 -L $RISCV_SYSROOT helloworld_riscv_compiled.out\n                    '''\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"Creating_jenkinsfile/#reference-links","title":"Reference Links","text":"<p>https://www.jenkins.io/doc/book/pipeline/syntax/</p>"},{"location":"Multinode_Pipelines/","title":"Multinode Pipelines","text":"<p>One can create multinode pipelines in jenkins freestyle pipeline job as well as in jenkins native pipeline job. In such a pipeline, different stages of the pipeline can run on separate nodes. The jenkins native pipeline job can have two types.  </p> <ol> <li>Scripted Pipeline</li> <li>Declarative pipeline</li> </ol> <p>For the sake of this documentation, two nodes are used: <code>container-node</code> and <code>container-node2</code></p>"},{"location":"Multinode_Pipelines/#label","title":"Label","text":"<p>In jenkins, label specifies where the job or a stage of the job can run. In jenkins, a label can be explicitly added in the node's configuration so that each time that label is mentioned in a jenkins job, that specific node is used for building the job, or the name of the node can also be used as label in case if no label is added in the node.</p>"},{"location":"Multinode_Pipelines/#creating-multinode-pipelines-in-jenkins-native-pipeline-job","title":"Creating multinode pipelines in Jenkins native pipeline job","text":""},{"location":"Multinode_Pipelines/#scripted-pipeline","title":"Scripted Pipeline","text":"<p>For creating a multinode pipeline with jenkins native scripted pipeline job, jenkinsfile can be tweaked. The keyword <code>node</code> can be used to specify the node in which the stages are going to run and this keyword can also be used in between stages. Following screenshot indicates a code in which first stage runs on node named <code>container-node</code>. While the second node runs on node named <code>container-node2</code>.  </p> <pre><code>node ('container-node') {\n    stage ('*** Creating a directory in container-node ***'){\n        sh '''#!/bin/bash\n        mkdir newdir_container-node-$RANDOM\n        '''\n    }\n    stage ('*** Creating a directory in container-node2'){\n        node ('container-node2'){\n                sh'''#!/bin/bash\n                mkdir newdir_container-node2-$RANDOM\n                '''\n        }\n    }\n}\n</code></pre>"},{"location":"Multinode_Pipelines/#declarative-pipeline","title":"Declarative pipeline","text":"<p>For a declarative pipeline the keyword <code>label</code> can be used in the agent block. This can either be done at the start of the pipeline after the keyword <code>pipeline</code> or it can be used inside the stages. The following code builds two stages each of which is built in separate node. The keyword <code>none</code> with agent means that the agent is not specified globally for each stage and it should be specified inside each stage.</p> <pre><code>pipeline {\n    agent none // Means no agent specified. This means each node will specify its own agent\n    stages {\n        stage('container-node') {\n            agent{\n                label \"container-node\" //Selecting container-node for this stage\n            }\n            steps {\n                sh '''#!/bin/bash\n                    echo 'Hello container-node'\n                    mkdir \"newdir-container-node-$RANDOM\"\n                '''\n            }\n        }\n        stage('container-node2'){\n            agent{\n                label \"container-node2\" //Selecting container-node2 for this stage\n            }\n            steps{\n                sh'''#!/bin/bash\n                    echo ''Hello container-node2\n                    mkdir \"newdir-container-node2-$RANDOM\"\n                '''\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"Multinode_Pipelines/#creating-multinode-pipelines-using-jenkins-freestyle-jobs","title":"Creating multinode pipelines using Jenkins freestyle jobs","text":"<p>Jenkins freestyle jobs can be combined together by mentioning post-build jobs in the freestyle job settings. Freestyle jobs are customizable and offer more options than jenkins native pipeline jobs. Each job represents a stage and each stage has separate settings. One can use jenkins freestyle jobs to run on a separate node by selecting the option <code>Restrict where this project can be run</code> in the <code>General</code> section of the job's configuration. In the <code>Label Expression</code>, the node's name can be mentioned in which to run the job. The following image shows the option from job's configuration.  </p> <p> </p> <p>This can be done in all the job's configuration for the specified node.  </p>"},{"location":"Software_Developer_Guide/","title":"Setting up CI/CD with Jenkins","text":"<p>This document describes how one can get access to Jenkins and add their project in Jenkins for CI/CD. The Jenkins server is currently hosted at https://dash.cloud-v.co</p> <p>All the compute instances in Jenkins have restrictions regarding which jobs will run on the compute instance. The administrator has to allow you for using specific instance. Be sure to contact administrator and tell them which instance you want to use.</p> <p>There are currently two ways to integrate projects with Jenkins on Cloud-V.</p> <ol> <li>Using Cloud-V automatic integration (beta)</li> <li>Manually integrating your project with Cloud-V</li> </ol>"},{"location":"Software_Developer_Guide/#pre-requisites","title":"Pre-requisites","text":"<ol> <li>GitHub/GitLab account.</li> <li>GitHub/GitLab project repository with owner rights.</li> <li>Access to https://dash.cloud-v.co and https://cloud-v.co (fill this form to request the access)</li> </ol>"},{"location":"Software_Developer_Guide/#getting-a-jenkins-account-on-cloud-v","title":"Getting a Jenkins account on Cloud-V","text":"<p>After getting access to the Jenkins server, use one of the following two methods to create a CI pipeline with Cloud-V.</p>"},{"location":"Software_Developer_Guide/#1-using-cloud-v-automatic-integration-beta","title":"1. Using Cloud-V automatic integration (beta)","text":"<p>Jenkins server in Cloud-V </p> <p>For ease of convenience for users and eliminating time delays of manual set up, users can add their GitHub and GitLab repository in Cloud-V by just adding their repository URL on the Cloud-V page. The source code for this is open-source here.</p> <p>Currently, there are support for following version control systems:</p> <ol> <li>GitHub</li> <li>GitLab</li> </ol>"},{"location":"Software_Developer_Guide/#for-github","title":"For GitHub","text":"<p>For integrating user repository with Cloud-V, there is a GitHub app which users can install in their repository. The purpose of creating the app and publishing it for users is that, GitHub app has all the permissions already set up. So, when a user installs GitHub app, the app automatically sets up all the permissions for the user's repository.</p> <p>Following is the procedure for installing and integrating the repository with Cloud-V github app and for creating the CI pipeline in Cloud-V dashboard.</p> <ul> <li>Visit this link for installing GitHub app.</li> <li>Click on \"Install\" button which will take you to permissions page where you can select the permissions for the repository and also choose the repository which you would like to integrate with Cloud-V app</li> <li>Select \"Only select repositories\" if you would like to integrate a specific repository or number of repositories instead of integrating Cloud-V app with all the repositories.</li> <li>Click on \"Install &amp; Authorize\" which will take you to the page where you can add repository URL</li> <li>Add repository URL and click on \"Submit\"</li> <li> <p>The next page will show you:</p> </li> <li> <p>Access Token (will be visible one-time)</p> </li> <li>URL of the GitHub repository which is configured (currently, one token can be configured with one repository)</li> <li> <p>The link of the CI pipeline which is created automatically in Cloud-V CI dashboard</p> </li> <li> <p>Now go to the repository settings in the following manner and create a webhook for trigger with pull requests and push to branches</p> </li> <li> <p><code>Settings &gt; Webhooks &gt; Add webhook</code></p> </li> <li> <p>Fill the webhook settings in following manner</p> </li> <li> <p><code>Payload URL: https://dash.cloud-v.co/github-webhook/</code></p> </li> <li><code>Content Type: application/json</code></li> <li><code>Enable SSL Verification</code></li> <li><code>Which events would you like to trigger this webhook?: Just the push event</code></li> <li>Leave other fields as is</li> </ul> <p>Note: This creates a github multibranch pipeline automatically and it builds when a PR is created. If you need to check the source code or want to suggest any improvement for this, visit https://github.com/10x-Engineers/Cloud-V-git-automation and create an issue.</p>"},{"location":"Software_Developer_Guide/#for-gitlab","title":"For GitLab","text":"<p>Following is the procedure for integrating repository automatically with GitLab:</p> <p>There is no app for gitlab integration with Cloud-V. So, for creating a pipeline in automated way, you will have to add gitlab access token and gitlab repository URL. Use the following steps to do so.</p> <ul> <li>Generate a GitLab personal access token in your repository settings</li> <li>Visit this link</li> <li>Add the personal access token and URL of the GitLab repository</li> <li>If the personal access token and the URL of the repostory is valid, you will get a link to the created pipeline</li> </ul>"},{"location":"Software_Developer_Guide/#2-manually-adding-the-repository-in-cloud-v","title":"2. Manually adding the repository in Cloud-V","text":"<p>This is the traditional method. The flow involves:</p> <ol> <li>Getting a personal access token from GitHub settings</li> <li>Adding the personal access token in the Cloud-V dashboard by logging in with the provided credentials</li> <li>Notifying the administrator about the credentials ID which user added, so they can add the credentials in the global settings</li> <li>Setting up webhook in the repository settings so that it can be triggered whenever a pull request is created</li> </ol> <p>NOTE: If you followed first method to integrate the repository with Cloud-V, you will not have to follow this method.</p>"},{"location":"Software_Developer_Guide/#setting-credentials-for-webhook","title":"Setting credentials for webhook","text":"<p>Cloud-V supports webhooks which can trigger the job from external sources such as GitHub. They work in a way such that, if a specified branch is committed or if a pull request is created, the specified job build starts running depending upon the trigger event which is set in build's configuration in Cloud-V.  </p> <p>This process requires access token of the repository CREATED BY OWNER OF REPOSITORY on which the webhook is to be set. These credentials can be safely added to Cloud-V without anyone (even administrator) seeing the passwords as follows.</p>"},{"location":"Software_Developer_Guide/#obtaining-github-access-token-for-repository","title":"Obtaining github access token for repository","text":"<p>Navigate to the dashboard of your github account and click on the your github profile picture on the top-right corner on dashboard.</p> <p></p> <p>Then click on the \"Settings\" from the list.</p> <p></p> <p>From the left option bar in Settings scroll down and click on \"Developer settings\".</p> <p></p> <p>Once there, click on \"Personal access tokens\", then click on \"Fine-grained tokens\" from the dropdown list and after that click on \"Generate new token\".</p> <p></p> <p>This will open the page for setting up new access token. Follow following steps for creating a token:  </p> <ol> <li>Give your token a meaningful name under \"Token name\"</li> <li>Set expiration date in \"Expiration\" depending upon how long you would like your repository to be integrated with Cloud-V (think of a meaningful upper bound)</li> <li>The \"Resource owner\" should be the owner of the repository who can access all kinds of settings of the repository</li> <li>Under \"Repository access\", check \"Only select repositories\" and then select the repository for which you would like to create the token</li> <li> <p>Under \"Permissions\" section, expand \"Repository Permissions\" and give the following two permissions:</p> <ul> <li>\"Read and write\" access to \"Commit statuses\" (Because after the CI has run, Cloud-V will be able to set the status of the commit accordingly)</li> <li>\"Read-only\" access to \"Webhooks\"</li> </ul> </li> </ol>"},{"location":"Software_Developer_Guide/#configuring-repository-webhook","title":"Configuring repository webhook","text":"<p>In GitHub,</p> <ul> <li>Go to repository settings which you want to integrate for Cloud-V.  </li> </ul> <p></p> <ul> <li>Go to <code>Webhooks</code> </li> </ul> <p> </p> <ul> <li>Click on <code>Add webhook</code> </li> </ul> <p></p> <ul> <li>Add <code>Payload URL</code> as <code>https://dash.cloud-v.co/ghprbhook/</code> </li> <li>Select content type as <code>application/json</code></li> <li>Check <code>Enable SSL verification</code> </li> <li>In the section Which events would you like to trigger this webhook? check <code>Let me select individual events</code> and check <code>Pul requests</code> as individual events and dont check any other permission.</li> </ul> <p>Webhook settings will look something like this:</p> <p> </p>"},{"location":"Software_Developer_Guide/#configurations-inside-cloud-v","title":"Configurations inside Cloud-V","text":"<p>Note: Currently users are not able to see or modify pipeline build configuration inside Jenkins, that is currently managed by administrator. Users are requested to inform administrator about how they want their pipeline configured.</p> <ul> <li>We will provide you with Cloud-V credentials on the provided email.</li> <li>Login with provided credentials.</li> <li>Click on the <code>Credentials</code> in the left menu.  </li> </ul> <p></p> <ul> <li>This will take you to the credentials page.</li> <li>Scroll down to the <code>Stores scoped to Jenkins</code> and click on the <code>System</code> as shown in the image.  </li> </ul> <p> </p> <ul> <li>Click on <code>Global credentials (unrestricted)</code>.  </li> </ul> <p></p> <ul> <li>Click on <code>Add Credentials</code>.  </li> </ul> <p></p> <ul> <li>This will take you to the <code>New Credentials</code> page.</li> <li>Select <code>Kind</code> as <code>Username with password</code>.</li> <li>Select <code>Scope</code> as <code>Global (Jenkins, nodes, items, all child items etc)</code>.</li> <li>Enter your GitHub username in <code>Username</code></li> <li>Enter <code>Password</code> as <code>GitHub personal authentication token</code> (PAT) which can be acquired from Github account settings.  </li> <li><code>ID</code> is optional but you can enter a unique <code>ID</code>. <code>Description</code> can be left empty. But it is recommended to give a suitable but careful description by which administrator will be able to identify and use these credentials to set up github webhook</li> <li>Select <code>Create</code></li> <li>This process will look something like this</li> </ul> <p> </p> <ul> <li>Now credentials will be available in the credentials list and will be shown to you as well as administrator as shown in the image below. This will create an option in configurations for using these credentials in github webhook without changing or viewing them.  </li> </ul> <p> </p> <ul> <li>Note the credentials ID (as shown in the image below) and email it to the same administrator email on which you received the login credentials for Cloud-V. It is important that administrator knows the credentials ID because they will use it in the job build configurations.  </li> </ul> <p> </p> <p>Note: Please make sure to inform the administrator via email that you have added the credentials in Cloud-V Dashboard. Also, send administrator the ID of credentials via email.</p>"},{"location":"Software_Developer_Guide/#requirements-for-administrator","title":"Requirements for administrator","text":"<p>After the above setup is complete from software developer's side, developer will need to provide the administrator with following information.  </p> <ul> <li>Dependencies for running the project which can be packages which are needed to install in the RISC-V CI environment by administrator.</li> <li>Events for triggering the job build.</li> <li>URL of GitHub repository.</li> <li>Path and name of <code>cloud-v-pipeline</code> file on the provided GitHub repository.</li> <li>Any additional information which should be given for successful execution of job builds.</li> </ul>"},{"location":"Tooling/","title":"Tools on Cloud-V","text":""},{"location":"Tooling/#using-environment-modules","title":"Using Environment Modules","text":"<p>Users can use environment modules to load different versions of same program. For using environment modules the pattern is as follows:</p> <pre><code>module load &lt;PACKAGENAME/VERSION&gt;\n</code></pre> <p>Important Note: Be sure to use <code>#!/bin/bash -l</code> instead of <code>#!/bin/bash</code> in CI pipeline file since that is required for environment modules to load</p> <p>For example if you want to load python version 3.9.2 compiled for x86, you will need to use following command:  </p> <pre><code>module load python/3.9.2\n</code></pre> <p>For packages compiled for RISC-V architecture host, you will need to append <code>_riscv</code> to package name. For example, for python 3.8.15 compiled for RISC-V, following command will be used.  </p> <pre><code>module load python_riscv/3.8.15\n</code></pre>"},{"location":"cicd/","title":"Running CI/CD pipelines on RISC-V with Cloud-V","text":"<p>This document describes how someone can integrate their CI/CD projects on RISC-V compute machine.</p>"},{"location":"cicd/#supported-cicd-platforms-on-cloud-v","title":"Supported CI/CD platforms on Cloud-V","text":"<p>There is a dedicated page on Cloud-V website where you can check which version control platforms or CI/CD services are supported by Cloud-V with RISC-V compute machines. At present, developers can use these compute machines free of cost. </p> <p>Services which are labeled \"Coming Soon!\" are in progress and will be added soon.</p>"},{"location":"cicd/#supported-compute-machine-types","title":"Supported compute machine types","text":"<p>Cloud-V offers vendor-agnostic physical compute machines as well as emulated compute instances. </p> <p>Physical compute machines are the SBCs, desktop computers or laptops with RISC-V SoCs in them while the emulated compute instances are running on the x86 host machines. The emulated RISC-V machines use QEMU.</p> <p>See Compute Instances in Cloud-V for a complete list of devices available in cloud.</p>"},{"location":"cicd/#why-do-we-even-need-qemu","title":"Why do we even need QEMU","text":"<p>Every RISC-V SoC in the market supports only a limited set of extensions. So developers don't get flexibility to test the software on the desired ISA string. With QEMU, developers can have an exhaustive list of RISC-V extensions (which do not conflict with each other).</p> <p>One other advantage is that if the x86 host machine (which is running the QEMU) has good computing power (say 128 or 256 cores), then it can be leveraged by QEMU to provide a far better computing power than what can be provided by RISC-V physical boards available in the market at present.</p> <p>Check QEMU Specifications section for specifications QEMU system running in Cloud-V.</p>"},{"location":"compute_instances/","title":"Compute Instances in Cloud-V","text":"<p>This page lists the compute instances available to the users in Cloud-V along with their specifications. </p> <p>Each instance type shown below may represent one or more instances in actual deployment. The number of instances per type depends on system requirements and scaling configurations.  </p> Compute Instance: StarFive VisionFive 1 <p>Architecture: riscv64</p> <p>Clock Frequency (per core): 1.2GHz</p> <p>Core Count: 2</p> <p>Memory: 8 GB DDR4</p> <p>Type: Physical Board</p> <p> <p>ISA Extensions: Standard + Privilege</p> Show complete ISA string <p>         rv64imafdc_zicntr_zicsr_zifencei_zihpm       </p> Compute Instance: StarFive VisionFive 2 <p>Architecture: riscv64</p> <p>Clock Frequency (per core): 1.5GHz</p> <p>Core Count: 4</p> <p>Memory: 8 GB DDR4</p> <p>Type: Physical Board</p> <p> <p>ISA Extensions: Standard + Privilege</p> Show complete ISA string <p>         rv64imafdc_zicntr_zicsr_zifencei_zihpm_zba_zbb       </p> Compute Instance: SiFive HiFive Unleashed <p>Architecture: riscv64</p> <p>Clock Frequency (per core): 1GHz</p> <p>Core Count: 4</p> <p>Memory: 8 GB DDR4</p> <p>Type: Physical Board</p> <p> <p>ISA Extensions: Standard + Privilege</p> Show complete ISA string <p>         rv64imafdc_zicntr_zicsr_zifencei_zihpm       </p> Compute Instance: Banana Pi F3 <p>Architecture: riscv64</p> <p>Clock Frequency (per core): 1.6GHz</p> <p>Core Count: 8</p> <p>Memory: 4/16 GB DDR4</p> <p>Type: Physical Board</p> <p> <p>ISA Extensions: Standard + Privilege + Vector</p> Show complete ISA string <p>         rv64imafdcv_zicbom_zicboz_zicntr_zicond_zicsr_zifencei_zihintpause_zihpm_zfh_zfhmin_zca_zcd_zba_zbb_zbc_zbs_zkt_zve32f_zve32x_zve64d_zve64f_zve64x_zvfh_zvfhmin_zvkt_sscofpmf_sstc_svinval_svnapot_svpbmt       </p> Compute Instance: Milk-V Jupiter <p>Architecture: riscv64</p> <p>Clock Frequency (per core): 1.8 GHz</p> <p>Core Count: 8</p> <p>Memory: 16 GB DDR4</p> <p>Type: Physical Board</p> <p> <p>ISA Extensions: Standard + Privilege + Vector</p> Show complete ISA string <p>         rv64imafdcv_zicbom_zicboz_zicntr_zicond_zicsr_zifencei_zihintpause_zihpm_zfh_zfhmin_zca_zcd_zba_zbb_zbc_zbs_zkt_zve32f_zve32x_zve64d_zve64f_zve64x_zvfh_zvfhmin_zvkt_sscofpmf_sstc_svinval_svnapot_svpbmt       </p> Compute Instance: Milk-V Pioneer Box <p>Architecture: riscv64</p> <p>Clock Frequency (per core): 2 GHz</p> <p>Core Count: 64</p> <p>Memory: 128 GB DDR4</p> <p>Type: Desktop Computer</p> <p> <p>ISA Extensions: Standard + Privilege + T-head Vector</p> Show complete ISA string <p>         rv64imafdc_zicntr_zicsr_zifencei_zihpm_xtheadvector       </p> <p></p> Compute Instance: QEMU System riscv64 <p>Host Architecture: x86</p> <p>Host CPU: Intel(R) Xeon(R) Gold 6138</p> <p>Emulated Architecture: riscv64</p> <p>Clock Frequency (host): 2 GHz</p> <p>Core Count: 12</p> <p>Memory: 6 GB</p> <p>QEMU Version: 10.0.0</p> <p>QEMU OS: Ubuntu 24.04 LTS</p> <p>Type: Emulated RISC-V 64-bit</p> <p> <p>ISA Extensions: Standard + Privilege + Vector + Bit manipulation + Cryptography</p> Show complete ISA string <p>         rv64imafdch_zicbom_zicboz_zicntr_zicond_zicsr_zifencei_zihintntl_zihintpause_zihpm_zacas_zfa_zfh_zfhmin_zba_zbb_zbc_zbkb_zbkc_zbkx_zbs_zknd_zkne_zknh_zkr_zkt_zksed_zksh_ztso_zvbb_zvbc_zvfh_zvfhmin_zvkb_zvkg_zvkned_zvknha_zvknhb_zvksed_smaia_smstateen_ssaia_sscofpmf_sstc_svinval_svnapot_svpbmt       </p>"},{"location":"jenkins_gitlab_integration/","title":"Integrating GitLab with Jenkins","text":"<p>This documentation will cover how to create an integration between jenkins and GitLab. This will allow users to trigger jenkins job when a <code>merge request</code> or a <code>push</code> is detected in GitLab.  </p>"},{"location":"jenkins_gitlab_integration/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>GitLab plugin  </li> <li>Git plugin  </li> <li>GitLab repository with owner's credentials  </li> </ul>"},{"location":"jenkins_gitlab_integration/#configuring-jenkins-system","title":"Configuring Jenkins System","text":"<p>First jenkins needs to be configured.  </p> <ul> <li>Go to <code>Dashboard &gt; Manage Jenkins &gt; Configure System</code> </li> <li>Scroll down to <code>Gitlab</code> section  </li> <li> <p>Check <code>Enable authentication for '/project' end-point</code> </p> </li> <li> <p>Enter a <code>Connection name</code>.  </p> </li> <li>Enter <code>Gitlab host URL</code> as <code>https://gitlab.com/</code>. In case there is a different domain name, then enter there instead of above url.  </li> <li>In <code>Credentials</code>, click on <code>Add</code> then click on <code>Jenkins</code>.  </li> <li>In <code>Kind</code>, select <code>GitLab API token</code>.  </li> <li>In <code>API token</code>, enter the gitlab personal access token (this will be obtained below while configuring GitLab).  </li> <li>Click on <code>Advanced</code>.</li> <li>Click on <code>Test Connection</code>.  </li> <li>If everything goes right, it should print <code>success</code>.  </li> </ul>"},{"location":"jenkins_gitlab_integration/#configuring-gitlab","title":"Configuring GitLab","text":"<ul> <li>Click on profile avatar in the top right.  </li> <li>Click on <code>Edit profile</code>.  </li> </ul> <ul> <li>Click on <code>Access Tokens</code>.</li> <li>Create a new personal access token and copy it (this is the GitLab API token used in above section <code>Configuring Jenkins System</code>).  </li> <li>Go to repository settings.</li> <li>On left-side pane, select <code>Webhooks</code>.  </li> <li>Enter <code>GitLab webhook URL</code> (this is explained below in next section).  </li> <li>Enter <code>Secret Token</code> (this is explained in the below section).</li> <li>Check desirable trigger options.  </li> <li>Click <code>Add webhook</code>.</li> </ul>"},{"location":"jenkins_gitlab_integration/#configuring-jenkins-job","title":"Configuring Jenkins Job","text":"<ul> <li>Create a jenkins job.</li> <li>On job configuration page, scroll down to <code>Source Code Management</code>.  </li> <li>Select <code>Git</code>.  </li> <li>In <code>Credentials</code>, add the owner credentials of GitLab repository. This will be <code>Username and Password</code>.  </li> <li>Select the appropriate branch (generally it is <code>main</code>).  </li> <li>Scroll down to <code>Build Triggers</code>.  </li> <li>Check <code>Build when a change is pushed to GitLab</code>. There will also be a <code>GitLab webhook URL</code>. This is needed in GitLab. This URL will be called as <code>Webhook URL</code>.  </li> <li>Click on <code>Advanced</code> in the same option, then scroll down and generate a secret token. This token is also needed. This token will be called as <code>Secret Token</code>.</li> <li>Scroll down to <code>Post-build Actions</code> and select <code>Publish build status to GitLab</code>.  </li> <li>Click on <code>Apply</code> and <code>Save</code>.  </li> </ul> <p>Now whenever there will be a push (or whatever build trigger option is set in jenkins and gitlab webhook), specified jenkins job will be triggered.</p>"},{"location":"runner_specs/","title":"Specifications of compute instances in Cloud-V","text":"<p>This document contains the specifications of the compute instances available for users to run builds in Cloud-V. The term \"Compute Instance\", can also be safely interchanged with the terms \"Build Executor\" and \"Runner\".</p> Name CI Name String Architecture ISA String Cores Memory Compute Instance Type N/A J-x86-1 (or) J-QMU-1 x86_64 N/A 4 8GiB Hardware with application-level emulator intel i7-6500U J-TESTVM-1 x86_64 N/A 4 8GiB Hardware Raspberry Pi 4 Model B J-RASP4-1 aarch64 ARMv8-A 4 4GiB Hardware &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD QEMU System Linux J-QMS-1 riscv64 See Ext 1 at bottom 12 6GiB QEMU System emulator VisionFive 1 J-VF1-1 riscv64 rv64imafdc 2 8GiB Hardware VisionFive 1 J-VF1-2 riscv64 rv64imafdc 2 8GiB Hardware VisionFive 1 J-VF1-3 riscv64 rv64imafdc 2 8GiB Hardware VisionFive 2 J-VF2-1 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-2 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-3 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-4 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-5 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-6 riscv64 rv64imafdc 4 8GiB Hardware HiFive Unleashed J-HF-1 riscv64 rv64imafdc 4 8GiB Hardware Banana Pi F3 J-BPF3-1 riscv64 rv64imafdcv_sscofpmf_sstc_svpbmt_zicbom_zicboz_zicbop_zihintpause 8 4GiB Hardware Banana Pi F3 J-BPF3-2 riscv64 rv64imafdcv_sscofpmf_sstc_svpbmt_zicbom_zicboz_zicbop_zihintpause 8 16GiB Hardware Banana Pi F3 J-BPF3-3 riscv64 rv64imafdcv_sscofpmf_sstc_svpbmt_zicbom_zicboz_zicbop_zihintpause 8 16GiB Hardware Banana Pi F3 J-BPF3-4 riscv64 rv64imafdcv_sscofpmf_sstc_svpbmt_zicbom_zicboz_zicbop_zihintpause 8 16GiB Hardware Banana Pi F3 J-BPF3-5 riscv64 rv64imafdcv_sscofpmf_sstc_svpbmt_zicbom_zicboz_zicbop_zihintpause 8 16GiB Hardware Milk-V Jupiter J-JUPITER-1 riscv64 rv64imafdcv_sscofpmf_sstcc_svpbmt_zicbom_zicboz_zicbop_zihintpause 8 16GiB Hardware Milk-V Pioneer Box J-pioneer-1 riscv64 rv64imafdc_zicntr_zicsr_zifencei_zihpm_xtheadvector 64 8GiB Hardware ======= QEMU System Linux J-QMS-1 riscv64 See at the bottom 12 6GiB QEMU System emulator VisionFive 1 sf1-1 riscv64 rv64imafdc 2 8GiB Hardware VisionFive 1 sf1-2 riscv64 rv64imafdc 2 8GiB Hardware VisionFive 1 sf1-3 riscv64 rv64imafdc 2 8GiB Hardware VisionFive 2 sf2-1 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 sf2-2 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 sf2-3 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 sf2-4 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 sf2-5 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 sf2-6 riscv64 rv64imafdc 4 8GiB Hardware HiFive Unleashed unleashed1-1 riscv64 rv64imafdc 4 8GiB Hardware Banana Pi F3 BPF3-4G-1 riscv64 rv64imafdcv_sscofpmf_sstc_svpbmt_zicbom_zicboz_zicbop_zihintpause 1 4GiB Hardware Banana Pi F3 BPF3-16G-1 riscv64 rv64imafdcv_sscofpmf_sstc_svpbmt_zicbom_zicboz_zicbop_zihintpause 1 4GiB Hardware Milk-V Jupiter jupiter-16G-1 riscv64 rv64imafdcv_sscofpmf_sstc_svpbmt_zicbom_zicboz_zicbop_zihintpause 1 16GiB Hardware Milk-V Jupiter jupiter-16G-2 riscv64 rv64imafdcv_sscofpmf_sstc_svpbmt_zicbom_zicboz_zicbop_zihintpause 1 16GiB Hardware Milk-V Pioneer Box pioneer-128G-1 riscv64 rv64imafdcv_sscofpmf_sstc_svpbmt_zicbom_zicboz_zicbop_zihintpause 4 8GiB Hardware"},{"location":"runner_specs/#qemu-specifications","title":"QEMU Specifications:My Content Title","text":"<p>QEMU Version: 10.0.0 QEMU OS: Ubuntu 24.04 LTS QEMU specifications: 12 cores, 6 GB memory QEMU ISA String: </p> <pre><code>rv64imafdch_zicbom_zicboz_zicntr_zicond_zicsr_zifencei_zihintntl_zihintpause_zihpm_zacas_zfa_zfh_zfhmin_zba_zbb_zbc_zbkb_zbkc_zbkx_zbs_zknd_zkne_zknh_zkr_zkt_zksed_zksh_ztso_zvbb_zvbc_zvfh_zvfhmin_zvkb_zvkg_zvkned_zvknha_zvknhb_zvksed_smaia_smstateen_ssaia_sscofpmf_sstc_svinval_svnapot_svpbmt\n</code></pre> <p>c1be17c (Changed docs according to new format)</p> <p>Note: The <code>J-QMU-1</code> and <code>J-x86-1</code> are one and the same runner. The purpose of creating two separate executors for same hardware is that <code>J-x86-1</code> is supposed to be specifically for x86 architecture whereas <code>J-QMU-1</code> is specifically for the users who want to cross compile source code for riscv64 architecture and then use qemu-usermode to execute them. Nevertheless, the tooling available for <code>J-x86-1</code> can also be used for <code>J-QMU-1</code></p> <p>     This is the main content on the left side. You can include text, lists, code blocks, and more.   </p>"},{"location":"sandboxing/","title":"Using SSH access on RISC-V instance","text":"<p>Fill this form to request a RISC-V device from Cloud-V for SSH. List of available devices and their specifications can be found here</p> <p>After getting SSH command and credentials from Cloud-V administrator, users can log in to RISC-V instance and start development like any Linux machine without GUI.</p>"},{"location":"sandboxing/#user-access-rights","title":"User Access Rights","text":"<p>In the current version of the Cloud-V platform, users are provided with unprivileged SSH access, meaning <code>sudo</code> privileges are not available. However, users are welcome and encouraged to build software from source within their environment.</p> <p>A list of pre-installed tools is available on the tooling page for each device. For example, tools installed on the VisionFive 1 can be found on the VisionFive 1 Tooling page.</p> <p>If additional tools are needed, users may request their installation by contacting the system administrator.</p> <p>Privileged access will be added soon in an upcoming version of the platform.</p>"},{"location":"sandboxing/#setting-up-vscodium-with-risc-v","title":"Setting up VScodium with RISC-V","text":"<p>It may not be convinient for users to use terminal editors (like Vim and Nano) if they are not accustomed.</p> <p>At present VScode remote extension does not have support for RISC-V architecture but VScodium supports it.</p> <p>This section explains how you can set up VScodium on x86 machine to get remote development access to RISC-V compute instance (assuming both the machines have SSH installed).</p> <ol> <li>Install VScodium from this link</li> <li>Install Open Remote - SSH Extension from jeanp413</li> </ol> <p></p> <ol> <li>Use <code>Ctrl+Shift+p</code> to open command pallet and search remote <code>Remote-SSH: Connect to host...</code></li> <li>Enter username, publicly accessible IP and port of the compute instance which you want to connect to.</li> </ol>"},{"location":"setting_up_gitlab_runner/","title":"Adding RISC-V compute machine as GitLab Runner","text":"<p>This document explains how developers working on GitLab projects can add RISC-V machines provided by Cloud-V as a GitLab runners.</p>"},{"location":"setting_up_gitlab_runner/#register-the-runner","title":"Register the runner","text":"<p>For registering the runner, visit this link. Add the necessary information on the page and then select a compute machine to use as GitLab runner. </p> <p>After the necessary information is placed, clicking the submit button will try to register the runner. The registration can take some time so please be patient with next page loading. Once an attempt to register is finished, you will see the result accordingly on the next page.</p>"},{"location":"setting_up_gitlab_runner/#how-the-gitlab-runner-works","title":"How the GitLab Runner works","text":"<p>The GitLab runner runs on a non-sudo user on the specified RISC-V machines. Every RISC-V machine can be a shared runner between different GitLab projects. The GitLab runner package runs as docker executor on the RISC-V machine so that every job is isolated from other jobs. The necessary information about the specification of GitLab runner is mentioned on the GitLab runner registration page of Cloud-V.</p> <p>Following diagram presents a view of how the job runs on the RISC-V machines in the Cloud-V.</p> <p> </p>"},{"location":"tooling_BPF3-x/","title":"Tools on Banana Pi F3 (BPI-F3) compute instance","text":"<p>Operating System: Bianbu 2.2</p> Package Name Version autoconf 2.71 automake 1.16.5 gcc 13.2.0 g++ 13.2.0 gfortran 13.2.0 git 2.43.0 openssh-server 9.6p1 openjdk-21-jdk 21.0.5 make 4.3 cmake 3.28.3 openssl 3.0.13 picocom 3.1 minicom 2.9 ninja-build 1.11.1 ruby 3.2.3 golang 1.22.2 rustc 1.75.0 flex 2.6.4 bison 3.8.2 gperf 3.1 python3 3.12.3 pip 24.0 htop 3.3.0 tmux 3.4 pkg-config 1.8.1 libtool 2.4.7 gettext 0.21 clang 18.1.8 ccache 4.9.1 gdb 15.0.50 strace 6.8 checkmk 1.3.4 lcov 2.0 shellcheck 0.9.0 bats 1.10.0 doxygen 1.9.8 texinfo 7.1 pandoc 3.1.3 man-db 2.12.0 graphviz (dot) 2.43.0 perl 5.38.2 lua5.4 5.4 vim 9.1 emacs 29.3 neovim 0.9.5 zip 3.0 rsync 3.2.7 tar 1.35"},{"location":"tooling_J-K230-1/","title":"Tools on <code>J-K230-1</code> node","text":"<p>This compute instance is Starfive's VisionFive 2 board and it has available packages ONLY for RISC-V architecture.  </p> <p>Operating System: Debian 12 (Bookworm)</p> Tool Version Installed from Git 2.40.1 apt OpenJDK 21.0.2 apt GCC 13.2.0 apt Python3 3.11.6 apt OpenSSL 3.0.10 apt Ruby 3.1.2p20 apt Go 1.21.1 apt rustc 1.75.0 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.27.4 apt make 4.3 apt automake 1.16.5 apt gfortran 13.2.0 apt openssh-server 9.3p1 apt vim 9.0 apt vim 3.3a apt"},{"location":"tooling_J-QMS-1/","title":"Tools on <code>qemu-system-riscv64</code> node","text":"<p>Operating System: Ubuntu 24.04.4 LTS  QEMU Version: 10.0.0</p> Package Name Version autoconf 2.71 automake 1.16.5 gcc 13.3.0 g++ 13.3.0 gfortran 13.3.0 git 2.43.0 net-tools (ifconfig) 2.10 openssh-server 9.6p1 openjdk-21-jdk 21.0.7 make 4.3 cmake 3.28.3 openssl 3.0.13 picocom 3.1 minicom 2.9 ninja-build 1.11.1 ruby 3.2.3 golang 1.22.2 rustc 1.75.0 flex 2.6.4 bison 3.8.2 gperf 3.1 python3 3.12.3 pip 24.0 prometheus-node-exporter 1.7.0 sssd-ldap 2.9.4 ldap-utils 2.6.7 htop 3.3.0 tmux 3.4 pkg-config 1.8.1 libtool 2.4.7 gettext 0.21 clang 18.1.3 ccache 4.9.1 gdb 15.0.50 strace 6.8 checkmk-agent 1.3.4 lcov 2.0 shellcheck 0.9.0 bats 1.10.0 doxygen 1.9.8 texinfo 7.1 pandoc 3.1.3 man-db 2.12.0 graphviz (dot) 2.43.0 perl 5.38.2 lua5.4 5.4 vim 9.1 emacs 29.3 neovim 0.9.5 zip 3.0 rsync 3.2.7 tar 1.35"},{"location":"tooling_J-QMU-1J-VM-1/","title":"Tools on <code>J-x86-1</code> or <code>J-QMU-1</code> node (Under Maintenance)","text":"<p>Tools which are mentioned for <code>x86</code> architecture are able to run on<code>J-x86-1</code>. Tools which are mentioned for <code>RISC-V</code> architecture are able to run on <code>J-QMU-1</code>.</p> <p>The packages which are supported for <code>QEMU User mode</code> can be used by normal commands once they are loaded.</p> <p>Here <code>PACKAGE_NAME</code> is the package which you want to run on QEMU user mode.</p> <p>Operating System: Debian 11 (bullseye) QEMU User Mode Version: Different Versions (see the table below)</p>"},{"location":"tooling_J-QMU-1J-VM-1/#tooling-available-for-j-x86-1","title":"Tooling available for <code>J-x86-1</code>","text":"<p>The tools available for <code>J-x86-1</code> is for use on x86 architecture and these tools do not support execution on RISC-V architecture</p> Tool Versions Installed from Host Architecture Environment Modules Support Git 2.3.0.2 source x86 N/A OpenJDK 19.0.1 apt x86 N/A GCC 10.4.0, 12.2.0 apt x86 Yes Python3 3.8.15, 3.9.2 source x86 Yes Go 1.18.8 apt x86 N/A rustc 1.65.0 source x86 N/A Flex 2.6.4 apt x86 N/A Ninja 1.10.1-1 apt x86 N/A Bison 3.7.5 apt x86 N/A autoconf 2.69 apt x86 N/A gperf 2.2.4 apt x86 N/A spike 1.1.1-dev source x86 Yes Verilator 4.038 apt x86 N/A Sail (riscv_sim_RV64, riscv_sim_RV32) 0.5 source x86 Yes cmake 3.18.4 apt x86 N/A make 4.3 apt x86 N/A ARM EABI GNU toolchain 13.2.Rel1 source x86 Yes qemu-system-arm 8.2.2 source x86 Yes SCons 4.7.0 source x86 Yes"},{"location":"tooling_J-QMU-1J-VM-1/#tooling-available-for-j-qmu-1","title":"Tooling available for <code>J-QMU-1</code>","text":"<p>The tools available for <code>J-QMU-1</code> is for use on RISC-V architecture and these tools do not support execution on x86 architecture</p> Tool Versions Installed from Host Architecture Environment Modules Support Python3 3.8.15 source RISC-V Yes zlib 1.2.13 source RISC-V N/A OpenSSL 1.1.1r source RISC-V Yes Ruby (without IRB) 3.2.0dev source RISC-V Yes rustc 1.65.0 source RISC-V N/A Flex 2.6.4 source RISC-V yes Ninja 1.12.0.git source RISC-V Yes Bison 3.8.2, 2.3 source RISC-V Yes clang 16.0.0 source RISC-V (cross compiler) Yes riscv-pk 1.0.0-91-g573c858 source RISC-V Yes"},{"location":"tooling_J-QMU-1J-VM-1/#qemu-user-mode-and-risc-v-gnu-cross-compilers","title":"QEMU User mode and RISC-V GNU Cross compilers","text":"<p>From now on RISC-V cross-compilers can only be loaded with their respective QEMU User mode on Cloud-V. This is configured so that there is no confusion between toolchain version and qemu user mode being used because both of these will be \"generally\" taken from the latest releases of nightly builds. Loading a certain RISC-V toolchain using environment modules will automatically load the respective qemu usermode version unless otherwise specified.</p> <p>The loading pattern for RISC-V 64-bit GNU Glibc toolchain will be as follows:</p> <pre><code>module load riscv64-gnu-glibc/&lt;release-date&gt;\n</code></pre> <p>And the loading pattern for RISC-V 64-bit GNU Glibc toolchain will be as follows:</p> <pre><code>module load riscv64-gnu-elf/&lt;release-date&gt;\n</code></pre> <p>Following table provides relevant information about version of the toolchain and the respective QEMU User mode version (where the release date is mentioned in pattern <code>MMDDYYYY</code>).</p> Release date GNU Toolchain version (elf and glibc) QEMU Version 03012024 13.2.0 8.2.1 02022024 13.2.0 8.2.1 02022024 13.2.0 8.1.1 <p>Note: The <code>J-QMU-1</code> and <code>J-x86-1</code> are one and the same runner. The purpose of creating two separate executors for same hardware is that <code>J-x86-1</code> is supposed to be specifically for x86 architecture whereas <code>J-QMU-1</code> is specifically for the users who want to cross compile source code for riscv64 architecture and then use qemu-usermode to execute them. Nevertheless, the tooling available for <code>J-x86-1</code> can also be used for <code>J-QMU-1</code></p>"},{"location":"tooling_jupiter-x/","title":"Tools on Milk-V Jupiter compute instance","text":"<p>Operating System: Bianbu 2.1</p> Package Name Version autoconf 2.71 automake 1.16.5 gcc 13.2.0 g++ 13.2.0 gfortran 13.2.0 git 2.43.0 net-tools (ifconfig) 2.10 openssh-server 9.6p1 openjdk-21-jdk 21.0.5 make 4.3 cmake 3.28.3 openssl 3.0.13 picocom 3.1 minicom 2.9 ninja-build 1.11.1 ruby 3.2.3 golang 1.22.2 rustc 1.75.0 flex 2.6.4 bison 3.8.2 gperf 3.1 python3 3.12.3 pip 24.0 htop 3.3.0 tmux 3.4 pkg-config 1.8.1 libtool 2.4.7 gettext 0.21 clang 18.1.8 ccache 4.9.1 gdb 15.0.50 strace 6.8 checkmk 1.3.4 lcov 2.0 shellcheck 0.9.0 bats 1.10.0 doxygen 1.9.8 texinfo 7.1 pandoc 3.1.3 man-db 2.12.0 graphviz (dot) 2.43.0 perl 5.38.2 lua5.4 5.4 vim 9.1 emacs 29.3 neovim 0.9.5 zip 3.0 rsync 3.2.7 tar 1.35"},{"location":"tooling_pioneer-x/","title":"Tools on Milk-V Pioneer Box compute instance","text":"<p>The Milk-V Pioneer Box runs Debian Trixie, a development release. This distribution includes mostly the latest, non-LTS versions of packages, which are frequently updated and subject to change. </p> <p>Operating System: Debian 13 (Trixie)</p> Package Name Version autoconf 2.72 automake 1.17 gcc 14.2.0 g++ 14.2.0 gfortran 14.2.0 git 2.47.2 openssh-server (sshd) 10.0 openjdk-21-jdk 21.0.7 make 4.4.1 cmake 3.31.6 openssl 3.5.0 picocom 3.1 minicom 2.10 ninja-build 1.12.1 ruby 3.3.8 golang 1.24.2 rustc 1.85.0 flex 2.6.4 bison 3.8.2 gperf 3.2.1 python3 3.13.3 pip 25.1.1 htop 3.4.1 tmux 3.5a pkg-config 1.8.1 libtool \u2014 gettext 0.23.1 clang 19.1.7 ccache 4.11.2 gdb 16.3 strace 6.13 checkmk-agent \u2014 lcov 2.0 shellcheck \u2014 bats 1.11.1 doxygen 1.9.8 texinfo 7.1.1 pandoc 3.1.11.1 man-db 2.13.1 graphviz (dot) 2.42.4 perl \u2014 lua5.4 \u2014 vim 9.1 emacs 30.1 neovim 0.10.4 zip \u2014 rsync 3.4.1 tar 1.35"},{"location":"tooling_sf1-x/","title":"Tools on VisionFive 1 compute machine","text":"<p>Operating System: Ubuntu 24.04 LTS</p> Package Name Version autoconf 2.71 automake 1.16.5 gcc 13.3.0 g++ 13.3.0 gfortran 13.3.0 git 2.43.0 net-tools (ifconfig) 2.10 openssh-server 9.6p1 openjdk-21-jdk 21.0.7 make 4.3 cmake 3.28.3 openssl 3.0.13 picocom 3.1 minicom 2.9 ninja-build 1.11.1 ruby 3.2.3 golang 1.22.2 rustc 1.75.0 flex 2.6.4 bison 3.8.2 gperf 3.1 python3 3.12.3 pip 24.0 prometheus-node-exporter 1.7.0 sssd-ldap 2.9.4 ldap-utils 2.6.7 htop 3.3.0 tmux 3.4 pkg-config 1.8.1 libtool 2.4.7 gettext 0.21 clang 18.1.3 ccache 4.9.1 gdb 15.0.50 strace 6.8 checkmk-agent 1.3.4 lcov 2.0 shellcheck 0.9.0 bats 1.10.0 doxygen 1.9.8 texinfo 7.1 pandoc 3.1.3 man-db 2.12.0 graphviz (dot) 2.43.0 perl 5.38.2 lua5.4 5.4 vim 9.1 emacs 29.3 neovim 0.9.5 zip 3.0 rsync 3.2.7 tar 1.35"},{"location":"tooling_sf2-x/","title":"Tools in VisionFive 2 compute machine","text":"<p>Operating System: Ubuntu 24.04 LTS</p> Package Name Version autoconf 2.71 automake 1.16.5 gcc 13.3.0 g++ 13.3.0 gfortran 13.3.0 git 2.43.0 openssh-server 9.6p1 openjdk-21-jdk 21.0.7 make 4.3 cmake 3.28.3 openssl 3.0.13 picocom 3.1 minicom 2.9 ninja-build 1.11.1 ruby 3.2.3 golang 1.22.2 rustc 1.75.0 flex 2.6.4 bison 3.8.2 gperf 3.1 python3 3.12.3 pip 24.0 htop 3.3.0 tmux 3.4 pkg-config 1.8.1 libtool 2.4.7 gettext 0.21 clang 18.1.3 ccache 4.9.1 gdb 15.0.50 strace 6.8 check 1.3.4 lcov 2.0 shellcheck 0.9.0 bats 1.10.0 doxygen 1.9.8 texinfo 7.1 pandoc 3.1.3 man-db 2.12.0 graphviz (dot) 2.43.0 perl 5.38.2 lua5.4 5.4 vim 9.1 emacs 29.3 neovim 0.9.5 zip 3.0 rsync 3.2.7 tar 1.35"},{"location":"tooling_unleashed1-1/","title":"Tools on SiFive HiFive Unleashed compute machine","text":"<p>Operating System: Ubuntu 24.04 LTS</p> Package Name Version autoconf 2.71 automake 1.16.5 gcc 13.3.0 g++ 13.3.0 gfortran 13.3.0 git 2.43.0 openssh-server 9.6p1 openjdk-21-jdk 21.0.7 make 4.3 cmake 3.28.3 openssl 3.0.13 picocom 3.1 minicom 2.9 ninja-build 1.11.1 ruby 3.2.3 golang 1.22.2 rustc 1.75.0 flex 2.6.4 bison 3.8.2 gperf 3.1 python3 3.12.3 pip 24.0 htop 3.3.0 tmux 3.4 pkg-config 1.8.1 libtool 2.4.7 gettext 0.21 clang 18.1.3 ccache 4.9.1 gdb 15.0.50 strace 6.8 check 1.3.4 lcov 2.0 shellcheck 0.9.0 bats 1.10.0 doxygen 1.9.8 texinfo 7.1 pandoc 3.1.3 man-db 2.12.0 graphviz (dot) 2.43.0 perl 5.38.2 lua5.4 5.4 vim 9.1 emacs 29.3 neovim 0.9.5 zip 3.0 rsync 3.2.7 tar 1.35"}]}