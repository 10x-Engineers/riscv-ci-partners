<<<<<<< HEAD
{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cloud-V Documentation This documentation lists the tooling and necessary information to use Cloud-V for RISC-V application development on RISC-V compute instances provided by Cloud-V Overview Slides: link Youtube Channel: youtube.com/@Cloud-V","title":"Home"},{"location":"#cloud-v-documentation","text":"This documentation lists the tooling and necessary information to use Cloud-V for RISC-V application development on RISC-V compute instances provided by Cloud-V Overview Slides: link Youtube Channel: youtube.com/@Cloud-V","title":"Cloud-V Documentation"},{"location":"Creating_CI_CD_pipeline/","text":"Creating a CI/CD pipeline in Jenkins Pre Requisistes For sake of this documentation, jenkins built-in sample script is used to create and execute a cd/cd pipeline in jenkins. In the built-in script maven is used as M3, so one must install Maven plugin inside jenkins and name it M3 . Usually maven is already present inside jenkins and can be configured from Global Configuration Tools . Following steps demonstrate configuring Maven plugin. Go to Jenkins Dashboard and click on Manage Jenkins In Manage Jenkins , under System Configuration section, click on Global Tool Configuration . In Global Tool Configuration , scroll down to Maven section and click on the respective option under the Maven Section (should be Maven installations\u2026 or Add Maven ). Under Maven installations , enter M3 in \u201cname\u201d text box, check Install Automatically and select Version greater than 3, then click Apply and Save . This should install Maven version 3 and configure as M3 . Steps for Jenkins pipeline creation After installing Jenkins and having all the suggested plugins installed, go to Jenkins dashboard and click on Create Job . On the next page, give your pipeline a name, select Pipeline and click OK . A Configuration page for the pipeline will appear. Select Build Triggers options and General options according to need and scroll down to the Pipeline section. Definition section contains configuration for stages and steps of the pipeline. Under Definition section, you can either choose Pipeline script and try writing your own script or try some sample pipeline (like Hello World , Github+Maven etc) or you could select Pipeline script from SCM and give a github repository containing configurations of Pipeline. Press Save and Apply . This should take you to the Pipeline and you can build the pipeline and if no unresolved dependencies are present, the pipeline should build without any error.","title":"Creating a CI/CD pipeline in Jenkins"},{"location":"Creating_CI_CD_pipeline/#creating-a-cicd-pipeline-in-jenkins","text":"","title":"Creating a CI/CD pipeline in Jenkins"},{"location":"Creating_CI_CD_pipeline/#pre-requisistes","text":"For sake of this documentation, jenkins built-in sample script is used to create and execute a cd/cd pipeline in jenkins. In the built-in script maven is used as M3, so one must install Maven plugin inside jenkins and name it M3 . Usually maven is already present inside jenkins and can be configured from Global Configuration Tools . Following steps demonstrate configuring Maven plugin. Go to Jenkins Dashboard and click on Manage Jenkins In Manage Jenkins , under System Configuration section, click on Global Tool Configuration . In Global Tool Configuration , scroll down to Maven section and click on the respective option under the Maven Section (should be Maven installations\u2026 or Add Maven ). Under Maven installations , enter M3 in \u201cname\u201d text box, check Install Automatically and select Version greater than 3, then click Apply and Save . This should install Maven version 3 and configure as M3 .","title":"Pre Requisistes"},{"location":"Creating_CI_CD_pipeline/#steps-for-jenkins-pipeline-creation","text":"After installing Jenkins and having all the suggested plugins installed, go to Jenkins dashboard and click on Create Job . On the next page, give your pipeline a name, select Pipeline and click OK . A Configuration page for the pipeline will appear. Select Build Triggers options and General options according to need and scroll down to the Pipeline section. Definition section contains configuration for stages and steps of the pipeline. Under Definition section, you can either choose Pipeline script and try writing your own script or try some sample pipeline (like Hello World , Github+Maven etc) or you could select Pipeline script from SCM and give a github repository containing configurations of Pipeline. Press Save and Apply . This should take you to the Pipeline and you can build the pipeline and if no unresolved dependencies are present, the pipeline should build without any error.","title":"Steps for Jenkins pipeline creation"},{"location":"Creating_jenkinsfile/","text":"Creating a Cloud-V CI Pipeline file What is a Cloud-V CI Pipeline file A cloud-v-pipeline file is a Continuous Integration (CI) jenkins pipeline script which is written in Groovy. It describes various stages (and possibly steps) which are executed in the defined pattern. These stages can be written in bash or they can be written in Groovy itself. Mainly there can be two types of Cloud-V pipeline files are of two types: Scripted: Only contains stages Declarative: Contains stages as well as steps (more feature-rich and recommended) This documentation will cover how to create a cloud-v-pipeline file with bash script inside it and run it on various compute instances (which are known as Nodes in jenkins). Jenkins Node In jenkins, node represents a compute instance. In simple words, it is the platform on which our job build is going to run. Jenkins Master There is a jenkins Master node which is actually the compute instance on which Jenkins is installed. It is the node which schedules builds on runners. For security reason, no job is allowed to run on this node. Jenkins Slave Jenkins slave nodes are the compute instances on which our job builds run safely. They may be attached with jenkins master via hardware or they may be connected through remote SSH connection. cloud-v-pipeline file written with bash Simple Hello World cloud-v-pipeline file In Cloud-V, all the platforms are running Linux operating system, so the cloud-v-pipeline should be written in bash. Following script is an example of how can we run a bash script in scripted cloud-v-pipeline . node{ stage('*** Phase 1 ***') { //Using bash commands sh '''#!/bin/bash echo \"Hello World !\\n\" ''' } } The keyword sh is used to specify a shell script As there is nothing mentioned with node , so the script will run job build on any available compute instance. cloud-v-pipeline for a Specific Node In previous script, the cloud-v-pipeline would run on any compute instances which are available. But in case if someone wants to run a job build on specific node, then a compute instance name must be specified with keyword node . The following script is an example of running above Hello World program on node named hifive_unleashed . node('hifive_unleashed'){ stage('*** Phase 1 ***') { //Using bash commands sh '''#!/bin/bash echo \"Hello World !\\n\" ''' } } cloud-v-pipeline for Cross-Platform Compilation and Execution Cloud-V supports cross-compilation and execution on emulated RISC-V compute instances. Following tools help in cross compilation and cross-platfrom execution: RISC-V GNU Toolchain QEMU user mode (for running standalone binaries) QEMU System (for running application in Linux) An example pipeline script is given below in scripted pipeline. node('x86_runner2'){ checkout scm //Getting content of this repo stage('*** Compilation Phase ***') { // for display purposes //Compiling helloworld.c using bash commands sh '''#!/bin/bash gcc -g ./helloworld.c -o helloworld.out riscv64-unknown-linux-gnu-gcc ./helloworld.c -o helloworld_riscv_compiled.out //Cross compiling for RISC-V ''' } stage (' *** Running Binaries ***'){ sh '''#!/bin/bash ./helloworld.out qemu-riscv64 -L $RISCV_SYSROOT helloworld_riscv_compiled.out //Running executable on RISC-V emulated platform ''' } } The equivalent declarative pipeline is as follows: pipeline { agent {label \"x86_runner2\"} stages { stage('Clone Repository') { steps('delegate'){ checkout scm //Clones the repository on the local machine } } stage ('Compilation Phase'){ steps{ sh '''#!/bin/bash gcc -g ./helloworld.c -o helloworld.out ''' sh '''#!/bin/bash riscv64-unknown-linux-gnu-gcc ./helloworld.c -o helloworld_riscv_compiled.out ''' } } stage ('Running Binaries'){ steps { sh ''' #!/bin/bash ./helloworld.out ''' sh'''#!/bin/bash qemu-riscv64 -L $RISCV_SYSROOT helloworld_riscv_compiled.out ''' } } } } Reference Links https://www.jenkins.io/doc/book/pipeline/syntax/","title":"Creating a CI pipeline file"},{"location":"Creating_jenkinsfile/#creating-a-cloud-v-ci-pipeline-file","text":"","title":"Creating a Cloud-V CI Pipeline file"},{"location":"Creating_jenkinsfile/#what-is-a-cloud-v-ci-pipeline-file","text":"A cloud-v-pipeline file is a Continuous Integration (CI) jenkins pipeline script which is written in Groovy. It describes various stages (and possibly steps) which are executed in the defined pattern. These stages can be written in bash or they can be written in Groovy itself. Mainly there can be two types of Cloud-V pipeline files are of two types: Scripted: Only contains stages Declarative: Contains stages as well as steps (more feature-rich and recommended) This documentation will cover how to create a cloud-v-pipeline file with bash script inside it and run it on various compute instances (which are known as Nodes in jenkins).","title":"What is a Cloud-V CI Pipeline file"},{"location":"Creating_jenkinsfile/#jenkins-node","text":"In jenkins, node represents a compute instance. In simple words, it is the platform on which our job build is going to run.","title":"Jenkins Node"},{"location":"Creating_jenkinsfile/#jenkins-master","text":"There is a jenkins Master node which is actually the compute instance on which Jenkins is installed. It is the node which schedules builds on runners. For security reason, no job is allowed to run on this node.","title":"Jenkins Master"},{"location":"Creating_jenkinsfile/#jenkins-slave","text":"Jenkins slave nodes are the compute instances on which our job builds run safely. They may be attached with jenkins master via hardware or they may be connected through remote SSH connection.","title":"Jenkins Slave"},{"location":"Creating_jenkinsfile/#cloud-v-pipeline-file-written-with-bash","text":"","title":"cloud-v-pipeline file written with bash"},{"location":"Creating_jenkinsfile/#simple-hello-world-cloud-v-pipeline-file","text":"In Cloud-V, all the platforms are running Linux operating system, so the cloud-v-pipeline should be written in bash. Following script is an example of how can we run a bash script in scripted cloud-v-pipeline . node{ stage('*** Phase 1 ***') { //Using bash commands sh '''#!/bin/bash echo \"Hello World !\\n\" ''' } } The keyword sh is used to specify a shell script As there is nothing mentioned with node , so the script will run job build on any available compute instance.","title":"Simple Hello World cloud-v-pipeline file"},{"location":"Creating_jenkinsfile/#cloud-v-pipeline-for-a-specific-node","text":"In previous script, the cloud-v-pipeline would run on any compute instances which are available. But in case if someone wants to run a job build on specific node, then a compute instance name must be specified with keyword node . The following script is an example of running above Hello World program on node named hifive_unleashed . node('hifive_unleashed'){ stage('*** Phase 1 ***') { //Using bash commands sh '''#!/bin/bash echo \"Hello World !\\n\" ''' } }","title":"cloud-v-pipeline for a Specific Node"},{"location":"Creating_jenkinsfile/#cloud-v-pipeline-for-cross-platform-compilation-and-execution","text":"Cloud-V supports cross-compilation and execution on emulated RISC-V compute instances. Following tools help in cross compilation and cross-platfrom execution: RISC-V GNU Toolchain QEMU user mode (for running standalone binaries) QEMU System (for running application in Linux) An example pipeline script is given below in scripted pipeline. node('x86_runner2'){ checkout scm //Getting content of this repo stage('*** Compilation Phase ***') { // for display purposes //Compiling helloworld.c using bash commands sh '''#!/bin/bash gcc -g ./helloworld.c -o helloworld.out riscv64-unknown-linux-gnu-gcc ./helloworld.c -o helloworld_riscv_compiled.out //Cross compiling for RISC-V ''' } stage (' *** Running Binaries ***'){ sh '''#!/bin/bash ./helloworld.out qemu-riscv64 -L $RISCV_SYSROOT helloworld_riscv_compiled.out //Running executable on RISC-V emulated platform ''' } } The equivalent declarative pipeline is as follows: pipeline { agent {label \"x86_runner2\"} stages { stage('Clone Repository') { steps('delegate'){ checkout scm //Clones the repository on the local machine } } stage ('Compilation Phase'){ steps{ sh '''#!/bin/bash gcc -g ./helloworld.c -o helloworld.out ''' sh '''#!/bin/bash riscv64-unknown-linux-gnu-gcc ./helloworld.c -o helloworld_riscv_compiled.out ''' } } stage ('Running Binaries'){ steps { sh ''' #!/bin/bash ./helloworld.out ''' sh'''#!/bin/bash qemu-riscv64 -L $RISCV_SYSROOT helloworld_riscv_compiled.out ''' } } } }","title":"cloud-v-pipeline for Cross-Platform Compilation and Execution"},{"location":"Creating_jenkinsfile/#reference-links","text":"https://www.jenkins.io/doc/book/pipeline/syntax/","title":"Reference Links"},{"location":"Multinode_Pipelines/","text":"Multinode Pipelines One can create multinode pipelines in jenkins freestyle pipeline job as well as in jenkins native pipeline job. In such a pipeline, different stages of the pipeline can run on separate nodes. The jenkins native pipeline job can have two types. Scripted Pipeline Declarative pipeline For the sake of this documentation, two nodes are used: container-node and container-node2 Label In jenkins, label specifies where the job or a stage of the job can run. In jenkins, a label can be explicitly added in the node's configuration so that each time that label is mentioned in a jenkins job, that specific node is used for building the job, or the name of the node can also be used as label in case if no label is added in the node. Creating multinode pipelines in Jenkins native pipeline job Scripted Pipeline For creating a multinode pipeline with jenkins native scripted pipeline job, jenkinsfile can be tweaked. The keyword node can be used to specify the node in which the stages are going to run and this keyword can also be used in between stages. Following screenshot indicates a code in which first stage runs on node named container-node . While the second node runs on node named container-node2 . node ('container-node') { stage ('*** Creating a directory in container-node ***'){ sh '''#!/bin/bash mkdir newdir_container-node-$RANDOM ''' } stage ('*** Creating a directory in container-node2'){ node ('container-node2'){ sh'''#!/bin/bash mkdir newdir_container-node2-$RANDOM ''' } } } Declarative pipeline For a declarative pipeline the keyword label can be used in the agent block. This can either be done at the start of the pipeline after the keyword pipeline or it can be used inside the stages. The following code builds two stages each of which is built in separate node. The keyword none with agent means that the agent is not specified globally for each stage and it should be specified inside each stage. pipeline { agent none // Means no agent specified. This means each node will specify its own agent stages { stage('container-node') { agent{ label \"container-node\" //Selecting container-node for this stage } steps { sh '''#!/bin/bash echo 'Hello container-node' mkdir \"newdir-container-node-$RANDOM\" ''' } } stage('container-node2'){ agent{ label \"container-node2\" //Selecting container-node2 for this stage } steps{ sh'''#!/bin/bash echo ''Hello container-node2 mkdir \"newdir-container-node2-$RANDOM\" ''' } } } } Creating multinode pipelines using Jenkins freestyle jobs Jenkins freestyle jobs can be combined together by mentioning post-build jobs in the freestyle job settings. Freestyle jobs are customizable and offer more options than jenkins native pipeline jobs. Each job represents a stage and each stage has separate settings. One can use jenkins freestyle jobs to run on a separate node by selecting the option Restrict where this project can be run in the General section of the job's configuration. In the Label Expression , the node's name can be mentioned in which to run the job. The following image shows the option from job's configuration. This can be done in all the job's configuration for the specified node.","title":"Multinode Pipelines"},{"location":"Multinode_Pipelines/#multinode-pipelines","text":"One can create multinode pipelines in jenkins freestyle pipeline job as well as in jenkins native pipeline job. In such a pipeline, different stages of the pipeline can run on separate nodes. The jenkins native pipeline job can have two types. Scripted Pipeline Declarative pipeline For the sake of this documentation, two nodes are used: container-node and container-node2","title":"Multinode Pipelines"},{"location":"Multinode_Pipelines/#label","text":"In jenkins, label specifies where the job or a stage of the job can run. In jenkins, a label can be explicitly added in the node's configuration so that each time that label is mentioned in a jenkins job, that specific node is used for building the job, or the name of the node can also be used as label in case if no label is added in the node.","title":"Label"},{"location":"Multinode_Pipelines/#creating-multinode-pipelines-in-jenkins-native-pipeline-job","text":"","title":"Creating multinode pipelines in Jenkins native pipeline job"},{"location":"Multinode_Pipelines/#scripted-pipeline","text":"For creating a multinode pipeline with jenkins native scripted pipeline job, jenkinsfile can be tweaked. The keyword node can be used to specify the node in which the stages are going to run and this keyword can also be used in between stages. Following screenshot indicates a code in which first stage runs on node named container-node . While the second node runs on node named container-node2 . node ('container-node') { stage ('*** Creating a directory in container-node ***'){ sh '''#!/bin/bash mkdir newdir_container-node-$RANDOM ''' } stage ('*** Creating a directory in container-node2'){ node ('container-node2'){ sh'''#!/bin/bash mkdir newdir_container-node2-$RANDOM ''' } } }","title":"Scripted Pipeline"},{"location":"Multinode_Pipelines/#declarative-pipeline","text":"For a declarative pipeline the keyword label can be used in the agent block. This can either be done at the start of the pipeline after the keyword pipeline or it can be used inside the stages. The following code builds two stages each of which is built in separate node. The keyword none with agent means that the agent is not specified globally for each stage and it should be specified inside each stage. pipeline { agent none // Means no agent specified. This means each node will specify its own agent stages { stage('container-node') { agent{ label \"container-node\" //Selecting container-node for this stage } steps { sh '''#!/bin/bash echo 'Hello container-node' mkdir \"newdir-container-node-$RANDOM\" ''' } } stage('container-node2'){ agent{ label \"container-node2\" //Selecting container-node2 for this stage } steps{ sh'''#!/bin/bash echo ''Hello container-node2 mkdir \"newdir-container-node2-$RANDOM\" ''' } } } }","title":"Declarative pipeline"},{"location":"Multinode_Pipelines/#creating-multinode-pipelines-using-jenkins-freestyle-jobs","text":"Jenkins freestyle jobs can be combined together by mentioning post-build jobs in the freestyle job settings. Freestyle jobs are customizable and offer more options than jenkins native pipeline jobs. Each job represents a stage and each stage has separate settings. One can use jenkins freestyle jobs to run on a separate node by selecting the option Restrict where this project can be run in the General section of the job's configuration. In the Label Expression , the node's name can be mentioned in which to run the job. The following image shows the option from job's configuration. This can be done in all the job's configuration for the specified node.","title":"Creating multinode pipelines using Jenkins freestyle jobs"},{"location":"Software_Developer_Guide/","text":"Software Developer Guide for RISC-V CI A software developer is the end-user who will develop or build his/her projects on RISC-V compute instance using RISC-V CI. This guide will cover all the things a software developer needs to integrate their project with RISC-V continuous integration. Note: All the compute instances have restrictions regarding which jobs will run on the compute instance. The administrator has to allow you for using specific instance. Be sure to contact administrator and tell them which instance you want to use There are currently two ways to integrate version control (Git) project with Cloud-V. Using Cloud-V automatic integration (beta) Manually integrating your project with Cloud-V Pre-requisites GitHub account. GitHub project repository with owner rights. Access to https://dash.cloud-v.co and https://cloud-v.co (visit this link to request the access) Getting an account for Cloud-V After getting access to the Cloud-V platform, use one of the following two methods to create a CI pipeline with Cloud-V. 1. Using Cloud-V automatic integration (beta) For ease of convinience for users and eliminating time delays of manual set up, users can add their GitHub and GitLab repository in Cloud-V by just adding their repository URL on the Cloud-V page. The source code for this is open-source here . Currently there are support for following version control systems: GitHub GitLab For GitHub For integrating user repository with Cloud-V, there is a GitHub app which users can install in their repository. The purpose of creating the app and publishing it for users is that, GitHub app has all the permissions already set up. So, when a user installs GitHub app, the app automatically sets up all the permissions for the user's repository. Following is the procedure for installing and integrating the repository with Cloud-V github app and for creating the CI pipeline in Cloud-V dashboard. Visit this link for installing GitHub app. Click on \"Install\" button which will take you to permissions page where you can select the permissions for the repository and also choose the repository which you would like to integrate with Cloud-V app Select \"Only select repositories\" if you would like to integrate a specific repository or number of repositories instead of integrating Cloud-V app with all the repositories. Click on \"Install & Authorize\" which will take you to the page where you can add repository URL Add repository URL and click on \"Submit\" The next page will show you: Access Token (will be visible one-time) URL of the GitHub repository which is configured (currently, one token can be configured with one repository) The link of the CI pipeline which is created automatically in Cloud-V CI dashboard Now go to the repository settings in the following manner and create a webhook for trigger with pull requests and push to branches Settings > Webhooks > Add webhook Fill the webhook settings in following manner Payload URL: https://dash.cloud-v.co/github-webhook/ Content Type: application/json Enable SSL Verification Which events would you like to trigger this webhook?: Just the push event Leave other fields as is Note: This creates a github multibranch pipeline automatically and it builds when a PR is created. If you need to check the source code or want to suggest any improvement for this, visit https://github.com/10x-Engineers/Cloud-V-git-automation and create an issue. For GitLab Following is the procedure for integrating repository automatically with GitLab: There is no app for gitlab integration with Cloud-V. So, for creating a pipeline in automated way, you will have to add gitlab access token and gitlab repository URL. Use the following steps to do so. Generate a GitLab personal access token in your repository settings Visit this link Add the personal access token and URL of the GitLab repository If the personal access token and the URL of the repostory is valid, you will get a link to the created pipeline 2. Manually adding the repository in Cloud-V This is the traditional method. The flow involves: Getting a personal access token from GitHub settings Adding the personal access token in the Cloud-V dashboard by logging in with the provided credentials Notifying the administrator about the credentials ID which user added, so they can add the credentials in the global settings Setting up webhook in the repository settings so that it can be triggered whenever a pull request is created NOTE: If you followed first method to integrate the repository with Cloud-V, you will not have to follow this method. Setting credentials for webhook Cloud-V supports webhooks which can trigger the job from external sources such as GitHub. They work in a way such that, if a specified branch is committed or if a pull request is created, the specified job build starts running depending upon the trigger event which is set in build's configuration in Cloud-V. This process requires access token of the repository CREATED BY OWNER OF REPOSITORY on which the webhook is to be set. These credentials can be safely added to Cloud-V without anyone (even administrator) seeing the passwords as follows. Obtaining github access token for repository Navigate to the dashboard of your github account and click on the your github profile picture on the top-right corner on dashboard. Then click on the \"Settings\" from the list. From the left option bar in Settings scroll down and click on \"Developer settings\". Once there, click on \"Personal access tokens\", then click on \"Fine-grained tokens\" from the dropdown list and after that click on \"Generate new token\". This will open the page for setting up new access token. Follow following steps for creating a token: Give your token a meaningful name under \"Token name\" Set expiration date in \"Expiration\" depending upon how long you would like your repository to be integrated with Cloud-V (think of a meaningful upper bound) The \"Resource owner\" should be the owner of the repository who can access all kinds of settings of the repository Under \"Repository access\", check \"Only select repositories\" and then select the repository for which you would like to create the token Under \"Permissions\" section, expand \"Repository Permissions\" and give the following two permissions: \"Read and write\" access to \"Commit statuses\" (Because after the CI has run, Cloud-V will be able to set the status of the commit accordingly) \"Read-only\" access to \"Webhooks\" Configuring repository webhook In GitHub, Go to repository settings which you want to integrate for Cloud-V. Go to Webhooks Click on Add webhook Add Payload URL as https://dash.cloud-v.co/ghprbhook/ Select content type as application/json Check Enable SSL verification In the section Which events would you like to trigger this webhook? check Let me select individual events and check Pul requests as individual events and dont check any other permission. Webhook settings will look something like this: Configurations inside Cloud-V Note: Currently users are not able to see or modify pipeline build configuration inside Jenkins, that is currently managed by administrator. Users are requested to inform administrator about how they want their pipeline configured. We will provide you with Cloud-V credentials on the provided email. Login with provided credentials. Click on the Credentials in the left menu. This will take you to the credentials page. Scroll down to the Stores scoped to Jenkins and click on the System as shown in the image. Click on Global credentials (unrestricted) . Click on Add Credentials . This will take you to the New Credentials page. Select Kind as Username with password . Select Scope as Global (Jenkins, nodes, items, all child items etc) . Enter your GitHub username in Username Enter Password as GitHub personal authentication token (PAT) which can be acquired from Github account settings. ID is optional but you can enter a unique ID . Description can be left empty. But it is recommended to give a suitable but careful description by which administrator will be able to identify and use these credentials to set up github webhook Select Create This process will look something like this Now credentials will be available in the credentials list and will be shown to you as well as administrator as shown in the image below. This will create an option in configurations for using these credentials in github webhook without changing or viewing them. Note the credentials ID (as shown in the image below) and email it to the same administrator email on which you received the login credentials for Cloud-V. It is important that administrator knows the credentials ID because they will use it in the job build configurations. Note: Please make sure to inform the administrator via email that you have added the credentials in Cloud-V Dashboard. Also, send administrator the ID of credentials via email. Requirements for administrator After the above setup is complete from software developer's side, developer will need to provide the administrator with following information. Dependencies for running the project which can be packages which are needed to install in the RISC-V CI environment by administrator. Events for triggering the job build. URL of GitHub repository. Path and name of cloud-v-pipeline file on the provided GitHub repository. Any additional information which should be given for successful execution of job builds.","title":"Setting up CI"},{"location":"Software_Developer_Guide/#software-developer-guide-for-risc-v-ci","text":"A software developer is the end-user who will develop or build his/her projects on RISC-V compute instance using RISC-V CI. This guide will cover all the things a software developer needs to integrate their project with RISC-V continuous integration. Note: All the compute instances have restrictions regarding which jobs will run on the compute instance. The administrator has to allow you for using specific instance. Be sure to contact administrator and tell them which instance you want to use There are currently two ways to integrate version control (Git) project with Cloud-V. Using Cloud-V automatic integration (beta) Manually integrating your project with Cloud-V","title":"Software Developer Guide for RISC-V CI"},{"location":"Software_Developer_Guide/#pre-requisites","text":"GitHub account. GitHub project repository with owner rights. Access to https://dash.cloud-v.co and https://cloud-v.co (visit this link to request the access)","title":"Pre-requisites"},{"location":"Software_Developer_Guide/#getting-an-account-for-cloud-v","text":"After getting access to the Cloud-V platform, use one of the following two methods to create a CI pipeline with Cloud-V.","title":"Getting an account for Cloud-V"},{"location":"Software_Developer_Guide/#1-using-cloud-v-automatic-integration-beta","text":"For ease of convinience for users and eliminating time delays of manual set up, users can add their GitHub and GitLab repository in Cloud-V by just adding their repository URL on the Cloud-V page. The source code for this is open-source here . Currently there are support for following version control systems: GitHub GitLab","title":"1. Using Cloud-V automatic integration (beta)"},{"location":"Software_Developer_Guide/#for-github","text":"For integrating user repository with Cloud-V, there is a GitHub app which users can install in their repository. The purpose of creating the app and publishing it for users is that, GitHub app has all the permissions already set up. So, when a user installs GitHub app, the app automatically sets up all the permissions for the user's repository. Following is the procedure for installing and integrating the repository with Cloud-V github app and for creating the CI pipeline in Cloud-V dashboard. Visit this link for installing GitHub app. Click on \"Install\" button which will take you to permissions page where you can select the permissions for the repository and also choose the repository which you would like to integrate with Cloud-V app Select \"Only select repositories\" if you would like to integrate a specific repository or number of repositories instead of integrating Cloud-V app with all the repositories. Click on \"Install & Authorize\" which will take you to the page where you can add repository URL Add repository URL and click on \"Submit\" The next page will show you: Access Token (will be visible one-time) URL of the GitHub repository which is configured (currently, one token can be configured with one repository) The link of the CI pipeline which is created automatically in Cloud-V CI dashboard Now go to the repository settings in the following manner and create a webhook for trigger with pull requests and push to branches Settings > Webhooks > Add webhook Fill the webhook settings in following manner Payload URL: https://dash.cloud-v.co/github-webhook/ Content Type: application/json Enable SSL Verification Which events would you like to trigger this webhook?: Just the push event Leave other fields as is Note: This creates a github multibranch pipeline automatically and it builds when a PR is created. If you need to check the source code or want to suggest any improvement for this, visit https://github.com/10x-Engineers/Cloud-V-git-automation and create an issue.","title":"For GitHub"},{"location":"Software_Developer_Guide/#for-gitlab","text":"Following is the procedure for integrating repository automatically with GitLab: There is no app for gitlab integration with Cloud-V. So, for creating a pipeline in automated way, you will have to add gitlab access token and gitlab repository URL. Use the following steps to do so. Generate a GitLab personal access token in your repository settings Visit this link Add the personal access token and URL of the GitLab repository If the personal access token and the URL of the repostory is valid, you will get a link to the created pipeline","title":"For GitLab"},{"location":"Software_Developer_Guide/#2-manually-adding-the-repository-in-cloud-v","text":"This is the traditional method. The flow involves: Getting a personal access token from GitHub settings Adding the personal access token in the Cloud-V dashboard by logging in with the provided credentials Notifying the administrator about the credentials ID which user added, so they can add the credentials in the global settings Setting up webhook in the repository settings so that it can be triggered whenever a pull request is created NOTE: If you followed first method to integrate the repository with Cloud-V, you will not have to follow this method.","title":"2. Manually adding the repository in Cloud-V"},{"location":"Software_Developer_Guide/#setting-credentials-for-webhook","text":"Cloud-V supports webhooks which can trigger the job from external sources such as GitHub. They work in a way such that, if a specified branch is committed or if a pull request is created, the specified job build starts running depending upon the trigger event which is set in build's configuration in Cloud-V. This process requires access token of the repository CREATED BY OWNER OF REPOSITORY on which the webhook is to be set. These credentials can be safely added to Cloud-V without anyone (even administrator) seeing the passwords as follows.","title":"Setting credentials for webhook"},{"location":"Software_Developer_Guide/#obtaining-github-access-token-for-repository","text":"Navigate to the dashboard of your github account and click on the your github profile picture on the top-right corner on dashboard. Then click on the \"Settings\" from the list. From the left option bar in Settings scroll down and click on \"Developer settings\". Once there, click on \"Personal access tokens\", then click on \"Fine-grained tokens\" from the dropdown list and after that click on \"Generate new token\". This will open the page for setting up new access token. Follow following steps for creating a token: Give your token a meaningful name under \"Token name\" Set expiration date in \"Expiration\" depending upon how long you would like your repository to be integrated with Cloud-V (think of a meaningful upper bound) The \"Resource owner\" should be the owner of the repository who can access all kinds of settings of the repository Under \"Repository access\", check \"Only select repositories\" and then select the repository for which you would like to create the token Under \"Permissions\" section, expand \"Repository Permissions\" and give the following two permissions: \"Read and write\" access to \"Commit statuses\" (Because after the CI has run, Cloud-V will be able to set the status of the commit accordingly) \"Read-only\" access to \"Webhooks\"","title":"Obtaining github access token for repository"},{"location":"Software_Developer_Guide/#configuring-repository-webhook","text":"In GitHub, Go to repository settings which you want to integrate for Cloud-V. Go to Webhooks Click on Add webhook Add Payload URL as https://dash.cloud-v.co/ghprbhook/ Select content type as application/json Check Enable SSL verification In the section Which events would you like to trigger this webhook? check Let me select individual events and check Pul requests as individual events and dont check any other permission. Webhook settings will look something like this:","title":"Configuring repository webhook"},{"location":"Software_Developer_Guide/#configurations-inside-cloud-v","text":"Note: Currently users are not able to see or modify pipeline build configuration inside Jenkins, that is currently managed by administrator. Users are requested to inform administrator about how they want their pipeline configured. We will provide you with Cloud-V credentials on the provided email. Login with provided credentials. Click on the Credentials in the left menu. This will take you to the credentials page. Scroll down to the Stores scoped to Jenkins and click on the System as shown in the image. Click on Global credentials (unrestricted) . Click on Add Credentials . This will take you to the New Credentials page. Select Kind as Username with password . Select Scope as Global (Jenkins, nodes, items, all child items etc) . Enter your GitHub username in Username Enter Password as GitHub personal authentication token (PAT) which can be acquired from Github account settings. ID is optional but you can enter a unique ID . Description can be left empty. But it is recommended to give a suitable but careful description by which administrator will be able to identify and use these credentials to set up github webhook Select Create This process will look something like this Now credentials will be available in the credentials list and will be shown to you as well as administrator as shown in the image below. This will create an option in configurations for using these credentials in github webhook without changing or viewing them. Note the credentials ID (as shown in the image below) and email it to the same administrator email on which you received the login credentials for Cloud-V. It is important that administrator knows the credentials ID because they will use it in the job build configurations. Note: Please make sure to inform the administrator via email that you have added the credentials in Cloud-V Dashboard. Also, send administrator the ID of credentials via email.","title":"Configurations inside Cloud-V"},{"location":"Software_Developer_Guide/#requirements-for-administrator","text":"After the above setup is complete from software developer's side, developer will need to provide the administrator with following information. Dependencies for running the project which can be packages which are needed to install in the RISC-V CI environment by administrator. Events for triggering the job build. URL of GitHub repository. Path and name of cloud-v-pipeline file on the provided GitHub repository. Any additional information which should be given for successful execution of job builds.","title":"Requirements for administrator"},{"location":"Tooling/","text":"Tools on Cloud-V Using Environment Modules Users can use environment modules to load different versions of same program. For using environment modules the pattern is as follows: module load <PACKAGENAME/VERSION> Important Note: Be sure to use #!/bin/bash -l instead of #!/bin/bash in CI pipeline file since that is required for environment modules to load For example if you want to load python version 3.9.2 compiled for x86, you will need to use following command: module load python/3.9.2 For packages compiled for RISC-V architecture host, you will need to append _riscv to package name. For example, for python 3.8.15 compiled for RISC-V, following command will be used. module load python_riscv/3.8.15","title":"Using Environment Modules"},{"location":"Tooling/#tools-on-cloud-v","text":"","title":"Tools on Cloud-V"},{"location":"Tooling/#using-environment-modules","text":"Users can use environment modules to load different versions of same program. For using environment modules the pattern is as follows: module load <PACKAGENAME/VERSION> Important Note: Be sure to use #!/bin/bash -l instead of #!/bin/bash in CI pipeline file since that is required for environment modules to load For example if you want to load python version 3.9.2 compiled for x86, you will need to use following command: module load python/3.9.2 For packages compiled for RISC-V architecture host, you will need to append _riscv to package name. For example, for python 3.8.15 compiled for RISC-V, following command will be used. module load python_riscv/3.8.15","title":"Using Environment Modules"},{"location":"jenkins_gitlab_integration/","text":"Integrating GitLab with Jenkins This documentation will cover how to create an integration between jenkins and GitLab. This will allow users to trigger jenkins job when a merge request or a push is detected in GitLab. Pre-requisites GitLab plugin Git plugin GitLab repository with owner's credentials Configuring Jenkins System First jenkins needs to be configured. Go to Dashboard > Manage Jenkins > Configure System Scroll down to Gitlab section Check Enable authentication for '/project' end-point Enter a Connection name . Enter Gitlab host URL as https://gitlab.com/ . In case there is a different domain name, then enter there instead of above url. In Credentials , click on Add then click on Jenkins . In Kind , select GitLab API token . In API token , enter the gitlab personal access token (this will be obtained below while configuring GitLab). Click on Advanced . Click on Test Connection . If everything goes right, it should print success . Configuring GitLab Click on profile avatar in the top right. Click on Edit profile . Click on Access Tokens . Create a new personal access token and copy it (this is the GitLab API token used in above section Configuring Jenkins System ). Go to repository settings. On left-side pane, select Webhooks . Enter GitLab webhook URL (this is explained below in next section). Enter Secret Token (this is explained in the below section). Check desirable trigger options. Click Add webhook . Configuring Jenkins Job Create a jenkins job. On job configuration page, scroll down to Source Code Management . Select Git . In Credentials , add the owner credentials of GitLab repository. This will be Username and Password . Select the appropriate branch (generally it is main ). Scroll down to Build Triggers . Check Build when a change is pushed to GitLab . There will also be a GitLab webhook URL . This is needed in GitLab. This URL will be called as Webhook URL . Click on Advanced in the same option, then scroll down and generate a secret token. This token is also needed. This token will be called as Secret Token . Scroll down to Post-build Actions and select Publish build status to GitLab . Click on Apply and Save . Now whenever there will be a push (or whatever build trigger option is set in jenkins and gitlab webhook), specified jenkins job will be triggered.","title":"Integrating GitLab with Jenkins"},{"location":"jenkins_gitlab_integration/#integrating-gitlab-with-jenkins","text":"This documentation will cover how to create an integration between jenkins and GitLab. This will allow users to trigger jenkins job when a merge request or a push is detected in GitLab.","title":"Integrating GitLab with Jenkins"},{"location":"jenkins_gitlab_integration/#pre-requisites","text":"GitLab plugin Git plugin GitLab repository with owner's credentials","title":"Pre-requisites"},{"location":"jenkins_gitlab_integration/#configuring-jenkins-system","text":"First jenkins needs to be configured. Go to Dashboard > Manage Jenkins > Configure System Scroll down to Gitlab section Check Enable authentication for '/project' end-point Enter a Connection name . Enter Gitlab host URL as https://gitlab.com/ . In case there is a different domain name, then enter there instead of above url. In Credentials , click on Add then click on Jenkins . In Kind , select GitLab API token . In API token , enter the gitlab personal access token (this will be obtained below while configuring GitLab). Click on Advanced . Click on Test Connection . If everything goes right, it should print success .","title":"Configuring Jenkins System"},{"location":"jenkins_gitlab_integration/#configuring-gitlab","text":"Click on profile avatar in the top right. Click on Edit profile . Click on Access Tokens . Create a new personal access token and copy it (this is the GitLab API token used in above section Configuring Jenkins System ). Go to repository settings. On left-side pane, select Webhooks . Enter GitLab webhook URL (this is explained below in next section). Enter Secret Token (this is explained in the below section). Check desirable trigger options. Click Add webhook .","title":"Configuring GitLab"},{"location":"jenkins_gitlab_integration/#configuring-jenkins-job","text":"Create a jenkins job. On job configuration page, scroll down to Source Code Management . Select Git . In Credentials , add the owner credentials of GitLab repository. This will be Username and Password . Select the appropriate branch (generally it is main ). Scroll down to Build Triggers . Check Build when a change is pushed to GitLab . There will also be a GitLab webhook URL . This is needed in GitLab. This URL will be called as Webhook URL . Click on Advanced in the same option, then scroll down and generate a secret token. This token is also needed. This token will be called as Secret Token . Scroll down to Post-build Actions and select Publish build status to GitLab . Click on Apply and Save . Now whenever there will be a push (or whatever build trigger option is set in jenkins and gitlab webhook), specified jenkins job will be triggered.","title":"Configuring Jenkins Job"},{"location":"runner_specs/","text":"Specifications of compute instances in Cloud-V This document contains the specifications of the compute instances available for users to run builds in Cloud-V. The term \"Compute Instance\", can also be safely interchanged with the terms \"Build Executor\" and \"Runner\". Name CI Name String Architecture ISA String Cores Memory Compute Instance Type N/A J-x86-1 (or) J-QMU-1 x86_64 N/A 4 8GiB Hardware with application-level emulator intel i7-6500U J-TESTVM-1 x86_64 N/A 4 8GiB Hardware Raspberry Pi 4 Model B J-RASP4-1 aarch64 ARMv8-A 4 4GiB Hardware QEMU System Linux J-QMS-1 riscv64 See Ext 1 at bottom 2 2GiB QEMU System emulator VisionFive 1 J-VF1-1 riscv64 rv64imafdc 2 8GiB Hardware VisionFive 1 J-VF1-2 riscv64 rv64imafdc 2 8GiB Hardware VisionFive 1 J-VF1-3 riscv64 rv64imafdc 2 8GiB Hardware VisionFive 2 J-VF2-1 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-2 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-3 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-4 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-5 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-6 riscv64 rv64imafdc 4 8GiB Hardware HiFive Unleashed J-HF-1 riscv64 rv64imafdc 4 8GiB Hardware Banana Pi F3 J-BPF3-1 riscv64 rv64imafdcv _sscofpmf_sstc _svpbmt_zicbom _zicboz_zicbop _zihintpause 8 4GiB Hardware Banana Pi F3 J-BPF3-2 riscv64 rv64imafdcv _sscofpmf_sstc _svpbmt_zicbom _zicboz_zicbop _zihintpause 8 16GiB Hardware Banana Pi F3 J-BPF3-3 riscv64 rv64imafdcv _sscofpmf_sstc _svpbmt_zicbom _zicboz_zicbop _zihintpause 8 16GiB Hardware Banana Pi F3 J-BPF3-4 riscv64 rv64imafdcv _sscofpmf_sstc _svpbmt_zicbom _zicboz_zicbop _zihintpause 8 16GiB Hardware Banana Pi F3 J-BPF3-5 riscv64 rv64imafdcv _sscofpmf_sstc _svpbmt_zicbom _zicboz_zicbop _zihintpause 8 16GiB Hardware Milk-V Jupiter J-JUPITER-1 riscv64 rv64imafdcv _sscofpmf _sstcc _svpbmt _zicbom _zicboz _zicbop _zihintpause 8 16GiB Hardware Milk-V Pioneer Box J-pioneer-1 riscv64 rv64imafdc _zicntr _zicsr _zifencei _zihpm _xtheadvector 64 8GiB Hardware Ext1: rv64imafdcvh_zicbom_zicboz_zicntr_zicond_zicsr_zifencei_zihintntl_zihintpause_zihpm_zacas_zfa_zfh zfhmin_zba_zbb_zbc_zbkb_zbkc_zbkx_zbs_zknd_zkne_zknh_zkr_zkt_zksed zksh_ztso_zvbb_zvbc_zvfh_zvfhmin_zvkb_zvkg_ zvkned_zvknha_zvknhb_zvksed_smaia_smstateen_ssaia_sscofpmf_sstc_svinval_svnapot_svpbmt Note: The J-QMU-1 and J-x86-1 are one and the same runner. The purpose of creating two separate executors for same hardware is that J-x86-1 is supposed to be specifically for x86 architecture whereas J-QMU-1 is specifically for the users who want to cross compile source code for riscv64 architecture and then use qemu-usermode to execute them. Nevertheless, the tooling available for J-x86-1 can also be used for J-QMU-1","title":"Compute Instance Specifications"},{"location":"runner_specs/#specifications-of-compute-instances-in-cloud-v","text":"This document contains the specifications of the compute instances available for users to run builds in Cloud-V. The term \"Compute Instance\", can also be safely interchanged with the terms \"Build Executor\" and \"Runner\". Name CI Name String Architecture ISA String Cores Memory Compute Instance Type N/A J-x86-1 (or) J-QMU-1 x86_64 N/A 4 8GiB Hardware with application-level emulator intel i7-6500U J-TESTVM-1 x86_64 N/A 4 8GiB Hardware Raspberry Pi 4 Model B J-RASP4-1 aarch64 ARMv8-A 4 4GiB Hardware QEMU System Linux J-QMS-1 riscv64 See Ext 1 at bottom 2 2GiB QEMU System emulator VisionFive 1 J-VF1-1 riscv64 rv64imafdc 2 8GiB Hardware VisionFive 1 J-VF1-2 riscv64 rv64imafdc 2 8GiB Hardware VisionFive 1 J-VF1-3 riscv64 rv64imafdc 2 8GiB Hardware VisionFive 2 J-VF2-1 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-2 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-3 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-4 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-5 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-6 riscv64 rv64imafdc 4 8GiB Hardware HiFive Unleashed J-HF-1 riscv64 rv64imafdc 4 8GiB Hardware Banana Pi F3 J-BPF3-1 riscv64 rv64imafdcv _sscofpmf_sstc _svpbmt_zicbom _zicboz_zicbop _zihintpause 8 4GiB Hardware Banana Pi F3 J-BPF3-2 riscv64 rv64imafdcv _sscofpmf_sstc _svpbmt_zicbom _zicboz_zicbop _zihintpause 8 16GiB Hardware Banana Pi F3 J-BPF3-3 riscv64 rv64imafdcv _sscofpmf_sstc _svpbmt_zicbom _zicboz_zicbop _zihintpause 8 16GiB Hardware Banana Pi F3 J-BPF3-4 riscv64 rv64imafdcv _sscofpmf_sstc _svpbmt_zicbom _zicboz_zicbop _zihintpause 8 16GiB Hardware Banana Pi F3 J-BPF3-5 riscv64 rv64imafdcv _sscofpmf_sstc _svpbmt_zicbom _zicboz_zicbop _zihintpause 8 16GiB Hardware Milk-V Jupiter J-JUPITER-1 riscv64 rv64imafdcv _sscofpmf _sstcc _svpbmt _zicbom _zicboz _zicbop _zihintpause 8 16GiB Hardware Milk-V Pioneer Box J-pioneer-1 riscv64 rv64imafdc _zicntr _zicsr _zifencei _zihpm _xtheadvector 64 8GiB Hardware Ext1: rv64imafdcvh_zicbom_zicboz_zicntr_zicond_zicsr_zifencei_zihintntl_zihintpause_zihpm_zacas_zfa_zfh zfhmin_zba_zbb_zbc_zbkb_zbkc_zbkx_zbs_zknd_zkne_zknh_zkr_zkt_zksed zksh_ztso_zvbb_zvbc_zvfh_zvfhmin_zvkb_zvkg_ zvkned_zvknha_zvknhb_zvksed_smaia_smstateen_ssaia_sscofpmf_sstc_svinval_svnapot_svpbmt Note: The J-QMU-1 and J-x86-1 are one and the same runner. The purpose of creating two separate executors for same hardware is that J-x86-1 is supposed to be specifically for x86 architecture whereas J-QMU-1 is specifically for the users who want to cross compile source code for riscv64 architecture and then use qemu-usermode to execute them. Nevertheless, the tooling available for J-x86-1 can also be used for J-QMU-1","title":"Specifications of compute instances in Cloud-V"},{"location":"sandboxing/","text":"Using SSH access on RISC-V instance After getting SSH command and credentials from Cloud-V administrator, users can log in to RISC-V instance and start development like any Linux machine without GUI. Setting up VScodium with RISC-V It may not be convinient for users to use terminal editors (like Vim and Nano) if they are not accustomed. At present VScode remote extension does not have support for RISC-V architecture but VScodium supports it. This section explains how you can set up VScodium on x86 machine to get remote development access to RISC-V compute instance (assuming both the machines have SSH installed). Install VScodium from this link Install Open Remote - SSH Extension from jeanp413 Use Ctrl+Shift+p to open command pallet and search remote Remote-SSH: Connect to host... Enter username, publicly accessible IP and port of the compute instance which you want to connect to.","title":"Setting up SSH/Sandboxing"},{"location":"sandboxing/#using-ssh-access-on-risc-v-instance","text":"After getting SSH command and credentials from Cloud-V administrator, users can log in to RISC-V instance and start development like any Linux machine without GUI.","title":"Using SSH access on RISC-V instance"},{"location":"sandboxing/#setting-up-vscodium-with-risc-v","text":"It may not be convinient for users to use terminal editors (like Vim and Nano) if they are not accustomed. At present VScode remote extension does not have support for RISC-V architecture but VScodium supports it. This section explains how you can set up VScodium on x86 machine to get remote development access to RISC-V compute instance (assuming both the machines have SSH installed). Install VScodium from this link Install Open Remote - SSH Extension from jeanp413 Use Ctrl+Shift+p to open command pallet and search remote Remote-SSH: Connect to host... Enter username, publicly accessible IP and port of the compute instance which you want to connect to.","title":"Setting up VScodium with RISC-V"},{"location":"tooling_J-BPF3-x/","text":"Tools on J-BPF3-x node This compute instance is Banana Pi F3 board and it has available packages ONLY for RISC-V architecture. Operating System: Bianbu 1.0.x Tool Version Installed from Git 2.43.0 apt OpenJDK 21.0.2 apt GCC 13.2.0 apt Python3 3.11.7 apt OpenSSL 3.1.5 apt Ruby 3.1.2p20 apt Go 1.22.0 apt rustc 1.70.0 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.28.3 apt make 4.3 apt automake 1.16.5 apt gfortran 13.2.0 apt openssh-server 9.6p1 apt vim 9.1 apt","title":"J-BPF3-1"},{"location":"tooling_J-BPF3-x/#tools-on-j-bpf3-x-node","text":"This compute instance is Banana Pi F3 board and it has available packages ONLY for RISC-V architecture. Operating System: Bianbu 1.0.x Tool Version Installed from Git 2.43.0 apt OpenJDK 21.0.2 apt GCC 13.2.0 apt Python3 3.11.7 apt OpenSSL 3.1.5 apt Ruby 3.1.2p20 apt Go 1.22.0 apt rustc 1.70.0 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.28.3 apt make 4.3 apt automake 1.16.5 apt gfortran 13.2.0 apt openssh-server 9.6p1 apt vim 9.1 apt","title":"Tools on J-BPF3-x node"},{"location":"tooling_J-HF-1/","text":"Tools on J-HF-1 node This compute instance is Sifive's HiFive Unleased board and it has available packages ONLY for RISC-V architecture. Operating System: Ubuntu 20.04.6 (Focal Fossa) Tool Version Installed from Git 2.25.1 apt OpenJDK 11.0.20.1 apt GCC 10.5.0 apt Python3 3.8.10 apt OpenSSL 1.1.1f apt Ruby 2.7.0p0 apt Go 1.14.3 apt rustc 1.41.0 apt Flex 2.6.4 apt Ninja 1.10.0 apt Bison 3.5.1 apt autoconf 2.69 apt gperf 3.1 apt cmake 3.16.3 apt make 4.2.1 apt automake 1.16.1 apt gfortran 9.4.0 apt openssh-server 8.2p1 apt","title":"J-HF-1"},{"location":"tooling_J-HF-1/#tools-on-j-hf-1-node","text":"This compute instance is Sifive's HiFive Unleased board and it has available packages ONLY for RISC-V architecture. Operating System: Ubuntu 20.04.6 (Focal Fossa) Tool Version Installed from Git 2.25.1 apt OpenJDK 11.0.20.1 apt GCC 10.5.0 apt Python3 3.8.10 apt OpenSSL 1.1.1f apt Ruby 2.7.0p0 apt Go 1.14.3 apt rustc 1.41.0 apt Flex 2.6.4 apt Ninja 1.10.0 apt Bison 3.5.1 apt autoconf 2.69 apt gperf 3.1 apt cmake 3.16.3 apt make 4.2.1 apt automake 1.16.1 apt gfortran 9.4.0 apt openssh-server 8.2p1 apt","title":"Tools on J-HF-1 node"},{"location":"tooling_J-K230-1/","text":"Tools on J-K230-1 node This compute instance is Starfive's VisionFive 2 board and it has available packages ONLY for RISC-V architecture. Operating System: Debian 12 (Bookworm) Tool Version Installed from Git 2.40.1 apt OpenJDK 21.0.2 apt GCC 13.2.0 apt Python3 3.11.6 apt OpenSSL 3.0.10 apt Ruby 3.1.2p20 apt Go 1.21.1 apt rustc 1.75.0 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.27.4 apt make 4.3 apt automake 1.16.5 apt gfortran 13.2.0 apt openssh-server 9.3p1 apt vim 9.0 apt vim 3.3a apt","title":"J-K230-1"},{"location":"tooling_J-K230-1/#tools-on-j-k230-1-node","text":"This compute instance is Starfive's VisionFive 2 board and it has available packages ONLY for RISC-V architecture. Operating System: Debian 12 (Bookworm) Tool Version Installed from Git 2.40.1 apt OpenJDK 21.0.2 apt GCC 13.2.0 apt Python3 3.11.6 apt OpenSSL 3.0.10 apt Ruby 3.1.2p20 apt Go 1.21.1 apt rustc 1.75.0 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.27.4 apt make 4.3 apt automake 1.16.5 apt gfortran 13.2.0 apt openssh-server 9.3p1 apt vim 9.0 apt vim 3.3a apt","title":"Tools on J-K230-1 node"},{"location":"tooling_J-QMS-1/","text":"Tools on J-QMS-1 node This is QEMU system compute instance with 64-bit RISC-V Linux and it has all the packages ONLY for RISC-V architecture Operating System: Ubuntu 22.04.4 LTS (Jammy Jellyfish) QEMU Linux Version: 7.2.90 (v9.0.0-rc0-68-g853546f812) Tool Version Installed from pip 22.0.2 apt Git 2.34.1 apt OpenJDK 19.0.1 apt GCC 11.4.0 apt Python3 3.10.12 apt OpenSSL 3.0.2 apt Ruby 3.0.2p107 apt Go 1.18.8 apt rustc 1.75.0 apt Flex 2.6.4 apt Ninja 1.10.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt make 4.3 apt cmake 3.22.1 apt","title":"J-QMS-1"},{"location":"tooling_J-QMS-1/#tools-on-j-qms-1-node","text":"This is QEMU system compute instance with 64-bit RISC-V Linux and it has all the packages ONLY for RISC-V architecture Operating System: Ubuntu 22.04.4 LTS (Jammy Jellyfish) QEMU Linux Version: 7.2.90 (v9.0.0-rc0-68-g853546f812) Tool Version Installed from pip 22.0.2 apt Git 2.34.1 apt OpenJDK 19.0.1 apt GCC 11.4.0 apt Python3 3.10.12 apt OpenSSL 3.0.2 apt Ruby 3.0.2p107 apt Go 1.18.8 apt rustc 1.75.0 apt Flex 2.6.4 apt Ninja 1.10.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt make 4.3 apt cmake 3.22.1 apt","title":"Tools on J-QMS-1 node"},{"location":"tooling_J-QMU-1J-VM-1/","text":"Tools on J-x86-1 or J-QMU-1 node Tools which are mentioned for x86 architecture are able to run on J-x86-1 . Tools which are mentioned for RISC-V architecture are able to run on J-QMU-1 . The packages which are supported for QEMU User mode can be used by normal commands once they are loaded. Here PACKAGE_NAME is the package which you want to run on QEMU user mode. Operating System: Debian 11 (bullseye) QEMU User Mode Version: Different Versions (see the table below) Tooling available for J-x86-1 The tools available for J-x86-1 is for use on x86 architecture and these tools do not support execution on RISC-V architecture Tool Versions Installed from Host Architecture Environment Modules Support Git 2.3.0.2 source x86 N/A OpenJDK 19.0.1 apt x86 N/A GCC 10.4.0, 12.2.0 apt x86 Yes Python3 3.8.15, 3.9.2 source x86 Yes Go 1.18.8 apt x86 N/A rustc 1.65.0 source x86 N/A Flex 2.6.4 apt x86 N/A Ninja 1.10.1-1 apt x86 N/A Bison 3.7.5 apt x86 N/A autoconf 2.69 apt x86 N/A gperf 2.2.4 apt x86 N/A spike 1.1.1-dev source x86 Yes Verilator 4.038 apt x86 N/A Sail (riscv_sim_RV64, riscv_sim_RV32) 0.5 source x86 Yes cmake 3.18.4 apt x86 N/A make 4.3 apt x86 N/A ARM EABI GNU toolchain 13.2.Rel1 source x86 Yes qemu-system-arm 8.2.2 source x86 Yes SCons 4.7.0 source x86 Yes Tooling available for J-QMU-1 The tools available for J-QMU-1 is for use on RISC-V architecture and these tools do not support execution on x86 architecture Tool Versions Installed from Host Architecture Environment Modules Support Python3 3.8.15 source RISC-V Yes zlib 1.2.13 source RISC-V N/A OpenSSL 1.1.1r source RISC-V Yes Ruby (without IRB) 3.2.0dev source RISC-V Yes rustc 1.65.0 source RISC-V N/A Flex 2.6.4 source RISC-V yes Ninja 1.12.0.git source RISC-V Yes Bison 3.8.2, 2.3 source RISC-V Yes clang 16.0.0 source RISC-V (cross compiler) Yes riscv-pk 1.0.0-91-g573c858 source RISC-V Yes QEMU User mode and RISC-V GNU Cross compilers From now on RISC-V cross-compilers can only be loaded with their respective QEMU User mode on Cloud-V. This is configured so that there is no confusion between toolchain version and qemu user mode being used because both of these will be \"generally\" taken from the latest releases of nightly builds. Loading a certain RISC-V toolchain using environment modules will automatically load the respective qemu usermode version unless otherwise specified. The loading pattern for RISC-V 64-bit GNU Glibc toolchain will be as follows: module load riscv64-gnu-glibc/<release-date> And the loading pattern for RISC-V 64-bit GNU Glibc toolchain will be as follows: module load riscv64-gnu-elf/<release-date> Following table provides relevant information about version of the toolchain and the respective QEMU User mode version (where the release date is mentioned in pattern MMDDYYYY ). Release date GNU Toolchain version (elf and glibc) QEMU Version 03012024 13.2.0 8.2.1 02022024 13.2.0 8.2.1 02022024 13.2.0 8.1.1 Note: The J-QMU-1 and J-x86-1 are one and the same runner. The purpose of creating two separate executors for same hardware is that J-x86-1 is supposed to be specifically for x86 architecture whereas J-QMU-1 is specifically for the users who want to cross compile source code for riscv64 architecture and then use qemu-usermode to execute them. Nevertheless, the tooling available for J-x86-1 can also be used for J-QMU-1","title":"J-x86-1 or J-QMU-1"},{"location":"tooling_J-QMU-1J-VM-1/#tools-on-j-x86-1-or-j-qmu-1-node","text":"Tools which are mentioned for x86 architecture are able to run on J-x86-1 . Tools which are mentioned for RISC-V architecture are able to run on J-QMU-1 . The packages which are supported for QEMU User mode can be used by normal commands once they are loaded. Here PACKAGE_NAME is the package which you want to run on QEMU user mode. Operating System: Debian 11 (bullseye) QEMU User Mode Version: Different Versions (see the table below)","title":"Tools on J-x86-1 or J-QMU-1 node"},{"location":"tooling_J-QMU-1J-VM-1/#tooling-available-for-j-x86-1","text":"The tools available for J-x86-1 is for use on x86 architecture and these tools do not support execution on RISC-V architecture Tool Versions Installed from Host Architecture Environment Modules Support Git 2.3.0.2 source x86 N/A OpenJDK 19.0.1 apt x86 N/A GCC 10.4.0, 12.2.0 apt x86 Yes Python3 3.8.15, 3.9.2 source x86 Yes Go 1.18.8 apt x86 N/A rustc 1.65.0 source x86 N/A Flex 2.6.4 apt x86 N/A Ninja 1.10.1-1 apt x86 N/A Bison 3.7.5 apt x86 N/A autoconf 2.69 apt x86 N/A gperf 2.2.4 apt x86 N/A spike 1.1.1-dev source x86 Yes Verilator 4.038 apt x86 N/A Sail (riscv_sim_RV64, riscv_sim_RV32) 0.5 source x86 Yes cmake 3.18.4 apt x86 N/A make 4.3 apt x86 N/A ARM EABI GNU toolchain 13.2.Rel1 source x86 Yes qemu-system-arm 8.2.2 source x86 Yes SCons 4.7.0 source x86 Yes","title":"Tooling available for J-x86-1"},{"location":"tooling_J-QMU-1J-VM-1/#tooling-available-for-j-qmu-1","text":"The tools available for J-QMU-1 is for use on RISC-V architecture and these tools do not support execution on x86 architecture Tool Versions Installed from Host Architecture Environment Modules Support Python3 3.8.15 source RISC-V Yes zlib 1.2.13 source RISC-V N/A OpenSSL 1.1.1r source RISC-V Yes Ruby (without IRB) 3.2.0dev source RISC-V Yes rustc 1.65.0 source RISC-V N/A Flex 2.6.4 source RISC-V yes Ninja 1.12.0.git source RISC-V Yes Bison 3.8.2, 2.3 source RISC-V Yes clang 16.0.0 source RISC-V (cross compiler) Yes riscv-pk 1.0.0-91-g573c858 source RISC-V Yes","title":"Tooling available for J-QMU-1"},{"location":"tooling_J-QMU-1J-VM-1/#qemu-user-mode-and-risc-v-gnu-cross-compilers","text":"From now on RISC-V cross-compilers can only be loaded with their respective QEMU User mode on Cloud-V. This is configured so that there is no confusion between toolchain version and qemu user mode being used because both of these will be \"generally\" taken from the latest releases of nightly builds. Loading a certain RISC-V toolchain using environment modules will automatically load the respective qemu usermode version unless otherwise specified. The loading pattern for RISC-V 64-bit GNU Glibc toolchain will be as follows: module load riscv64-gnu-glibc/<release-date> And the loading pattern for RISC-V 64-bit GNU Glibc toolchain will be as follows: module load riscv64-gnu-elf/<release-date> Following table provides relevant information about version of the toolchain and the respective QEMU User mode version (where the release date is mentioned in pattern MMDDYYYY ). Release date GNU Toolchain version (elf and glibc) QEMU Version 03012024 13.2.0 8.2.1 02022024 13.2.0 8.2.1 02022024 13.2.0 8.1.1 Note: The J-QMU-1 and J-x86-1 are one and the same runner. The purpose of creating two separate executors for same hardware is that J-x86-1 is supposed to be specifically for x86 architecture whereas J-QMU-1 is specifically for the users who want to cross compile source code for riscv64 architecture and then use qemu-usermode to execute them. Nevertheless, the tooling available for J-x86-1 can also be used for J-QMU-1","title":"QEMU User mode and RISC-V GNU Cross compilers"},{"location":"tooling_J-VF1-x/","text":"Tools on J-VF1-x node This compute instance is Starfive's VisionFive 1 board and it has available packages ONLY for RISC-V architecture. Operating System: Ubuntu 23.04 (Lunar Lobster) Tool Version Installed from Git 2.39.2 apt OpenJDK 21-ea apt GCC 13.1.0 apt Python3 3.11.4 apt OpenSSL 3.0.8 apt Ruby 3.1.2p20 apt Go 1.20.3 apt rustc 1.67.1 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.25.1 apt make 4.3 apt automake 1.16.5 apt gfortran 13.1.0 apt openssh-server 9.0p1 apt","title":"J-VF1-x"},{"location":"tooling_J-VF1-x/#tools-on-j-vf1-x-node","text":"This compute instance is Starfive's VisionFive 1 board and it has available packages ONLY for RISC-V architecture. Operating System: Ubuntu 23.04 (Lunar Lobster) Tool Version Installed from Git 2.39.2 apt OpenJDK 21-ea apt GCC 13.1.0 apt Python3 3.11.4 apt OpenSSL 3.0.8 apt Ruby 3.1.2p20 apt Go 1.20.3 apt rustc 1.67.1 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.25.1 apt make 4.3 apt automake 1.16.5 apt gfortran 13.1.0 apt openssh-server 9.0p1 apt","title":"Tools on J-VF1-x node"},{"location":"tooling_J-VF2-x/","text":"Tools on J-VF2-x node This compute instance is Starfive's VisionFive 2 board and it has available packages ONLY for RISC-V architecture. Operating System: Debian 12 (Bookworm) Tool Version Installed from Git 2.43.0 apt OpenJDK 21.0.2 apt GCC 13.2.0 apt Python3 3.11.7 apt OpenSSL 3.1.5 apt Ruby 3.1.2p20 apt Go 1.22.0 apt rustc 1.70.0 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.28.3 apt make 4.3 apt automake 1.16.5 apt gfortran 13.2.0 apt openssh-server 9.6p1 apt vim 9.1 apt","title":"J-VF2-x"},{"location":"tooling_J-VF2-x/#tools-on-j-vf2-x-node","text":"This compute instance is Starfive's VisionFive 2 board and it has available packages ONLY for RISC-V architecture. Operating System: Debian 12 (Bookworm) Tool Version Installed from Git 2.43.0 apt OpenJDK 21.0.2 apt GCC 13.2.0 apt Python3 3.11.7 apt OpenSSL 3.1.5 apt Ruby 3.1.2p20 apt Go 1.22.0 apt rustc 1.70.0 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.28.3 apt make 4.3 apt automake 1.16.5 apt gfortran 13.2.0 apt openssh-server 9.6p1 apt vim 9.1 apt","title":"Tools on J-VF2-x node"}]}
=======
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cloud-V Documentation","text":"<p>This documentation lists the tooling and necessary information to use Cloud-V for RISC-V application development on RISC-V compute instances provided by Cloud-V</p> <p>Overview Slides: link</p> <p>Youtube Channel: youtube.com/@Cloud-V</p>"},{"location":"Creating_CI_CD_pipeline/","title":"Creating a CI/CD pipeline in Jenkins","text":""},{"location":"Creating_CI_CD_pipeline/#pre-requisistes","title":"Pre Requisistes","text":"<p>For sake of this documentation, jenkins built-in sample script is used to create and execute a cd/cd pipeline in jenkins. In the built-in script maven is used as M3, so one must install <code>Maven</code> plugin inside jenkins and name it <code>M3</code>. Usually maven is already present inside jenkins and can be configured from <code>Global Configuration Tools</code>. Following steps demonstrate configuring Maven plugin.  </p> <ul> <li>Go to Jenkins <code>Dashboard</code> and click on <code>Manage Jenkins</code></li> <li>In <code>Manage Jenkins</code>, under <code>System Configuration</code> section, click on <code>Global Tool Configuration</code>.  </li> </ul> <p></p> <ul> <li>In <code>Global Tool Configuration</code>, scroll down to Maven section and click on the respective option under the <code>Maven</code> Section (should be <code>Maven installations\u2026</code> or <code>Add Maven</code>).  </li> </ul> <p></p> <ul> <li> <p>Under <code>Maven installations</code>, enter <code>M3</code> in \u201cname\u201d text box, check <code>Install Automatically</code> and select <code>Version</code> greater than 3, then click <code>Apply</code> and <code>Save</code>.  </p> </li> <li> <p>This should install Maven version 3 and configure as <code>M3</code>.</p> </li> </ul>"},{"location":"Creating_CI_CD_pipeline/#steps-for-jenkins-pipeline-creation","title":"Steps for Jenkins pipeline creation","text":"<ul> <li> <p>After installing Jenkins and having all the suggested plugins installed, go to Jenkins dashboard and click on <code>Create Job</code>.  </p> </li> <li> <p>On the next page, give your pipeline a name, select <code>Pipeline</code> and click <code>OK</code>.</p> </li> </ul> <p></p> <ul> <li> <p>A <code>Configuration</code> page for the pipeline will appear.  </p> </li> <li> <p>Select <code>Build Triggers</code> options and <code>General</code> options according to need and scroll down to the Pipeline section.</p> </li> </ul> <p></p> <ul> <li> <p>Definition section contains configuration for stages and steps of the pipeline. Under <code>Definition</code> section, you can either choose <code>Pipeline script</code> and try writing your own script or try some sample pipeline (like <code>Hello World</code>, <code>Github+Maven</code> etc) or you could select <code>Pipeline script from SCM</code> and give a github repository containing configurations of Pipeline.  </p> </li> <li> <p>Press <code>Save</code> and <code>Apply</code>.  </p> </li> <li> <p>This should take you to the Pipeline and you can build the pipeline and if no unresolved dependencies are present, the pipeline should build without any error.</p> </li> </ul>"},{"location":"Creating_jenkinsfile/","title":"Creating a Cloud-V CI Pipeline file","text":""},{"location":"Creating_jenkinsfile/#what-is-a-cloud-v-ci-pipeline-file","title":"What is a Cloud-V CI Pipeline file","text":"<p>A <code>cloud-v-pipeline</code> file is a Continuous Integration (CI) jenkins pipeline script which is written in Groovy. It describes various stages (and possibly steps) which are executed in the defined pattern. These stages can be written in bash or they can be written in Groovy itself. Mainly there can be two types of Cloud-V pipeline files are of two types:  </p> <ol> <li>Scripted: Only contains stages</li> <li>Declarative: Contains stages as well as steps (more feature-rich and recommended)</li> </ol> <p>This documentation will cover how to create a <code>cloud-v-pipeline</code> file with bash script inside it and run it on various compute instances (which are known as <code>Nodes</code> in jenkins).</p>"},{"location":"Creating_jenkinsfile/#jenkins-node","title":"Jenkins Node","text":"<p>In jenkins, node represents a compute instance. In simple words, it is the platform on which our job build is going to run.  </p>"},{"location":"Creating_jenkinsfile/#jenkins-master","title":"Jenkins Master","text":"<p>There is a <code>jenkins Master</code> node which is actually the compute instance on which Jenkins is installed. It is the node which schedules builds on runners. For security reason, no job is allowed to run on this node.</p>"},{"location":"Creating_jenkinsfile/#jenkins-slave","title":"Jenkins Slave","text":"<p>Jenkins slave nodes are the compute instances on which our job builds run safely. They may be attached with jenkins master via hardware or they may be connected through remote SSH connection.</p>"},{"location":"Creating_jenkinsfile/#cloud-v-pipeline-file-written-with-bash","title":"<code>cloud-v-pipeline</code> file written with bash","text":""},{"location":"Creating_jenkinsfile/#simple-hello-world-cloud-v-pipeline-file","title":"Simple Hello World <code>cloud-v-pipeline</code> file","text":"<p>In Cloud-V, all the platforms are running Linux operating system, so the <code>cloud-v-pipeline</code> should be written in bash. Following script is an example of how can we run a bash script in scripted <code>cloud-v-pipeline</code>.  </p> <pre><code>\nnode{\n    stage('*** Phase 1 ***') {\n        //Using bash commands\n        sh '''#!/bin/bash\n            echo \"Hello World !\\n\"\n         '''\n    }\n}\n</code></pre> <p>The keyword <code>sh</code> is used to specify a shell script</p> <p>As there is nothing mentioned with <code>node</code>, so the script will run job build on any available compute instance.  </p>"},{"location":"Creating_jenkinsfile/#cloud-v-pipeline-for-a-specific-node","title":"<code>cloud-v-pipeline</code> for a Specific Node","text":"<p>In previous script, the <code>cloud-v-pipeline</code> would run on any compute instances which are available. But in case if someone wants to run a job build on specific node, then a compute instance name must be specified with keyword <code>node</code>. The following script is an example of running above <code>Hello World</code> program on node named <code>hifive_unleashed</code>.  </p> <pre><code>\nnode('hifive_unleashed'){\n    stage('*** Phase 1 ***') {\n        //Using bash commands\n        sh '''#!/bin/bash\n            echo \"Hello World !\\n\"\n         '''\n    }\n}\n</code></pre>"},{"location":"Creating_jenkinsfile/#cloud-v-pipeline-for-cross-platform-compilation-and-execution","title":"<code>cloud-v-pipeline</code> for Cross-Platform Compilation and Execution","text":"<p>Cloud-V supports cross-compilation and execution on emulated RISC-V compute instances. Following tools help in cross compilation and cross-platfrom execution:  </p> <ul> <li>RISC-V GNU Toolchain</li> <li>QEMU user mode (for running standalone binaries)</li> <li>QEMU System (for running application in Linux)  </li> </ul> <p>An example pipeline script is given below in scripted pipeline.  </p> <pre><code>node('x86_runner2'){\n    checkout scm //Getting content of this repo\n    stage('*** Compilation Phase ***') { // for display purposes\n        //Compiling helloworld.c using bash commands\n        sh '''#!/bin/bash\n            gcc -g ./helloworld.c -o helloworld.out\n            riscv64-unknown-linux-gnu-gcc ./helloworld.c -o helloworld_riscv_compiled.out //Cross compiling for RISC-V\n         '''\n    }\n    stage (' *** Running Binaries ***'){\n        sh '''#!/bin/bash\n            ./helloworld.out\n            qemu-riscv64 -L $RISCV_SYSROOT helloworld_riscv_compiled.out //Running executable on RISC-V emulated platform\n         '''\n    }\n}\n</code></pre> <p>The equivalent declarative pipeline is as follows:  </p> <pre><code>pipeline {\n    agent {label \"x86_runner2\"}\n\n    stages {\n        stage('Clone Repository') {\n            steps('delegate'){\n                    checkout scm //Clones the repository on the local machine\n            }\n        }\n        stage ('Compilation Phase'){\n            steps{\n                    sh '''#!/bin/bash\n                        gcc -g ./helloworld.c -o helloworld.out\n                    '''\n                    sh '''#!/bin/bash\n                        riscv64-unknown-linux-gnu-gcc ./helloworld.c -o helloworld_riscv_compiled.out\n                    '''\n            }\n        }\n        stage ('Running Binaries'){\n            steps {\n                    sh ''' #!/bin/bash\n                        ./helloworld.out\n                    '''\n                    sh'''#!/bin/bash\n                        qemu-riscv64 -L $RISCV_SYSROOT helloworld_riscv_compiled.out\n                    '''\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"Creating_jenkinsfile/#reference-links","title":"Reference Links","text":"<p>https://www.jenkins.io/doc/book/pipeline/syntax/</p>"},{"location":"Multinode_Pipelines/","title":"Multinode Pipelines","text":"<p>One can create multinode pipelines in jenkins freestyle pipeline job as well as in jenkins native pipeline job. In such a pipeline, different stages of the pipeline can run on separate nodes. The jenkins native pipeline job can have two types.  </p> <ol> <li>Scripted Pipeline</li> <li>Declarative pipeline</li> </ol> <p>For the sake of this documentation, two nodes are used: <code>container-node</code> and <code>container-node2</code></p>"},{"location":"Multinode_Pipelines/#label","title":"Label","text":"<p>In jenkins, label specifies where the job or a stage of the job can run. In jenkins, a label can be explicitly added in the node's configuration so that each time that label is mentioned in a jenkins job, that specific node is used for building the job, or the name of the node can also be used as label in case if no label is added in the node.</p>"},{"location":"Multinode_Pipelines/#creating-multinode-pipelines-in-jenkins-native-pipeline-job","title":"Creating multinode pipelines in Jenkins native pipeline job","text":""},{"location":"Multinode_Pipelines/#scripted-pipeline","title":"Scripted Pipeline","text":"<p>For creating a multinode pipeline with jenkins native scripted pipeline job, jenkinsfile can be tweaked. The keyword <code>node</code> can be used to specify the node in which the stages are going to run and this keyword can also be used in between stages. Following screenshot indicates a code in which first stage runs on node named <code>container-node</code>. While the second node runs on node named <code>container-node2</code>.  </p> <pre><code>node ('container-node') {\n    stage ('*** Creating a directory in container-node ***'){\n        sh '''#!/bin/bash\n        mkdir newdir_container-node-$RANDOM\n        '''\n    }\n    stage ('*** Creating a directory in container-node2'){\n        node ('container-node2'){\n                sh'''#!/bin/bash\n                mkdir newdir_container-node2-$RANDOM\n                '''\n        }\n    }\n}\n</code></pre>"},{"location":"Multinode_Pipelines/#declarative-pipeline","title":"Declarative pipeline","text":"<p>For a declarative pipeline the keyword <code>label</code> can be used in the agent block. This can either be done at the start of the pipeline after the keyword <code>pipeline</code> or it can be used inside the stages. The following code builds two stages each of which is built in separate node. The keyword <code>none</code> with agent means that the agent is not specified globally for each stage and it should be specified inside each stage.</p> <pre><code>pipeline {\n    agent none // Means no agent specified. This means each node will specify its own agent\n    stages {\n        stage('container-node') {\n            agent{\n                label \"container-node\" //Selecting container-node for this stage\n            }\n            steps {\n                sh '''#!/bin/bash\n                    echo 'Hello container-node'\n                    mkdir \"newdir-container-node-$RANDOM\"\n                '''\n            }\n        }\n        stage('container-node2'){\n            agent{\n                label \"container-node2\" //Selecting container-node2 for this stage\n            }\n            steps{\n                sh'''#!/bin/bash\n                    echo ''Hello container-node2\n                    mkdir \"newdir-container-node2-$RANDOM\"\n                '''\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"Multinode_Pipelines/#creating-multinode-pipelines-using-jenkins-freestyle-jobs","title":"Creating multinode pipelines using Jenkins freestyle jobs","text":"<p>Jenkins freestyle jobs can be combined together by mentioning post-build jobs in the freestyle job settings. Freestyle jobs are customizable and offer more options than jenkins native pipeline jobs. Each job represents a stage and each stage has separate settings. One can use jenkins freestyle jobs to run on a separate node by selecting the option <code>Restrict where this project can be run</code> in the <code>General</code> section of the job's configuration. In the <code>Label Expression</code>, the node's name can be mentioned in which to run the job. The following image shows the option from job's configuration.  </p> <p> </p> <p>This can be done in all the job's configuration for the specified node.  </p>"},{"location":"Software_Developer_Guide/","title":"Software Developer Guide for RISC-V CI","text":"<p>A software developer is the end-user who will develop or build his/her projects on RISC-V compute instance using RISC-V CI. This guide will cover all the things a software developer needs to integrate their project with RISC-V continuous integration.</p> <p>Note: All the compute instances in Jenkins have restrictions regarding which jobs will run on the compute instance. The administrator has to allow you for using specific instance. Be sure to contact administrator and tell them which instance you want to use</p> <p>There are currently two ways to integrate version control (Git) project with Cloud-V.</p> <ol> <li>Using Cloud-V automatic integration (beta)</li> <li>Manually integrating your project with Cloud-V</li> </ol>"},{"location":"Software_Developer_Guide/#pre-requisites","title":"Pre-requisites","text":"<ol> <li>GitHub account.</li> <li>GitHub project repository with owner rights.</li> <li>Access to https://dash.cloud-v.co and https://cloud-v.co (visit this link to request the access)</li> </ol>"},{"location":"Software_Developer_Guide/#getting-an-account-for-cloud-v","title":"Getting an account for Cloud-V","text":"<p>After getting access to the Cloud-V platform, use one of the following two methods to create a CI pipeline with Cloud-V.</p>"},{"location":"Software_Developer_Guide/#1-using-cloud-v-automatic-integration-beta","title":"1. Using Cloud-V automatic integration (beta)","text":"<p>For ease of convinience for users and eliminating time delays of manual set up, users can add their GitHub and GitLab repository in Cloud-V by just adding their repository URL on the Cloud-V page. The source code for this is open-source here.</p> <p>Currently there are support for following version control systems:</p> <ol> <li>GitHub</li> <li>GitLab</li> </ol>"},{"location":"Software_Developer_Guide/#for-github","title":"For GitHub","text":"<p>For integrating user repository with Cloud-V, there is a GitHub app which users can install in their repository. The purpose of creating the app and publishing it for users is that, GitHub app has all the permissions already set up. So, when a user installs GitHub app, the app automatically sets up all the permissions for the user's repository.</p> <p>Following is the procedure for installing and integrating the repository with Cloud-V github app and for creating the CI pipeline in Cloud-V dashboard.</p> <ul> <li>Visit this link for installing GitHub app.</li> <li>Click on \"Install\" button which will take you to permissions page where you can select the permissions for the repository and also choose the repository which you would like to integrate with Cloud-V app</li> <li>Select \"Only select repositories\" if you would like to integrate a specific repository or number of repositories instead of integrating Cloud-V app with all the repositories.</li> <li>Click on \"Install &amp; Authorize\" which will take you to the page where you can add repository URL</li> <li>Add repository URL and click on \"Submit\"</li> <li> <p>The next page will show you:</p> </li> <li> <p>Access Token (will be visible one-time)</p> </li> <li>URL of the GitHub repository which is configured (currently, one token can be configured with one repository)</li> <li> <p>The link of the CI pipeline which is created automatically in Cloud-V CI dashboard</p> </li> <li> <p>Now go to the repository settings in the following manner and create a webhook for trigger with pull requests and push to branches</p> </li> <li> <p><code>Settings &gt; Webhooks &gt; Add webhook</code></p> </li> <li> <p>Fill the webhook settings in following manner</p> </li> <li> <p><code>Payload URL: https://dash.cloud-v.co/github-webhook/</code></p> </li> <li><code>Content Type: application/json</code></li> <li><code>Enable SSL Verification</code></li> <li><code>Which events would you like to trigger this webhook?: Just the push event</code></li> <li>Leave other fields as is</li> </ul> <p>Note: This creates a github multibranch pipeline automatically and it builds when a PR is created. If you need to check the source code or want to suggest any improvement for this, visit https://github.com/10x-Engineers/Cloud-V-git-automation and create an issue.</p>"},{"location":"Software_Developer_Guide/#for-gitlab","title":"For GitLab","text":"<p>Following is the procedure for integrating repository automatically with GitLab:</p> <p>There is no app for gitlab integration with Cloud-V. So, for creating a pipeline in automated way, you will have to add gitlab access token and gitlab repository URL. Use the following steps to do so.</p> <ul> <li>Generate a GitLab personal access token in your repository settings</li> <li>Visit this link</li> <li>Add the personal access token and URL of the GitLab repository</li> <li>If the personal access token and the URL of the repostory is valid, you will get a link to the created pipeline</li> </ul>"},{"location":"Software_Developer_Guide/#2-manually-adding-the-repository-in-cloud-v","title":"2. Manually adding the repository in Cloud-V","text":"<p>This is the traditional method. The flow involves:</p> <ol> <li>Getting a personal access token from GitHub settings</li> <li>Adding the personal access token in the Cloud-V dashboard by logging in with the provided credentials</li> <li>Notifying the administrator about the credentials ID which user added, so they can add the credentials in the global settings</li> <li>Setting up webhook in the repository settings so that it can be triggered whenever a pull request is created</li> </ol> <p>NOTE: If you followed first method to integrate the repository with Cloud-V, you will not have to follow this method.</p>"},{"location":"Software_Developer_Guide/#setting-credentials-for-webhook","title":"Setting credentials for webhook","text":"<p>Cloud-V supports webhooks which can trigger the job from external sources such as GitHub. They work in a way such that, if a specified branch is committed or if a pull request is created, the specified job build starts running depending upon the trigger event which is set in build's configuration in Cloud-V.  </p> <p>This process requires access token of the repository CREATED BY OWNER OF REPOSITORY on which the webhook is to be set. These credentials can be safely added to Cloud-V without anyone (even administrator) seeing the passwords as follows.</p>"},{"location":"Software_Developer_Guide/#obtaining-github-access-token-for-repository","title":"Obtaining github access token for repository","text":"<p>Navigate to the dashboard of your github account and click on the your github profile picture on the top-right corner on dashboard.</p> <p></p> <p>Then click on the \"Settings\" from the list.</p> <p></p> <p>From the left option bar in Settings scroll down and click on \"Developer settings\".</p> <p></p> <p>Once there, click on \"Personal access tokens\", then click on \"Fine-grained tokens\" from the dropdown list and after that click on \"Generate new token\".</p> <p></p> <p>This will open the page for setting up new access token. Follow following steps for creating a token:  </p> <ol> <li>Give your token a meaningful name under \"Token name\"</li> <li>Set expiration date in \"Expiration\" depending upon how long you would like your repository to be integrated with Cloud-V (think of a meaningful upper bound)</li> <li>The \"Resource owner\" should be the owner of the repository who can access all kinds of settings of the repository</li> <li>Under \"Repository access\", check \"Only select repositories\" and then select the repository for which you would like to create the token</li> <li> <p>Under \"Permissions\" section, expand \"Repository Permissions\" and give the following two permissions:</p> <ul> <li>\"Read and write\" access to \"Commit statuses\" (Because after the CI has run, Cloud-V will be able to set the status of the commit accordingly)</li> <li>\"Read-only\" access to \"Webhooks\"</li> </ul> </li> </ol>"},{"location":"Software_Developer_Guide/#configuring-repository-webhook","title":"Configuring repository webhook","text":"<p>In GitHub,</p> <ul> <li>Go to repository settings which you want to integrate for Cloud-V.  </li> </ul> <p></p> <ul> <li>Go to <code>Webhooks</code> </li> </ul> <p> </p> <ul> <li>Click on <code>Add webhook</code> </li> </ul> <p></p> <ul> <li>Add <code>Payload URL</code> as <code>https://dash.cloud-v.co/ghprbhook/</code> </li> <li>Select content type as <code>application/json</code></li> <li>Check <code>Enable SSL verification</code> </li> <li>In the section Which events would you like to trigger this webhook? check <code>Let me select individual events</code> and check <code>Pul requests</code> as individual events and dont check any other permission.</li> </ul> <p>Webhook settings will look something like this:</p> <p> </p>"},{"location":"Software_Developer_Guide/#configurations-inside-cloud-v","title":"Configurations inside Cloud-V","text":"<p>Note: Currently users are not able to see or modify pipeline build configuration inside Jenkins, that is currently managed by administrator. Users are requested to inform administrator about how they want their pipeline configured.</p> <ul> <li>We will provide you with Cloud-V credentials on the provided email.</li> <li>Login with provided credentials.</li> <li>Click on the <code>Credentials</code> in the left menu.  </li> </ul> <p></p> <ul> <li>This will take you to the credentials page.</li> <li>Scroll down to the <code>Stores scoped to Jenkins</code> and click on the <code>System</code> as shown in the image.  </li> </ul> <p> </p> <ul> <li>Click on <code>Global credentials (unrestricted)</code>.  </li> </ul> <p></p> <ul> <li>Click on <code>Add Credentials</code>.  </li> </ul> <p></p> <ul> <li>This will take you to the <code>New Credentials</code> page.</li> <li>Select <code>Kind</code> as <code>Username with password</code>.</li> <li>Select <code>Scope</code> as <code>Global (Jenkins, nodes, items, all child items etc)</code>.</li> <li>Enter your GitHub username in <code>Username</code></li> <li>Enter <code>Password</code> as <code>GitHub personal authentication token</code> (PAT) which can be acquired from Github account settings.  </li> <li><code>ID</code> is optional but you can enter a unique <code>ID</code>. <code>Description</code> can be left empty. But it is recommended to give a suitable but careful description by which administrator will be able to identify and use these credentials to set up github webhook</li> <li>Select <code>Create</code></li> <li>This process will look something like this</li> </ul> <p> </p> <ul> <li>Now credentials will be available in the credentials list and will be shown to you as well as administrator as shown in the image below. This will create an option in configurations for using these credentials in github webhook without changing or viewing them.  </li> </ul> <p> </p> <ul> <li>Note the credentials ID (as shown in the image below) and email it to the same administrator email on which you received the login credentials for Cloud-V. It is important that administrator knows the credentials ID because they will use it in the job build configurations.  </li> </ul> <p> </p> <p>Note: Please make sure to inform the administrator via email that you have added the credentials in Cloud-V Dashboard. Also, send administrator the ID of credentials via email.</p>"},{"location":"Software_Developer_Guide/#requirements-for-administrator","title":"Requirements for administrator","text":"<p>After the above setup is complete from software developer's side, developer will need to provide the administrator with following information.  </p> <ul> <li>Dependencies for running the project which can be packages which are needed to install in the RISC-V CI environment by administrator.</li> <li>Events for triggering the job build.</li> <li>URL of GitHub repository.</li> <li>Path and name of <code>cloud-v-pipeline</code> file on the provided GitHub repository.</li> <li>Any additional information which should be given for successful execution of job builds.</li> </ul>"},{"location":"Tooling/","title":"Tools on Cloud-V","text":""},{"location":"Tooling/#using-environment-modules","title":"Using Environment Modules","text":"<p>Users can use environment modules to load different versions of same program. For using environment modules the pattern is as follows:</p> <pre><code>module load &lt;PACKAGENAME/VERSION&gt;\n</code></pre> <p>Important Note: Be sure to use <code>#!/bin/bash -l</code> instead of <code>#!/bin/bash</code> in CI pipeline file since that is required for environment modules to load</p> <p>For example if you want to load python version 3.9.2 compiled for x86, you will need to use following command:  </p> <pre><code>module load python/3.9.2\n</code></pre> <p>For packages compiled for RISC-V architecture host, you will need to append <code>_riscv</code> to package name. For example, for python 3.8.15 compiled for RISC-V, following command will be used.  </p> <pre><code>module load python_riscv/3.8.15\n</code></pre>"},{"location":"cicd/","title":"Running CI/CD pipelines on RISC-V with Cloud-V","text":"<p>This document describes how someone can integrate their projects on RISC-V compute machine.</p>"},{"location":"cicd/#supported-cicd-platforms-on-cloud-v","title":"Supported CI/CD platforms on Cloud-V","text":"<p>There is a dedicated page on Cloud-V website where you can check which version control platforms or CI/CD services are supported by Cloud-V with RISC-V compute machines. At present, developers can use these compute machines free of cost. </p> <p>Services which are labeled \"Coming Soon!\" are in progress and will be added soon.</p>"},{"location":"cicd/#supported-compute-machine-types","title":"Supported compute machine types","text":"<p>Cloud-V offers vendor-agnostic physical compute machines as well as emulated compute instances. </p> <p>Physical compute machines are the SBCs, desktop computers or laptops with RISC-V SoCs in them while the emulated compute instances are running on the x86 host machines. The emulated RISC-V machines use QEMU.</p>"},{"location":"cicd/#why-do-we-even-need-qemu","title":"Why do we even need QEMU","text":"<p>Every RISC-V SoC in the market supports only a limited set of extensions. So developers don't get flexibility to test the software on the desired ISA string. With QEMU, developers can have an exhaustive list of RISC-V extensions (which do not conflict with each other).</p> <p>One other advantage is that if the x86 host machine (which is running the QEMU) has good computing power (say 128 or 256 cores), then it can be leveraged by QEMU to provide a far better computing power than what can be provided by RISC-V physical boards available in the market at present.</p> <p>Check &lt;&gt; for ISA extensions running in QEMU.</p>"},{"location":"jenkins_gitlab_integration/","title":"Integrating GitLab with Jenkins","text":"<p>This documentation will cover how to create an integration between jenkins and GitLab. This will allow users to trigger jenkins job when a <code>merge request</code> or a <code>push</code> is detected in GitLab.  </p>"},{"location":"jenkins_gitlab_integration/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>GitLab plugin  </li> <li>Git plugin  </li> <li>GitLab repository with owner's credentials  </li> </ul>"},{"location":"jenkins_gitlab_integration/#configuring-jenkins-system","title":"Configuring Jenkins System","text":"<p>First jenkins needs to be configured.  </p> <ul> <li>Go to <code>Dashboard &gt; Manage Jenkins &gt; Configure System</code> </li> <li>Scroll down to <code>Gitlab</code> section  </li> <li> <p>Check <code>Enable authentication for '/project' end-point</code> </p> </li> <li> <p>Enter a <code>Connection name</code>.  </p> </li> <li>Enter <code>Gitlab host URL</code> as <code>https://gitlab.com/</code>. In case there is a different domain name, then enter there instead of above url.  </li> <li>In <code>Credentials</code>, click on <code>Add</code> then click on <code>Jenkins</code>.  </li> <li>In <code>Kind</code>, select <code>GitLab API token</code>.  </li> <li>In <code>API token</code>, enter the gitlab personal access token (this will be obtained below while configuring GitLab).  </li> <li>Click on <code>Advanced</code>.</li> <li>Click on <code>Test Connection</code>.  </li> <li>If everything goes right, it should print <code>success</code>.  </li> </ul>"},{"location":"jenkins_gitlab_integration/#configuring-gitlab","title":"Configuring GitLab","text":"<ul> <li>Click on profile avatar in the top right.  </li> <li>Click on <code>Edit profile</code>.  </li> </ul> <ul> <li>Click on <code>Access Tokens</code>.</li> <li>Create a new personal access token and copy it (this is the GitLab API token used in above section <code>Configuring Jenkins System</code>).  </li> <li>Go to repository settings.</li> <li>On left-side pane, select <code>Webhooks</code>.  </li> <li>Enter <code>GitLab webhook URL</code> (this is explained below in next section).  </li> <li>Enter <code>Secret Token</code> (this is explained in the below section).</li> <li>Check desirable trigger options.  </li> <li>Click <code>Add webhook</code>.</li> </ul>"},{"location":"jenkins_gitlab_integration/#configuring-jenkins-job","title":"Configuring Jenkins Job","text":"<ul> <li>Create a jenkins job.</li> <li>On job configuration page, scroll down to <code>Source Code Management</code>.  </li> <li>Select <code>Git</code>.  </li> <li>In <code>Credentials</code>, add the owner credentials of GitLab repository. This will be <code>Username and Password</code>.  </li> <li>Select the appropriate branch (generally it is <code>main</code>).  </li> <li>Scroll down to <code>Build Triggers</code>.  </li> <li>Check <code>Build when a change is pushed to GitLab</code>. There will also be a <code>GitLab webhook URL</code>. This is needed in GitLab. This URL will be called as <code>Webhook URL</code>.  </li> <li>Click on <code>Advanced</code> in the same option, then scroll down and generate a secret token. This token is also needed. This token will be called as <code>Secret Token</code>.</li> <li>Scroll down to <code>Post-build Actions</code> and select <code>Publish build status to GitLab</code>.  </li> <li>Click on <code>Apply</code> and <code>Save</code>.  </li> </ul> <p>Now whenever there will be a push (or whatever build trigger option is set in jenkins and gitlab webhook), specified jenkins job will be triggered.</p>"},{"location":"runner_specs/","title":"Specifications of compute instances in Cloud-V","text":"<p>This document contains the specifications of the compute instances available for users to run builds in Cloud-V. The term \"Compute Instance\", can also be safely interchanged with the terms \"Build Executor\" and \"Runner\".</p> Name CI Name String Architecture ISA String Cores Memory Compute Instance Type N/A J-x86-1 (or) J-QMU-1 x86_64 N/A 4 8GiB Hardware with application-level emulator intel i7-6500U J-TESTVM-1 x86_64 N/A 4 8GiB Hardware Raspberry Pi 4 Model B J-RASP4-1 aarch64 ARMv8-A 4 4GiB Hardware QEMU System Linux J-QMS-1 riscv64 See Ext 1 at bottom 12 6GiB QEMU System emulator VisionFive 1 J-VF1-1 riscv64 rv64imafdc 2 8GiB Hardware VisionFive 1 J-VF1-2 riscv64 rv64imafdc 2 8GiB Hardware VisionFive 1 J-VF1-3 riscv64 rv64imafdc 2 8GiB Hardware VisionFive 2 J-VF2-1 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-2 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-3 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-4 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-5 riscv64 rv64imafdc 4 8GiB Hardware VisionFive 2 J-VF2-6 riscv64 rv64imafdc 4 8GiB Hardware HiFive Unleashed J-HF-1 riscv64 rv64imafdc 4 8GiB Hardware Banana Pi F3 J-BPF3-1 riscv64 rv64imafdcv_sscofpmf_sstc_svpbmt_zicbom_zicboz_zicbop_zihintpause 1 4GiB Hardware Milk-V Jupiter J-JUPITER-1 riscv64 rv64imafdcv_sscofpmf_sstc_svpbmt_zicbom_zicboz_zicbop_zihintpause 1 16GiB Hardware Milk-V Pioneer Box J-pioneer-1 riscv64 rv64imafdcv_sscofpmf_sstc_svpbmt_zicbom_zicboz_zicbop_zihintpause 4 8GiB Hardware <p>Ext1: rv64imafdch_zicbom_zicboz_zicntr_zicond_zicsr_zifencei_zihintntl_zihintpause_zihpm_zacas_zfa_zfh_zfhmin_zba_zbb_zbc_zbkb_zbkc_zbkx_zbs_zknd_zkne_zknh_zkr_zkt_zksed_zksh_ztso_zvbb_zvbc_zvfh_zvfhmin_zvkb_zvkg_zvkned_zvknha_zvknhb_zvksed_smaia_smstateen_ssaia_sscofpmf_sstc_svinval_svnapot_svpbmt</p> <p>Note: The <code>J-QMU-1</code> and <code>J-x86-1</code> are one and the same runner. The purpose of creating two separate executors for same hardware is that <code>J-x86-1</code> is supposed to be specifically for x86 architecture whereas <code>J-QMU-1</code> is specifically for the users who want to cross compile source code for riscv64 architecture and then use qemu-usermode to execute them. Nevertheless, the tooling available for <code>J-x86-1</code> can also be used for <code>J-QMU-1</code></p>"},{"location":"sandboxing/","title":"Using SSH access on RISC-V instance","text":"<p>After getting SSH command and credentials from Cloud-V administrator, users can log in to RISC-V instance and start development like any Linux machine without GUI.</p>"},{"location":"sandboxing/#setting-up-vscodium-with-risc-v","title":"Setting up VScodium with RISC-V","text":"<p>It may not be convinient for users to use terminal editors (like Vim and Nano) if they are not accustomed.</p> <p>At present VScode remote extension does not have support for RISC-V architecture but VScodium supports it.</p> <p>This section explains how you can set up VScodium on x86 machine to get remote development access to RISC-V compute instance (assuming both the machines have SSH installed).</p> <ol> <li>Install VScodium from this link</li> <li>Install Open Remote - SSH Extension from jeanp413</li> </ol> <p></p> <ol> <li>Use <code>Ctrl+Shift+p</code> to open command pallet and search remote <code>Remote-SSH: Connect to host...</code></li> <li>Enter username, publicly accessible IP and port of the compute instance which you want to connect to.</li> </ol>"},{"location":"setting_up_gitlab_runner/","title":"Adding RISC-V compute machine as GitLab Runner","text":"<p>This document explains how developers working on GitLab projects can add RISC-V machines provided by Cloud-V as a GitLab runners.</p>"},{"location":"setting_up_gitlab_runner/#register-the-runner","title":"Register the runner","text":"<p>For registering the runner, visit this link. Add the necessary information on the page and then select a compute machine to use as GitLab runner. </p> <p>After the necessary information is placed, clicking the submit button will try to register the runner. The registration can take some time so please be patient with next page loading. Once an attempt to register is finished, you will see the result accordingly on the next page.</p>"},{"location":"setting_up_gitlab_runner/#how-the-gitlab-runner-works","title":"How the GitLab Runner works","text":"<p>The GitLab runner runs on a non-sudo user on the specified RISC-V machines. Every RISC-V machine can be a shared runner between different GitLab projects. The GitLab runner package runs as docker executor on the RISC-V machine so that every job is isolated from other jobs. The necessary information about the specification of GitLab runner is mentioned on the GitLab runner registration page of Cloud-V.</p> <p>Following diagram presents a view of how the job runs on the RISC-V machines in the Cloud-V.</p> <p> </p>"},{"location":"tooling_J-BPF3-x/","title":"Tools on <code>J-BPF3-x</code> node","text":"<p>This compute instance is Banana Pi F3 board and it has available packages ONLY for RISC-V architecture.  </p> <p>Operating System: Bianbu 1.0.x</p> Tool Version Installed from Git 2.43.0 apt OpenJDK 21.0.2 apt GCC 13.2.0 apt Python3 3.11.7 apt OpenSSL 3.1.5 apt Ruby 3.1.2p20 apt Go 1.22.0 apt rustc 1.70.0 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.28.3 apt make 4.3 apt automake 1.16.5 apt gfortran 13.2.0 apt openssh-server 9.6p1 apt vim 9.1 apt"},{"location":"tooling_J-HF-1/","title":"Tools on <code>J-HF-1</code> node","text":"<p>This compute instance is Sifive's HiFive Unleased board and it has available packages ONLY for RISC-V architecture.  </p> <p>Operating System: Ubuntu 20.04.6 (Focal Fossa)</p> Tool Version Installed from Git 2.25.1 apt OpenJDK 11.0.20.1 apt GCC 10.5.0 apt Python3 3.8.10 apt OpenSSL 1.1.1f apt Ruby 2.7.0p0 apt Go 1.14.3 apt rustc 1.41.0 apt Flex 2.6.4 apt Ninja 1.10.0 apt Bison 3.5.1 apt autoconf 2.69 apt gperf 3.1 apt cmake 3.16.3 apt make 4.2.1 apt automake 1.16.1 apt gfortran 9.4.0 apt openssh-server 8.2p1 apt"},{"location":"tooling_J-K230-1/","title":"Tools on <code>J-K230-1</code> node","text":"<p>This compute instance is Starfive's VisionFive 2 board and it has available packages ONLY for RISC-V architecture.  </p> <p>Operating System: Debian 12 (Bookworm)</p> Tool Version Installed from Git 2.40.1 apt OpenJDK 21.0.2 apt GCC 13.2.0 apt Python3 3.11.6 apt OpenSSL 3.0.10 apt Ruby 3.1.2p20 apt Go 1.21.1 apt rustc 1.75.0 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.27.4 apt make 4.3 apt automake 1.16.5 apt gfortran 13.2.0 apt openssh-server 9.3p1 apt vim 9.0 apt vim 3.3a apt"},{"location":"tooling_J-QMS-1/","title":"Tools on <code>J-QMS-1</code> node","text":"<p>This is QEMU system compute instance with 64-bit RISC-V Linux and it has all the packages ONLY for RISC-V architecture  </p> <p>Operating System: Ubuntu 22.04.4 LTS (Jammy Jellyfish) QEMU Linux Version: 7.2.90 (v9.0.0-rc0-68-g853546f812)</p> Tool Version Installed from pip 22.0.2 apt Git 2.34.1 apt OpenJDK 19.0.1 apt GCC 11.4.0 apt Python3 3.10.12 apt OpenSSL 3.0.2 apt Ruby 3.0.2p107 apt Go 1.18.8 apt rustc 1.75.0 apt Flex 2.6.4 apt Ninja 1.10.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt make 4.3 apt cmake 3.22.1 apt"},{"location":"tooling_J-QMU-1J-VM-1/","title":"Tools on <code>J-x86-1</code> or <code>J-QMU-1</code> node","text":"<p>Tools which are mentioned for <code>x86</code> architecture are able to run on<code>J-x86-1</code>. Tools which are mentioned for <code>RISC-V</code> architecture are able to run on <code>J-QMU-1</code>.</p> <p>The packages which are supported for <code>QEMU User mode</code> can be used by normal commands once they are loaded.</p> <p>Here <code>PACKAGE_NAME</code> is the package which you want to run on QEMU user mode.</p> <p>Operating System: Debian 11 (bullseye) QEMU User Mode Version: Different Versions (see the table below)</p>"},{"location":"tooling_J-QMU-1J-VM-1/#tooling-available-for-j-x86-1","title":"Tooling available for <code>J-x86-1</code>","text":"<p>The tools available for <code>J-x86-1</code> is for use on x86 architecture and these tools do not support execution on RISC-V architecture</p> Tool Versions Installed from Host Architecture Environment Modules Support Git 2.3.0.2 source x86 N/A OpenJDK 19.0.1 apt x86 N/A GCC 10.4.0, 12.2.0 apt x86 Yes Python3 3.8.15, 3.9.2 source x86 Yes Go 1.18.8 apt x86 N/A rustc 1.65.0 source x86 N/A Flex 2.6.4 apt x86 N/A Ninja 1.10.1-1 apt x86 N/A Bison 3.7.5 apt x86 N/A autoconf 2.69 apt x86 N/A gperf 2.2.4 apt x86 N/A spike 1.1.1-dev source x86 Yes Verilator 4.038 apt x86 N/A Sail (riscv_sim_RV64, riscv_sim_RV32) 0.5 source x86 Yes cmake 3.18.4 apt x86 N/A make 4.3 apt x86 N/A ARM EABI GNU toolchain 13.2.Rel1 source x86 Yes qemu-system-arm 8.2.2 source x86 Yes SCons 4.7.0 source x86 Yes"},{"location":"tooling_J-QMU-1J-VM-1/#tooling-available-for-j-qmu-1","title":"Tooling available for <code>J-QMU-1</code>","text":"<p>The tools available for <code>J-QMU-1</code> is for use on RISC-V architecture and these tools do not support execution on x86 architecture</p> Tool Versions Installed from Host Architecture Environment Modules Support Python3 3.8.15 source RISC-V Yes zlib 1.2.13 source RISC-V N/A OpenSSL 1.1.1r source RISC-V Yes Ruby (without IRB) 3.2.0dev source RISC-V Yes rustc 1.65.0 source RISC-V N/A Flex 2.6.4 source RISC-V yes Ninja 1.12.0.git source RISC-V Yes Bison 3.8.2, 2.3 source RISC-V Yes clang 16.0.0 source RISC-V (cross compiler) Yes riscv-pk 1.0.0-91-g573c858 source RISC-V Yes"},{"location":"tooling_J-QMU-1J-VM-1/#qemu-user-mode-and-risc-v-gnu-cross-compilers","title":"QEMU User mode and RISC-V GNU Cross compilers","text":"<p>From now on RISC-V cross-compilers can only be loaded with their respective QEMU User mode on Cloud-V. This is configured so that there is no confusion between toolchain version and qemu user mode being used because both of these will be \"generally\" taken from the latest releases of nightly builds. Loading a certain RISC-V toolchain using environment modules will automatically load the respective qemu usermode version unless otherwise specified.</p> <p>The loading pattern for RISC-V 64-bit GNU Glibc toolchain will be as follows:</p> <pre><code>module load riscv64-gnu-glibc/&lt;release-date&gt;\n</code></pre> <p>And the loading pattern for RISC-V 64-bit GNU Glibc toolchain will be as follows:</p> <pre><code>module load riscv64-gnu-elf/&lt;release-date&gt;\n</code></pre> <p>Following table provides relevant information about version of the toolchain and the respective QEMU User mode version (where the release date is mentioned in pattern <code>MMDDYYYY</code>).</p> Release date GNU Toolchain version (elf and glibc) QEMU Version 03012024 13.2.0 8.2.1 02022024 13.2.0 8.2.1 02022024 13.2.0 8.1.1 <p>Note: The <code>J-QMU-1</code> and <code>J-x86-1</code> are one and the same runner. The purpose of creating two separate executors for same hardware is that <code>J-x86-1</code> is supposed to be specifically for x86 architecture whereas <code>J-QMU-1</code> is specifically for the users who want to cross compile source code for riscv64 architecture and then use qemu-usermode to execute them. Nevertheless, the tooling available for <code>J-x86-1</code> can also be used for <code>J-QMU-1</code></p>"},{"location":"tooling_J-VF1-x/","title":"Tools on <code>J-VF1-x</code> node","text":"<p>This compute instance is Starfive's VisionFive 1 board and it has available packages ONLY for RISC-V architecture.  </p> <p>Operating System: Ubuntu 24.04 (Noble Numbat)</p> Tool Version Installed from Git 2.39.2 apt OpenJDK 21-ea apt GCC 13.1.0 apt Python3 3.11.4 apt OpenSSL 3.0.8 apt Ruby 3.1.2p20 apt Go 1.20.3 apt rustc 1.67.1 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.25.1 apt make 4.3 apt automake 1.16.5 apt gfortran 13.1.0 apt openssh-server 9.0p1 apt"},{"location":"tooling_J-VF2-x/","title":"Tools on <code>J-VF2-x</code> node","text":"<p>This compute instance is Starfive's VisionFive 2 board and it has available packages ONLY for RISC-V architecture.  </p> <p>Operating System: Debian 12 (Bookworm)</p> Tool Version Installed from Git 2.43.0 apt OpenJDK 21.0.2 apt GCC 13.2.0 apt Python3 3.11.7 apt OpenSSL 3.1.5 apt Ruby 3.1.2p20 apt Go 1.22.0 apt rustc 1.70.0 apt Flex 2.6.4 apt Ninja 1.11.1 apt Bison 3.8.2 apt autoconf 2.71 apt gperf 3.1 apt cmake 3.28.3 apt make 4.3 apt automake 1.16.5 apt gfortran 13.2.0 apt openssh-server 9.6p1 apt vim 9.1 apt"}]}
>>>>>>> c1be17c (Changed docs according to new format)
